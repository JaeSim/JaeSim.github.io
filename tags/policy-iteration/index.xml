<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Policy Iteration on JaeSim&#39;s Workspace</title>
    <link>https://JaeSim.github.io/tags/policy-iteration/</link>
    <description>Recent content in Policy Iteration on JaeSim&#39;s Workspace</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Thu, 29 May 2025 12:21:05 +0900</lastBuildDate>
    <atom:link href="https://JaeSim.github.io/tags/policy-iteration/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>temp : 3. Planning by Dynamic Programming</title>
      <link>https://JaeSim.github.io/reinforcement-learning/rl-dp/</link>
      <pubDate>Thu, 29 May 2025 12:21:05 +0900</pubDate>
      <guid>https://JaeSim.github.io/reinforcement-learning/rl-dp/</guid>
      <description>&lt;h1 id=&#34;3-planning-by-dynamic-programming&#34;&gt;&#xD;&#xA;  &lt;strong&gt;3. Planning by Dynamic Programming&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#3-planning-by-dynamic-programming&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h1&gt;&#xD;&#xA;&lt;blockquote class=&#34;book-hint warning&#34;&gt;&#xD;&#xA;  // NOTE: 이 페이지는 임시로 작성되었습니다.&#xD;&#xA;&lt;/blockquote&gt;&#xD;&#xA;&lt;p&gt;MDP를 푸는 방식들이 여러개가 있다.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Policy evaluation, Policy iteration, value iteration&lt;/strong&gt; 등이 있고, 이것들은 &lt;strong&gt;환경을 정확하게 안다면&lt;/strong&gt; DP가 적용이 가능하다&lt;/p&gt;&#xA;&lt;p&gt;먼저 간략하게 언급하자면&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;policy를 평가하고 iteration 하면서 발전해나가는 방식과  (policy evaluation + policy iteration)&lt;/li&gt;&#xA;&lt;li&gt;value function을 iteration하면서 옵티멀을 찾아가는 방법 (value iteration)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;이 있다.&lt;/p&gt;&#xA;&lt;p&gt;이번 섹션은 DP로 known MDP를 푸는 방법에 대한것이고, 이것은 강화학습의 flow 와 수식 간의 이해를 위한 섹션이다.&#xA;4장부터 unknown MDP를 푸는 방법이 기술되어 있다.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
