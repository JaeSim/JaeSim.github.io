<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on JaeSim&#39;s Workspace</title>
    <link>https://JaeSim.github.io/</link>
    <description>Recent content in Home on JaeSim&#39;s Workspace</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Thu, 12 Jun 2025 10:57:55 +0900</lastBuildDate>
    <atom:link href="https://JaeSim.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>두번째 글입니다. 블로그 실험용 테스트 글입니다.</title>
      <link>https://JaeSim.github.io/development/secondpost/</link>
      <pubDate>Tue, 20 May 2025 16:53:50 +0900</pubDate>
      <guid>https://JaeSim.github.io/development/secondpost/</guid>
      <description>&lt;h1 id=&#34;test&#34;&gt;&#xD;&#xA;  &lt;strong&gt;test&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#test&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h1&gt;&#xD;&#xA;&lt;h2 id=&#34;test2&#34;&gt;&#xD;&#xA;  &lt;strong&gt;test2&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#test2&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h2&gt;&#xD;&#xA;&lt;p&gt;RL 관련 항목을 기록하기 위한 post 입니다.&lt;/p&gt;&#xA;&lt;p&gt;페이지 업로드 확인용 임시 포스트&lt;/p&gt;</description>
    </item>
    <item>
      <title>할일</title>
      <link>https://JaeSim.github.io/development/firstpost/</link>
      <pubDate>Wed, 21 May 2025 10:26:34 +0900</pubDate>
      <guid>https://JaeSim.github.io/development/firstpost/</guid>
      <description>&lt;p&gt;개발 관련 항목을 기록하기 위한 post 입니다.&lt;/p&gt;&#xA;&lt;p&gt;Todo List:&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;상태&lt;/th&gt;&#xA;          &lt;th&gt;날짜&lt;/th&gt;&#xA;          &lt;th&gt;작업 내용&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;✅&lt;/td&gt;&#xA;          &lt;td&gt;&lt;em&gt;2025-05-21&lt;/em&gt;&lt;/td&gt;&#xA;          &lt;td&gt;블로그 GitHub Page 연동&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;✅&lt;/td&gt;&#xA;          &lt;td&gt;&lt;em&gt;2025-05-22&lt;/em&gt;&lt;/td&gt;&#xA;          &lt;td&gt;RL 관련 Post 남기기 (on-going)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;✅&lt;/td&gt;&#xA;          &lt;td&gt;&lt;em&gt;2025-05-22&lt;/em&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;em&gt;hugo + github page + giscus&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;✅&lt;/td&gt;&#xA;          &lt;td&gt;&lt;em&gt;2025-05-22&lt;/em&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Google 서치 연동&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;☐&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Naver 서치 연동&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;✅&lt;/td&gt;&#xA;          &lt;td&gt;&lt;em&gt;2025-05-22&lt;/em&gt;&lt;/td&gt;&#xA;          &lt;td&gt;블로그 setup 관련 post 남기기&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;☐&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;About Me&lt;/code&gt; 완성&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;☐&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Google 서치 연동&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>Github pages 를 이용한 blog Setting (hugo &#43; hugo-book theme &#43; giscus</title>
      <link>https://JaeSim.github.io/development/blogsetup/</link>
      <pubDate>Fri, 23 May 2025 11:06:13 +0900</pubDate>
      <guid>https://JaeSim.github.io/development/blogsetup/</guid>
      <description>&lt;h1 id=&#34;github-pages-를-이용한-blog-setting-hugo--hugo-book-theme--giscus&#34;&gt;&#xD;&#xA;  &lt;strong&gt;Github pages 를 이용한 blog Setting (hugo + hugo-book theme + giscus&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#github-pages-%eb%a5%bc-%ec%9d%b4%ec%9a%a9%ed%95%9c-blog-setting-hugo--hugo-book-theme--giscus&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h1&gt;&#xD;&#xA;&lt;h2 id=&#34;0-blog-host-방식-선택&#34;&gt;&#xD;&#xA;  &lt;strong&gt;0. blog host 방식 선택&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#0-blog-host-%eb%b0%a9%ec%8b%9d-%ec%84%a0%ed%83%9d&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h2&gt;&#xD;&#xA;&lt;p&gt;여러 블로그 host방식을 고려하였으나, 유지보수가 손이 덜가며, 오랫동안 hosting이 되는것이 우선 순위였고.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Github pages&lt;/strong&gt;를 통한 호스팅 방법을 선택하였다.&lt;/p&gt;&#xA;&lt;p&gt;그렇다면, static page를 generation을 해주는 framework로 jekyll 와 hugo를 고민하였고&#xA;ruby 보다 go로 이루어진 hugo를 믿기로 하였다.&lt;/p&gt;&#xA;&lt;p&gt;다른 blog들이 생성한 페이지를 주로 참조하였으며, 이 포스팅은 해당 글들의 엮음에 불과한점을 참조 부탁한다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>1. Reinforcement Learning Essential</title>
      <link>https://JaeSim.github.io/reinforcement-learning/rl-essential/</link>
      <pubDate>Thu, 22 May 2025 11:13:00 +0900</pubDate>
      <guid>https://JaeSim.github.io/reinforcement-learning/rl-essential/</guid>
      <description>&lt;h1 id=&#34;1-강화학습에-대한-기초-내용&#34;&gt;&#xD;&#xA;  &lt;strong&gt;1. 강화학습에 대한 기초 내용&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1-%ea%b0%95%ed%99%94%ed%95%99%ec%8a%b5%ec%97%90-%eb%8c%80%ed%95%9c-%ea%b8%b0%ec%b4%88-%eb%82%b4%ec%9a%a9&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h1&gt;&#xD;&#xA;&lt;h2 id=&#34;들어가기-앞서&#34;&gt;&#xD;&#xA;  &lt;strong&gt;들어가기 앞서&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%eb%93%a4%ec%96%b4%ea%b0%80%ea%b8%b0-%ec%95%9e%ec%84%9c&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h2&gt;&#xD;&#xA;&lt;p&gt;강화학습의 기초적인 내용을 학습한 뒤 정리한 것으로,&#xA;남들에게 보여주기 보다는 본인의 이해와 기억을 위해서 기술한 것입니다.&lt;/p&gt;&#xA;&lt;p&gt;나만의 방식으로 이해한 것이기 때문에, 주요하다고 생각하는 부분이 다를수 있으며 생략되거나 놓친 부분이 많이 있습니다.&lt;/p&gt;&#xA;&lt;h2 id=&#34;강화학습rl-reinforcement-learning-이란&#34;&gt;&#xD;&#xA;  &lt;strong&gt;강화학습(RL: Reinforcement Learning) 이란?&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%ea%b0%95%ed%99%94%ed%95%99%ec%8a%b5rl-reinforcement-learning-%ec%9d%b4%eb%9e%80&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h2&gt;&#xD;&#xA;&lt;h3 id=&#34;definition&#34;&gt;&#xD;&#xA;  &lt;strong&gt;Definition&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#definition&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h3&gt;&#xD;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;“Reinforcement learning is learning what to do—how to map &lt;strong&gt;situations&lt;/strong&gt; to &lt;strong&gt;actions&lt;/strong&gt;—so as to maximize a numerical &lt;strong&gt;reward&lt;/strong&gt; signal.”&lt;br&gt;&#xA;— &lt;em&gt;Richard S. Sutton and Andrew G. Barto&lt;/em&gt;, &lt;em&gt;Reinforcement Learning: An Introduction&lt;/em&gt; (2nd ed), p.1&lt;/p&gt;</description>
    </item>
    <item>
      <title>2. Markov Decision Process</title>
      <link>https://JaeSim.github.io/reinforcement-learning/rl-mdp/</link>
      <pubDate>Tue, 27 May 2025 16:43:57 +0900</pubDate>
      <guid>https://JaeSim.github.io/reinforcement-learning/rl-mdp/</guid>
      <description>&lt;h1 id=&#34;2-markov-decision-process&#34;&gt;&#xD;&#xA;  &lt;strong&gt;2. Markov Decision Process&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2-markov-decision-process&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h1&gt;&#xD;&#xA;&lt;h2 id=&#34;markov-processmp-란&#34;&gt;&#xD;&#xA;  &lt;strong&gt;Markov Process(MP) 란?&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#markov-processmp-%eb%9e%80&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h2&gt;&#xD;&#xA;&lt;h3 id=&#34;mp-속성&#34;&gt;&#xD;&#xA;  &lt;strong&gt;MP 속성&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mp-%ec%86%8d%ec%84%b1&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h3&gt;&#xD;&#xA;&lt;p&gt;MDP는 환경에 대해서 Reinforcement Learning이 이해가능하도록 수식화한다&lt;/p&gt;&#xA;&lt;p&gt;&lt;U&gt;거의 모든 RL 관련 문제들은 MDP로 수식화 할 수 있다(Fully observable이나 Partially observation이나) &lt;/U&gt;&lt;/p&gt;&#xA;&lt;p&gt;Markov Property를 이용하는데, &amp;ndash;이전 강의참조&amp;ndash;&lt;/p&gt;&#xA;&lt;p&gt;요약하면, 현재 state만으로 미래를 예측해도 된다는 속성이다.&#xA;(다르게 말하면, 현재 state가 이미 유용한 정보를 포함하고 있다. = memoryless)&lt;/p&gt;&#xA;&lt;p&gt;Markov state &#xD;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://JaeSim.github.io/katex/katex.min.css&#34; /&gt;&#xD;&#xA;&lt;script defer src=&#34;https://JaeSim.github.io/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xD;&#xA;&lt;script defer src=&#34;https://JaeSim.github.io/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xD;&#xA;  \(s\)&#xD;&#xA;&lt;/span&gt;&#xD;&#xA;로부터 &lt;span&gt;&#xD;&#xA;  \(s&#39;\)&#xD;&#xA;&lt;/span&gt;&#xD;&#xA; 으로 변경하는 transition probability 를 하는 수식은 다음과 같다.&#xA;&lt;span&gt;&#xD;&#xA;  \[&#xD;&#xA;\mathcal{P}_{ss&#39;}  = \mathbb{P}[S_{t+1} = s&#39;\mid S_t = s]&#xD;&#xA;\]&#xD;&#xA;&lt;/span&gt;&#xD;&#xA;&#xA;매트릭스로 표현하면 다음과 같다. (합은 1)&#xA;&lt;span&gt;&#xD;&#xA;  \[&#xD;&#xA;\mathcal{P} =  \left[&#xD;&#xA;\begin{array}{ccc}&#xD;&#xA;\mathcal{P}_{11} &amp; \cdots &amp; \mathcal{P}_{1n} \\&#xD;&#xA;\vdots &amp; \ddots &amp; \vdots \\&#xD;&#xA;\mathcal{P}_{n1} &amp; \cdots &amp; \mathcal{P}_{nn}&#xD;&#xA;\end{array}&#xD;&#xA;\right]&#xD;&#xA;\]&#xD;&#xA;&lt;/span&gt;&#xD;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>3. Planning by Dynamic Programming</title>
      <link>https://JaeSim.github.io/reinforcement-learning/rl-dp/</link>
      <pubDate>Thu, 29 May 2025 12:21:05 +0900</pubDate>
      <guid>https://JaeSim.github.io/reinforcement-learning/rl-dp/</guid>
      <description>&lt;h1 id=&#34;3-planning-by-dynamic-programming&#34;&gt;&#xD;&#xA;  &lt;strong&gt;3. Planning by Dynamic Programming&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#3-planning-by-dynamic-programming&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h1&gt;&#xD;&#xA;&lt;p&gt;MDP를 푸는 방식들이 여러 방법이 있다.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Policy evaluation, Policy iteration, Value iteration&lt;/strong&gt; 등이 있고, 이것들은 &lt;strong&gt;환경을 정확하게 안다면(=모델을 안다면)&lt;/strong&gt; DP가 적용이 가능하다&lt;/p&gt;&#xA;&lt;p&gt;먼저 간략하게 언급하자면&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;policy iteration&lt;/strong&gt; : policy를 평가하고 iteration 하면서 발전해나가는 방식과  (policy evaluation + policy improvement)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;value iteration&lt;/strong&gt; : value function을 iteration하면서 옵티멀을 찾아가는 방법&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;이 있다.&lt;/p&gt;&#xA;&lt;p&gt;이번 섹션은 DP로 known MDP를 푸는 방법에 대한것이고, 이것은 강화학습의 flow 와 수식 간의 이해를 위한 섹션이다.&#xA;4장부터 unknown MDP를 푸는 방법이 기술되어 있다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>temp. 4. Model Free Prediction</title>
      <link>https://JaeSim.github.io/reinforcement-learning/rl-model-free-prediction/</link>
      <pubDate>Thu, 12 Jun 2025 10:57:55 +0900</pubDate>
      <guid>https://JaeSim.github.io/reinforcement-learning/rl-model-free-prediction/</guid>
      <description>&lt;h1 id=&#34;4-model-free-prediction&#34;&gt;&#xD;&#xA;  &lt;strong&gt;4. model-free prediction&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#4-model-free-prediction&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h1&gt;&#xD;&#xA;&lt;blockquote class=&#34;book-hint warning&#34;&gt;&#xD;&#xA;  // NOTE: 이 페이지는 임시로 작성되었습니다.&#xD;&#xA;&lt;/blockquote&gt;&#xD;&#xA;&lt;p&gt;3장 DP에 있는것처럼, Model-free prediction 하고, Model-free control 하는 순서로 진행된다.&lt;/p&gt;&#xA;&lt;p&gt;episode : 에이전트가 시작 상태에서 행동을 시작해서, 어떤 종료 조건(End state)에 도달할 때까지의 전체 과정&lt;/p&gt;&#xA;&lt;p&gt;MC와 TD는 major model-free algo.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Monte Carlo (MC)&lt;/strong&gt; : 한 에피소드가 끝날 때까지 기다린 후, 그 전체 리턴 값을 이용하여 value function을 업데이트&lt;br&gt;&#xA;MC는 &lt;strong&gt;하나의 에피소드 전체&lt;/strong&gt;(시작 ~ 종료)를 관찰한 뒤, &lt;br&gt;&#xA;실제로 받은 reward들을 기반으로 학습합니다. &lt;br&gt;&#xA;환경 모델 없이, 경험만으로 value function이나 policy를 추정합니다. &lt;br&gt;&#xA;Monte-Carlo policy evaluation uses &lt;em&gt;empirical mean&lt;/em&gt; return&#xA;instead of expected return &lt;br&gt;&#xA;높은 variance와 zero bias를 가짐&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;즉, 가장 손쉬운 방법으로 epside를 돌려보고 가능서들의 mean 값으로 처리 &lt;br&gt;&#xA;방문할때마다 횟수와 토탈 return을 늘리고, 이것의 평균을 통해 value function을 estimate한다.&#xA;lecture-4, 7 page&#xA;&#xD;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://JaeSim.github.io/katex/katex.min.css&#34; /&gt;&#xD;&#xA;&lt;script defer src=&#34;https://JaeSim.github.io/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xD;&#xA;&lt;script defer src=&#34;https://JaeSim.github.io/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xD;&#xA;  \[&#xD;&#xA;V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)} \left(G_t - V(S_t)\right)&#xD;&#xA;\]&#xD;&#xA;&lt;/span&gt;&#xD;&#xA;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
