[{"id":0,"href":"/development/secondpost/","title":"ë‘ë²ˆì§¸ ê¸€ì…ë‹ˆë‹¤. ë¸”ë¡œê·¸ ì‹¤í—˜ìš© í…ŒìŠ¤íŠ¸ ê¸€ì…ë‹ˆë‹¤.","section":"Development / ê°œë°œ","content":"\rtest\r#\rtest2\r#\rRL ê´€ë ¨ í•­ëª©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•œ post ì…ë‹ˆë‹¤.\ní˜ì´ì§€ ì—…ë¡œë“œ í™•ì¸ìš© ì„ì‹œ í¬ìŠ¤íŠ¸\n"},{"id":1,"href":"/aboutme/","title":"About Me","section":"Home","content":"\rProfile\r#\rName : Jaeyoung Sim\n"},{"id":2,"href":"/development/firstpost/","title":"í• ì¼","section":"Development / ê°œë°œ","content":"ê°œë°œ ê´€ë ¨ í•­ëª©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•œ post ì…ë‹ˆë‹¤.\nTodo List:\nìƒíƒœ ë‚ ì§œ ì‘ì—… ë‚´ìš© âœ… 2025-05-21 ë¸”ë¡œê·¸ GitHub Page ì—°ë™ âœ… 2025-05-22 RL ê´€ë ¨ Post ë‚¨ê¸°ê¸° (on-going) âœ… 2025-05-22 hugo + github page + giscus âœ… 2025-05-22 Google ì„œì¹˜ ì—°ë™ â˜ Naver ì„œì¹˜ ì—°ë™ âœ… 2025-05-22 ë¸”ë¡œê·¸ setup ê´€ë ¨ post ë‚¨ê¸°ê¸° â˜ About Me ì™„ì„± â˜ Google ì„œì¹˜ ì—°ë™ "},{"id":3,"href":"/development/blogsetup/","title":"Github pages ë¥¼ ì´ìš©í•œ blog Setting (hugo + hugo-book theme + giscus","section":"Development / ê°œë°œ","content":"\rGithub pages ë¥¼ ì´ìš©í•œ blog Setting (hugo + hugo-book theme + giscus\r#\r0. blog host ë°©ì‹ ì„ íƒ\r#\rì—¬ëŸ¬ ë¸”ë¡œê·¸ hostë°©ì‹ì„ ê³ ë ¤í•˜ì˜€ìœ¼ë‚˜, ìœ ì§€ë³´ìˆ˜ê°€ ì†ì´ ëœê°€ë©°, ì˜¤ë«ë™ì•ˆ hostingì´ ë˜ëŠ”ê²ƒì´ ìš°ì„  ìˆœìœ„ì˜€ê³ .\nGithub pagesë¥¼ í†µí•œ í˜¸ìŠ¤íŒ… ë°©ë²•ì„ ì„ íƒí•˜ì˜€ë‹¤.\nê·¸ë ‡ë‹¤ë©´, static pageë¥¼ generationì„ í•´ì£¼ëŠ” frameworkë¡œ jekyll ì™€ hugoë¥¼ ê³ ë¯¼í•˜ì˜€ê³  ruby ë³´ë‹¤ goë¡œ ì´ë£¨ì–´ì§„ hugoë¥¼ ë¯¿ê¸°ë¡œ í•˜ì˜€ë‹¤.\në‹¤ë¥¸ blogë“¤ì´ ìƒì„±í•œ í˜ì´ì§€ë¥¼ ì£¼ë¡œ ì°¸ì¡°í•˜ì˜€ìœ¼ë©°, ì´ í¬ìŠ¤íŒ…ì€ í•´ë‹¹ ê¸€ë“¤ì˜ ì—®ìŒì— ë¶ˆê³¼í•œì ì„ ì°¸ì¡° ë¶€íƒí•œë‹¤.\n1. Setup git, hugo\r#\rê°œë°œí™˜ê²½ì€ window 11 (Intel processor) ì´ë¯€ë¡œ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆë‹¤.\ngit ì„¤ì¹˜\r#\rhttps://git-scm.com/downloads ì‚¬ì´íŠ¸ì—ì„œ Git for Windows/x64 Setup. í•­ëª©ì„ í´ë¦­í•˜ì—¬ githubì„ ì„¤ì¹˜í•˜ì˜€ë‹¤. ê° ì„¤ì •ì€ defaultë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤.\nhugo ì„¤ì¹˜\r#\rhttps://github.com/gohugoio/hugo/releases ì—ì„œ ë³¸ì¸ì— ë§ëŠ” ìµœì‹  ë²„ì „ì„ ë°›ëŠ”ë‹¤. (hugo_extended_0.147.4_windows-amd64.zip)\ní›„ì— ì–¸ê¸‰í•˜ê² ì§€ë§Œ hugo-book theme ì˜ ê²½ìš° hugp-extended ë²„ì „ì„ ë°›ì•„ì•¼í–ˆë‹¤.\nhugo ëª…ë ¹ì–´ë¥¼ ì¹˜ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ, í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì˜€ë‹¤. (ë˜ë„ë¡ path íŒŒì‹±ì— ì—ëŸ¬ê°€ ì—†ë„ë¡ í´ë”ë“¤ì„ ì˜ì–´ë¡œ êµ¬ì„±í•˜ì˜€ë‹¤.)\ngithub repository ìƒì„±\r#\rhttps://minyeamer.github.io/blog/hugo-blog-1/ ì—ì„œ ì–¸ê¸‰ëœê²ƒ ê°™ì´ í•˜ë‚˜ì˜ repositoryë¥¼ branchë¡œ ë‚˜ëˆ„ì–´ì„œ submoduleë¡œ í™œìš©í•˜ëŠ” ë°©ì•ˆì„ ì±„íƒí–ˆë‹¤.\n\u0026lt;USERNAME\u0026gt;.github.io ì¸ repositoryë¥¼ í•˜ë‚˜ ìƒì„±í•œë‹¤. UserNameì´ Jon ì´ë¼ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìƒì„±ë  ê²ƒì´ë‹¤. Jon/Jon.github.io\nhugoë¥¼ í†µí•œ ê¸°ë³¸ ë¼ˆëŒ€ ë§Œë“¤ê¸°. ì—¬ê¸°ì„œ í´ë”ëª…ì„ ìƒì„±í•œ repositoryë¡œ ë§ì¶˜ë‹¤.\nhugo new site \u0026lt;USERNAME\u0026gt;.github.io cd \u0026lt;USERNAME\u0026gt;.github.io git init git add . git commit -m \u0026#34;feat: new site\u0026#34; git branch -M main git remote add origin https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git git push -u origin main branch ë§Œë“¤ê¸° git branch gh-pages main git checkout gh-pages git push origin gh-pages git checkout main gh-pagesë¥¼ submoudleë¡œ ì—°ë™í•˜ê¸° rm -rf public git submodule add -b gh-pages https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public git add public git add .gitmodules git commit -m \u0026#34;feat: add submodule for github pages\u0026#34; git push hugo Theme : hugo-book\r#\rhttps://themes.gohugo.io/ ì—ì„œ ì—¬ëŸ¬ í…Œë§ˆê°€ ì„ íƒì´ ê°€ëŠ¥í•˜ê³ , ê°€ë²¼ì›Œë³´ì´ëŠ” hugo-book theme ë¥¼ ì±„íƒí•˜ì˜€ë‹¤\nhttps://github.com/alex-shpak/hugo-book ì— Read.mdë¥¼ ì°¸ì¡°í•˜ì˜€ë‹¤.\ngit submodule add https://github.com/alex-shpak/hugo-book themes/hugo-book git add . git commit -m \u0026#34;feat: import hugo theme\u0026#34; root path ì— ìˆëŠ” hugo.toml íŒŒì¼ì— ì•„ë˜ ë‚´ìš©ì„ ì‚½ì…í•œë‹¤.\nbaseURL = \u0026#39;https://\u0026lt;USERNAME\u0026gt;.github.io\u0026#39;\rlanguageCode = \u0026#39;ko-kr\u0026#39;\rtitle = \u0026#34;\u0026lt; title what you want\u0026gt;\u0026#34;\rtheme = \u0026#39;hugo-book\u0026#39; Github ì„¤ì •\r#\rGithub -\u0026gt; .github.io -\u0026gt; Settings -\u0026gt; Pages -\u0026gt; Branch -\u0026gt; gh-pages ë¡œ ë³€ê²½\n2. ë¸”ë¡œê·¸ ë°°í¬ ë°©ë²•\r#\rhugo static page ìƒì„±\r#\rì•„ë˜ ëª…ë ¹ì–´ë¡œ \u0026lt;root-path\u0026gt;/content/ pathì— firstPost.md íŒŒì¼ì´ ìƒì„±ëœë‹¤\nhugo new firstPost.md ì•„ë˜ ì»¤ë§¨ë“œë¥¼ ì´ìš©í•˜ë©´ í˜„ì¬ static í˜ì´ì§€ë¥¼ 127.0.0.1:1313 ì—ì„œ í™•ì¸ì´ ê°€ëŠ¥í•˜ë‹¤\nhugo server -D ì•„ë˜ì™€ ê°™ì´ draftê°€ trueê°€ ìˆë‹¤ë©´ ë””ë²„ê¹…ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ìµœì¢… ì‚°ì¶œë¬¼ì—ì„œ ë¹ ì§€ê²Œ ëœë‹¤. ë”°ë¼ì„œ ë‚˜ì¤‘ì— ë¹¼ë†“ì§€ ë§ê³  draft = true ë¬¸êµ¬ë¥¼ ì§€ìš°ë˜ falseë¡œ ë³€ê²½í•œë‹¤\n// firstPost.md\r+++\rtitle = \u0026#39;firstPost\u0026#39;\rdraft = true\r+++ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ í†µí•´ì„œ public/ í´ë” ë°‘ì— ì‚°ì¶œë¬¼ë“¤ì„ ìƒì„±í•œë‹¤.\nhugo ì•„ë˜ ëª…ë ¹ì–´ë¥¼ í†µí•´ì„œ github repositoryì— ì‚°ì¶œë¬¼ë“¤ì„ ë°°í¬í•œë‹¤. ì´ ì‚°ì¶œë¬¼ë“¤ì€ gitub action ì— ì˜í•´ì„œ ìë™ìœ¼ë¡œ hostingë˜ë„ë¡ ì²˜ë¦¬ëœë‹¤.\ncd public git add . git commit -m \u0026#34;\u0026lt;comment what you want to add\u0026gt;\u0026#34; git push origin gh-pages cd .. git add . git commit -m \u0026#34;\u0026lt;comment what you want to add\u0026gt;\u0026#34; git push origin main 3. ì¶”ê°€ hugo ì„¸íŒ…íŒ\r#\rrecent-post ìš© home ë§Œë“¤ê¸°\r#\rì•„ë˜ì™€ ê°™ì´ í´ë” ë° ê¸°ë³¸ md íŒŒì¼ì„ êµ¬ì„±í•˜ì˜€ê³ .\ncontent\râ”œâ”€â”€ _index.md\râ”œâ”€â”€ Content-Category1\râ”‚ â”œâ”€â”€ _index.md\râ”‚ â””â”€â”€ article1.md\râ””â”€â”€ Content-Category2\râ”œâ”€â”€ _index.md\râ””â”€â”€ post1.md conent/_index.md íŒŒì¼ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•˜ì˜€ë‹¤.\n\u0026ldquo;recent-posts\u0026rdquo; ì—ì„œ \u0026quot; ì„ ì‚­ì œ\r--- title: \u0026#34;Home\u0026#34; comments: false --- # **Recent Posts** {{ {{ \u0026#34;recent-posts\u0026#34; }} }} layouts/shortcodes/recent-posts.html ì„ ìƒì„±í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ì‘ì„±\n\u0026lt;style\u0026gt; .summary-box { background: #f8f9fa; padding: 1rem; border-radius: 8px; transition: background 0.3s ease; max-height: 14rem; /* ë†’ì´ ì œí•œ */ overflow: hidden; /* ë„˜ì¹˜ëŠ” ë‚´ìš© ìˆ¨ê¹€ */ display: -webkit-box; -webkit-line-clamp: 6; /* ìµœëŒ€ ì¤„ ìˆ˜ (ì˜ˆ: 6ì¤„) */ -webkit-box-orient: vertical; text-overflow: ellipsis; /* ... ì²˜ë¦¬ */ } .summary-box h1, .summary-box h2, .summary-box h3, .summary-box h4, .summary-box h5, .summary-box h6, .summary-box p { margin: 0 0 1rem 0; } .summary-box:hover { background: #e9ecef; } \u0026lt;/style\u0026gt; \u0026lt;ul\u0026gt; {{ range (first 5 (sort .Site.RegularPages \u0026#34;Date\u0026#34; \u0026#34;desc\u0026#34;)) }} \u0026lt;li style=\u0026#34;margin-bottom: 2rem;\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt; \u0026lt;strong style=\u0026#34;font-size: 1.25rem;\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/strong\u0026gt; \u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; {{ with .Params.subtitle }} \u0026lt;small style=\u0026#34;color: #888;\u0026#34;\u0026gt;{{ . }}\u0026lt;/small\u0026gt;\u0026lt;br/\u0026gt; {{ end }} \u0026lt;small\u0026gt;{{ .Date.Format \u0026#34;2006-01-02 15:04:05 MST\u0026#34; }}\u0026lt;/small\u0026gt; \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34; style=\u0026#34;text-decoration: none; color: inherit;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;summary-box\u0026#34;\u0026gt; \u0026lt;!-- delete anchor in title--\u0026gt; {{ .Summary | replaceRE \u0026#34;\u0026lt;a class=\\\u0026#34;anchor\\\u0026#34; href=\\\u0026#34;#.*?\\\u0026#34;\u0026gt;#\u0026lt;/a\u0026gt;\u0026#34; \u0026#34;\u0026#34; | safeHTML | truncate 400}} \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; MarkDown code block copy ë²„íŠ¼ ë§Œë“¤ê¸°\r#\rlayouts/partials/docs/body.html ì„ ìƒì„±í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ì‘ì„±\n\u0026lt;script\u0026gt; document.addEventListener(\u0026#39;DOMContentLoaded\u0026#39;, () =\u0026gt; { document.querySelectorAll(\u0026#39;pre \u0026gt; code[class^=\u0026#34;language-\u0026#34;]\u0026#39;).forEach(codeBlock =\u0026gt; { const pre = codeBlock.parentElement; // ì½”ë“œë¸”ëŸ­ì˜ ë³µì‚¬ ë²„íŠ¼ ìƒì„± const button = document.createElement(\u0026#39;button\u0026#39;); button.innerText = \u0026#39;ğŸ“‹\u0026#39;; button.title = \u0026#39;Copy code\u0026#39;; button.style = ` position: absolute; top: 0.5em; right: 0.5em; padding: 2px 6px; font-size: 0.8rem; background: #f5f5f5; border: 1px solid #ccc; border-radius: 4px; cursor: pointer; z-index: 10; `; // ë³µì‚¬ ë™ì‘ ì •ì˜ button.addEventListener(\u0026#39;click\u0026#39;, () =\u0026gt; { navigator.clipboard.writeText(codeBlock.innerText); button.innerText = \u0026#39;âœ”\u0026#39;; setTimeout(() =\u0026gt; button.innerText = \u0026#39;ğŸ“‹\u0026#39;, 1000); }); // ìŠ¤íƒ€ì¼ ì ìš© pre.style.position = \u0026#39;relative\u0026#39;; pre.appendChild(button); }); }); \u0026lt;/script\u0026gt; 4. giscus ì—°ë™í•˜ê¸°\r#\rgiscus githubì— ì„¤ì¹˜ ë° discuss í™œì„±í™”\r#\rhttps://github.com/apps/giscus ì—ì„œ giscus install\ngithub page repository ë§Œ ì„ íƒ\nSetting -\u0026gt; Discusss í™œì„±í™”\nDiscussions ì—ì„œ ì¢Œì¸¡ Categories ì„ íƒí•˜ì—¬ ìƒˆ ì¹´í…Œê³ ë¦¬ ë§Œë“¤ê¸°\nCategory Name ì™€ Descriptionì„ ê°ê° ì…ë ¥í•˜ê³  Discussion Format ì„ Announcement ì„ íƒ\nhugo ì—ì„œ ë³´ì´ê²Œ ì„¤ì •\r#\rhttps://giscus.app/ko í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬\n/.github.io ë¡œ ì €ì¥ì†Œ ì…ë ¥ Discussion ì œëª©ì´ í˜ì´ì§€ ê²½ë¡œ í¬í•¨ ì„ íƒ í˜ì´ì§€ì— ìƒì„±ëœ scriptë¥¼ ë³µì‚¬ (ì•„ë˜ì™€ ìœ ì‚¬í•œ í˜•ì‹) \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026lt;.github.io\u0026#34; data-repo-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-category=\u0026#34;Comment\u0026#34; data-category-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-mapping=\u0026#34;pathname\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;preferred_color_scheme\u0026#34; data-lang=\u0026#34;ko\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; ì´ë¥¼ `layouts/partials/comments.htmlì— ì•„ë˜ì²˜ëŸ¼ ë¶™ì—¬ë„£ê¸° {{ if and (.IsPage) (not .IsHome) (not (eq .Params.comments false)) }} \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026lt;.github.io\u0026#34; data-repo-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-category=\u0026#34;Comment\u0026#34; data-category-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-mapping=\u0026#34;pathname\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;preferred_color_scheme\u0026#34; data-lang=\u0026#34;ko\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; {{ end }} comments = falseë¥¼ ë„£ì€ í˜ì´ì§€ë“¤ì€ github ì½”ë©˜íŠ¸ê°€ ì•ˆë³´ì„\r5. tag ì„¤ì •\r#\rhugo.toml ì— ì•„ë˜ ì„¤ì • ì‚½ì…\n[taxonomies] tag = \u0026#34;tags\u0026#34; category = \u0026#34;categories\u0026#34; ê° í¬ìŠ¤íŠ¸ë§ˆë‹¤ ì•„ë˜ front matterì— ì•„ë˜ í•­ëª©ì„ ê¸°ì…\n+++ ... tags = [\u0026#34;Definition\u0026#34;, \u0026#34;Essential\u0026#34;] categories = [\u0026#34;Reinforcement Learning\u0026#34;] ... +++ ì´ ë¸”ë¡œê·¸ì˜ ê²½ìš°, ìš°ì¸¡ ToC í•­ëª© í•˜ë‹¨ì— ìœ„ì¹˜ Tag ë° Categoriesë¥¼ ìœ„ì¹˜ì‹œí‚´\nì´ëŠ” í…Œë§ˆë§ˆë‹¤ ë‹¤ë¥´ë‹ˆ ê° í…Œë§ˆë³„ ì ìš© ë°©ë²• í™•ì¸ í•„ìš”\nlayouts/partials/doc/inject/toc-after.html ì„ ìƒì„±\n{{ with .Params.tags }} \u0026lt;div class=\u0026#34;post-tags\u0026#34;\u0026gt; \u0026lt;strong\u0026gt;Tags:\u0026lt;/strong\u0026gt; {{ range . }} \u0026lt;a href=\u0026#34;{{ \u0026#34;tags/\u0026#34; | relLangURL }}{{ . | urlize }}\u0026#34; class=\u0026#34;tag\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} {{ with .Params.categories }} \u0026lt;div class=\u0026#34;post-categories\u0026#34;\u0026gt; \u0026lt;strong\u0026gt;Categories:\u0026lt;/strong\u0026gt; {{ range . }} \u0026lt;a href=\u0026#34;{{ \u0026#34;categories/\u0026#34; | relLangURL }}{{ . | urlize }}\u0026#34; class=\u0026#34;category\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} ì¹´í…Œê³ ë¦¬ ë° tag ì„ íƒí–ˆì„ë•Œ í•´ë‹¹ listê°€ ë³´ì´ë„ë¡ ì„¤ì • layouts/_default/term.html ì„ ìƒì„±\n{{ define \u0026#34;main\u0026#34; }} \u0026lt;main\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; {{ range .Pages }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt; {{ .Date }}\u0026lt;/li\u0026gt; {{ else }} \u0026lt;li\u0026gt;\u0026lt;em\u0026gt;No posts found.\u0026lt;/em\u0026gt;\u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; \u0026lt;/main\u0026gt; {{ end }} 6. ê²€ìƒ‰ ì—°ë™\r#\rêµ¬êµ´ ì„œì¹˜ ì½˜ì†”\r#\rì†Œìœ ê¶Œ í™•ì¸ https://search.google.com/search-console/about ì ‘ì†í•˜ì—¬ url \u0026lt;USERNAME\u0026gt;.github.io ì„ ì…ë ¥ google_.html ì„ ë‹¤ìš´ë¡œë“œ í›„ì— public/ ì— ìœ„ì¹˜ì‹œí‚´\nsitemap ë§Œë“¤ê¸° hugo.toml ì— ì•„ë˜ì™€ ê°™ì´ ì‚½ì… -\u0026gt; sitemap.xmlì´ ìƒì„±ëœê²ƒì„ í™•ì¸ enableRobotsTXT = true [sitemap] # always, hourly daily, weekly, monthly, yearly, never changefreq = \u0026#34;weekly\u0026#34; filename = \u0026#34;sitemap.xml\u0026#34; priority = 0.5 layouts/robots.txt íŒŒì¼ì„ ìˆ˜ì • User-agent: * Allow: / Sitemap: {{ .Site.BaseURL }}sitemap.xml hugo.toml ì—ì„œ ì•„ë˜ ì„¤ì •\nenableRobotsTXT = true ì•„ë˜ ì»¤ë§¨ë“œë¡œ ë¹Œë“œì‹œì— public/robots.txt ìƒì„±ì„ í™•ì¸\nhugo sitemap.xmlì„ ì„œì¹˜ ì½˜ì†”ì— ë“±ë¡ ë„¤ì´ë²„ ì„œì¹˜ ì–´ë“œë°”ì´ì ¸\r#\rnaver search advisor ì ‘ì† blog url ì…ë ¥ html ë‹¤ìš´ë¡œë“œí›„ public ì•ˆì— ìœ„ì¹˜ì‹œí‚¤ê³  ë°°í¬ -\u0026gt; ì†Œìœ ê¶Œ í™•ì¸ ì°¸ì¡°\r#\rhttps://ialy1595.github.io/post/blog-construct-1/\nhttps://ialy1595.github.io/post/blog-construct-2/\nhttps://minyeamer.github.io/blog/hugo-blog-1/\nhttps://d5br5.dev/blog/nextjs_blog/giscus\nhttps://velog.io/@eona1301/Github-Blog-%EA%B2%80%EC%83%89%EC%B0%BD-%EB%85%B8%EC%B6%9C%EC%8B%9C%ED%82%A4%EA%B8%B0\nhttps://golangkorea.github.io/post/hugo-intro/taxonomy-basic/\nhttps://boyinblue.github.io/002_github_blog/003_naver_search_advisor.html\n"},{"id":4,"href":"/reinforcement-learning/rl-essential/","title":"1. Reinforcement Learning Essential","section":"Reinforcement Learning / ê°•í™”í•™ìŠµ","content":"\r1. ê°•í™”í•™ìŠµì— ëŒ€í•œ ê¸°ì´ˆ ë‚´ìš©\r#\rë“¤ì–´ê°€ê¸° ì•ì„œ\r#\rê°•í™”í•™ìŠµì˜ ê¸°ì´ˆì ì¸ ë‚´ìš©ì„ í•™ìŠµí•œ ë’¤ ì •ë¦¬í•œ ê²ƒìœ¼ë¡œ, ë‚¨ë“¤ì—ê²Œ ë³´ì—¬ì£¼ê¸° ë³´ë‹¤ëŠ” ë³¸ì¸ì˜ ì´í•´ì™€ ê¸°ì–µì„ ìœ„í•´ì„œ ê¸°ìˆ í•œ ê²ƒì…ë‹ˆë‹¤.\në‚˜ë§Œì˜ ë°©ì‹ìœ¼ë¡œ ì´í•´í•œ ê²ƒì´ê¸° ë•Œë¬¸ì—, ì£¼ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” ë¶€ë¶„ì´ ë‹¤ë¥¼ìˆ˜ ìˆìœ¼ë©° ìƒëµë˜ê±°ë‚˜ ë†“ì¹œ ë¶€ë¶„ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤.\nê°•í™”í•™ìŠµ(RL: Reinforcement Learning) ì´ë€?\r#\rDefinition\r#\râ€œReinforcement learning is learning what to doâ€”how to map situations to actionsâ€”so as to maximize a numerical reward signal.â€\nâ€” Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction (2nd ed), p.1\nsituationsì„ stateë¡œ í‘œê¸°í•˜ì—¬\ní†µìƒì ìœ¼ë¡œ action, stateì™€ reward ê°€ RLì˜ í•µì‹¬ ìš”ì†Œì´ë‹¤.\nìš”ì•½í•˜ë©´, ì£¼ì–´ì§„ ìƒíƒœ(State)ì—ì„œ ë³´ìƒ(Reward)ì„ ìµœëŒ€í™” í•  ìˆ˜ ìˆëŠ” í–‰ë™(Action)ì„ í•™ìŠµí•˜ëŠ” ê²ƒ\nì´ëŠ” Reward Hypothesisë¥¼ ê¸°ë°˜ìœ¼ë¡œí•œë‹¤.\nReward Hypothesis\nAll goals can be described by the maximisation of expected cumulative reward\nRewardsëŠ” Scalar feedback signalì´ë‹¤ AgentëŠ” ë¯¸ë˜ì— ê¸°ëŒ€ë˜ëŠ” cumulative rewardê°€ ìµœëŒ€í™” ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ í•œë‹¤\nState and MDP\r#\rEnvrionment State, Agent State ê°€ ìˆê³  ê°ê° Envrionment, Agentê´€ì ì—ì„œì˜ ìˆ˜ì‹ì /ë‚´ë¶€ì  í‘œí˜„ì´ë‹¤.\nEnvrionment StateëŠ” Envrionment ê´€ì ì—ì„œ ë‹¤ìŒ stepì—ì„œ Envrionmentê°€ ì–´ë–»ê²Œ ë³€í™”í• ì§€ë¥¼(ì–´ë–¤ Stateë¡œ ë³€í™”í• ì§€) ë‚˜íƒ€ë‚¸ë‹¤. Envrionmentì—ì„œëŠ” microsecond ì—ì„œë„ ìˆ˜ë§ì€ ì •ë³´ê°€ ì˜¤ê¸° ë•Œë¬¸ì— ë¶ˆí•„ìš”í•œ ì •ë³´ë“¤ë„ ë§ë‹¤. Envrionment StateëŠ” ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ë§Œë“œëŠ”ë° ìœ ìš©í•˜ì§„ ì•Šë‹¤. ì™œëƒí•˜ë©´ Agentì˜ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸. (ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ì€ Agentì— ìˆì„í…Œë‹ˆ ë¼ê³  ì´í•´í•¨)\nAgent StateëŠ” ë‹¤ìŒ step ì—ì„œ Agentê°€ ì–´ë–¤ í–‰ë™ì„ ì„ íƒí• ì§€ë¥¼ ë‚˜íƒ€ë‚¸ ìˆ˜ì‹/í‘œí˜„ì´ë‹¤.\nInformation StateëŠ” ê³¼ê±° historyë¶€í„° ëª¨ë“  ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•œ ìˆ˜í•™ì  ì •ì˜ë¥¼ ê°€ì§„ Stateì´ë‹¤. ì£¼ë¡œ Markov Stateë¼ ë¶€ë¥¸ë‹¤. (Markov ì†ì„±ì„ ë§Œì¡±í•œë‹¤)\nMarkov Properties : ì´ì „ì˜ ëª¨ë“  ìŠ¤í…Œì´íŠ¸ì •ë³´ë¥¼ ì´ìš©í•´ì„œ ë‹¤ìŒ Stateë¥¼ ì„ íƒí•˜ëŠ”ê²ƒì´, í˜„ì¬Stateë§Œ ë³´ê³  í•˜ëŠ”ê²ƒê³¼ ê°™ë‹¤ \\[\r\\mathbb{P}[S_{t+1} \\mid S_t] = \\mathbb{P}[S_{t+1} \\mid S_1, \\dots, S_t]\r\\]\rì´ë¥¼ ì´ìš©í•˜ë©´, ë¯¸ë˜(future) ëŠ” ê³¼ê±°ì— ë¬´ì—‡ì´ ì£¼ì–´ì¡Œë“ ì§€ ë…ë¦½ì ì´ë‹¤. (=The future is independent of the past given the present) ë‹¬ë¦¬ë§í•˜ë©´, í˜„ì¬ \\(S_t\\)\rë§Œ ì €ì¥í•´ë„ ëœë‹¤ \\[\rH_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\r\\]\rí˜„ì¬ì˜ Stateê°€ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì´ë¯¸ ë‹´ê³  ìˆë‹¤ê³ ë„ ë³¼ ìˆ˜ ì‡ë‹¤.\nAppendix\rhistory ëŠ” Observationê³¼ actions, rewardsì˜ ì—°ì†ì´ë‹¤ \\[\r\\quad \\quad H_t = O_1, R_1, A_1, ..., A_{tâˆ’1}, O_t, R_t\r\\]\rState ëŠ” ë‹¤ìŒ actionì„ ê²°ì •í•˜ê¸° ìœ„í•œ ì •ë³´ì´ë‹¤ \\[\r\\quad \\quad S_t = f(H_t)\r\\\\\r\\]\rì´ì „ stateëŠ” \\[\r\\mathbb{P}[S_{t+1} \\mid S_t] = \\mathbb{P}[S_{t+1} \\mid S_1, \\dots, S_t]\r\\]\rThe future is independent of the past given the present ëª¨ë“  ì´ì „ Stateë¥¼ ì•Œì§€ ì•Šì•„ë„ ì§ì „ Stateë§Œ ë³´ê³  ê²°ì • í•  ìˆ˜ ìˆë‹¤ \\[\rH_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\r\\]\rFully Observable Envrionments ëŠ” Agentê°€ envrionmentì—ì„œ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ë°”ë¡œ ê´€ì¸¡ì´ ê°€ëŠ¥í•¨ì„ ë‚˜íƒ€ë‚´ê³ , ê²°ê³¼ì ìœ¼ë¡œ Envrionment State = Information State = Agent State ìƒíƒœì´ë‹¤. ì´ë¥¼ Markov Desicion Process(MDP) ë¼ê³  í•œë‹¤\nPartial Observabable Envrionments ëŠ” ì¢€ë” í˜„ì‹¤ì ì¸ í™˜ê²½. ë¡œë´‡ì´ ì¹´ë©”ë¼ë¥¼ í†µí•´ì„œ í™”ë©´ì„ ë³´ì§€ë§Œ í˜„ì¬ ìê¸°ì˜ ìœ„ì¹˜ë¥¼ ëª¨ë¥´ëŠ” ê²ƒì²˜ëŸ¼. ì¦‰, Agent State \\( \\ne \\)\rEnvironment State ì´ë‹¤. Partially Observable Markov Decision Process(POMDP) ë¡œ ìˆ˜ì‹ì´ í‘œí˜„ëœë‹¤.\nìš°ë¦¬ëŠ” í™˜ê²½ì— ëŒ€í•´ì„œ ì˜ ëª¨ë¥´ì§€ë§Œ\nì´ì „ Historyë¥¼ ì´ìš©í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ìˆê³ , \\[S_t^a = H_t\\]\rProbability ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•ì´ ìˆê³ , \\[S_t^a = \\left( \\mathbb{P}[S_t^e = s_1], \\dots, \\mathbb{P}[S_t^e = s_n] \\right)\\]\rìˆœí™˜ì‹ ê²½ë§(Recurrent neural network) ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•ë„ ìˆë‹¤. \\[S_t^a = \\sigma(S_{t-1}^a W_s + O_t W_o)\\]\rPolicy, Value Function, Model\r#\rRLì€ ì•„ë˜ componentsë¥¼ í•œê°œ ì´ìƒ í¬í•¨í•œë‹¤.\nPolicyëŠ” Agentê°€ ì–´ë–»ê²Œ Actionì„ ì„ íƒí•˜ëŠ”ì§€(=behavior function) ì´ë‹¤.\nStateë¡œë¶€í„° function íŒŒì´ë¥¼ ì´ìš©í•´ì„œ(policy) ë¥¼ í†µí•´ actionì„ ê²°ì •í•œë‹¤.\nDeterministic policy: \\( a = \\pi(s)\\)\rprobabilityë¡œ policyë¥¼ í‘œí˜„í•˜ê³ ì í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\nStochastic policy: \\( \\pi(a \\mid s) = \\mathbb{P}[A_t = a \\mid S_t = s]\\)\rValue Functionì€ Stateë‚˜ Actionì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€(ê¸°ëŒ€ë˜ëŠ” ë¯¸ë˜ì˜ rewardê°€ ì–¼ë§ˆì¼ì§€ ì˜ˆì¸¡) ì„ ë‚˜íƒ€ë‚¸ë‹¤.\nState oneê³¼ State two, action oneê³¼ action twoë¥¼ ì„ íƒí• ë•Œ ìµœì¢… rewardê°€ ë” ì¢‹ì€ìª½ìœ¼ë¡œ ì„ íƒí•œë‹¤.\nì•„ë˜ì™€ ê°™ì´ í‘œí˜„ë˜ëŠ”ë° \\( R_{t+1}, R_{t+2}, R_{t+3} \\)\rë¥¼ ë”í•˜ëŠ” ê²ƒê³¼ ê°™ì´ ë‹¤ìŒ(ë¯¸ë˜)ì˜ rewardì˜ í•©ì˜ ê¸°ëŒ€ê°’ \\[\rv_{\\pi}(s) = \\mathbb{E}_{\\pi} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s \\right]\r\\]\rgamma(\r\\( \\gamma\\)\r) ëŠ” ë‹¤ìŒ ìŠ¤íƒ­ì— ëŒ€í•œ discounting factor (ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ê²ƒì´ë‹ˆ ë„£ì€ ë³€ìˆ˜ë¡œ ì´í•´í•¨)\nModelì€ Agentê´€ì ì—ì„œ Envrionmentê°€ ì–´ë–»ê²Œ ë™ì‘í• ì§€ ìƒê°í•˜ëŠ” ê²ƒ ì„ ë‚˜íƒ€ë‚¸ë‹¤.\ntransitions model, rewards model ì „í†µì ìœ¼ë¡œ ë‘ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤\ntranstions ëª¨ë¸ì€ directly ë‹¤ìŒ stateë¥¼ ì˜ˆì¸¡í•œë‹¤. \\[\\mathcal{P}_{ss'}^a = \\mathbb{P}[S_{t+1} = s' \\mid S_t = s, A_t = a]\\]\rRewards ëª¨ë¸ì€ rewardë¥¼ ì˜ˆì¸¡í•œë‹¤. \\[\\mathcal{R}_s^a = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]\\]\rRL agentì˜ ë¶„ë¥˜\r#\rì–´ë–¤ key componentë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ì— ë”°ë¼ì„œ RLì„ ë¶„ë¥˜í•œë‹¤\nvalue-based RLì€ value functionì„ ê°€ì§€ê³  ìˆë‹¤. policy-based RLì€ policyë¥¼ ê°€ì§€ê³  ìˆë‹¤. actor-criticì€ value functionê³¼ policyë¥¼ ê°€ì§€ê³  ì‡ë‹¤. Model baseë¡œ êµ¬ë¶„í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.\nmodel freeëŠ” ëª¨ë¸ì´ ì—†ì§€ë§Œ(=í™˜ê²½ì— ëŒ€í•œ representationì´ ì—†ì§€ë§Œ) value function + policyë¡œ êµ¬ì„±ëœ RL model basedëŠ” value function + policy, model ì´ ì¡´ì¬ Sequential decision makingì˜ ë‘ê°€ì§€\r#\rReinforcement Learningì€ í™˜ê²½(Environment)ì„ ëª¨ë¥´ê³  ìƒí˜¸ì‘ìš©í•˜ë©´ì„œ rewardê°€ ìµœëŒ€ê°€ ë˜ë„ë¡ í•™ìŠµ Planningì€ í™˜ê²½ì„ ì•Œê³ (í™˜ê²½ì— í•´ë‹¹í•˜ëŠ” modelì„ ì£¼ê³ ) agentê°€ ê³„ì‚°í•˜ëŠ” ê²ƒ Exploration and Exploitation\r#\rExploration ì™€ Exploitation ëŠ” trade-off\nExploration ì€ í™˜ê²½ì— ëŒ€í•œ ì •ë³´ë¥¼ ë” ì°¾ëŠ”ê²ƒ Exploitation ì€ ì•Œê³  ìˆëŠ” ì •ë³´ë¥¼ í™œìš©í•´ì„œ rewardë¥¼ ìµœëŒ€í™” í•˜ëŠ”ê²ƒ ì˜ˆ) ìƒˆë¡œìš´ ë ˆìŠ¤í† ë‘ ì°¾ê¸° vs ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ë ˆìŠ¤í† ë‘ ì¬ë°©ë¬¸\në¨¸ì‹ ëŸ¬ë‹(ML)ê³¼ ë”¥ëŸ¬ë‹(DL)ê³¼ì˜ ê´€ê³„\r#\rë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì¸ê³µì§€ëŠ¥(AI)ì˜ ê°œë…ìœ¼ë¡œì¨, í•™ìŠµì„ í†µí•´ ì˜ˆì¸¡(ë˜ëŠ” ë¶„ë¥˜)ë¥¼ í•˜ëŠ” ê²ƒ\në”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ê°œë…ìœ¼ë¡œì¨, ì¸ê³µì‹ ê²½ë§(Neural Network)ë¥¼ ì´ìš©í•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒ\nê°•í™” í•™ìŠµì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œê°ˆë˜ë¡œì¨, ë³´ìƒì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œ í–‰ë™ í•™ìŠµí•˜ëŠ” ê²ƒ\nAI\râ”œâ”€â”€ Machine Learning\râ”‚ â”œâ”€â”€ Supervised / Unsupervised\râ”‚ â”œâ”€â”€ Reinforcement Learning\râ”‚ â””â”€â”€ Deep Learning\râ”‚ â””â”€â”€ Deep Reinforcement Learning (e.g., DQN, PPO) ì°¸ê³ \r#\rDavid Silver ì˜ RL ê°•ì¢Œ https://davidstarsilver.wordpress.com/teaching\ní•œê¸€ ìœ íŠœë¸Œ + ë¸”ë¡œê·¸ https://smj990203.tistory.com/2\n"},{"id":5,"href":"/reinforcement-learning/rl-mdp/","title":"2. Markov Decision Process","section":"Reinforcement Learning / ê°•í™”í•™ìŠµ","content":"\r2. Markov Decision Process\r#\rMarkov Process(MP) ë€?\r#\rMP ì†ì„±\r#\rMDPëŠ” í™˜ê²½ì— ëŒ€í•´ì„œ Reinforcement Learningì´ ì´í•´ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì‹í™”í•œë‹¤\nê±°ì˜ ëª¨ë“  RL ê´€ë ¨ ë¬¸ì œë“¤ì€ MDPë¡œ ìˆ˜ì‹í™” í•  ìˆ˜ ìˆë‹¤(Fully observableì´ë‚˜ Partially ovservableì´ë‚˜)\nMarkov Propertyë¥¼ ì´ìš©í•˜ëŠ”ë°, \u0026ndash;ì´ì „ ê°•ì˜ì°¸ì¡°\u0026ndash;\nìš”ì•½í•˜ë©´, í˜„ì¬ stateë§Œìœ¼ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•´ë„ ëœë‹¤ëŠ” ì†ì„±ì´ë‹¤. (ë‹¤ë¥´ê²Œ ë§í•˜ë©´, í˜„ì¬ stateê°€ ì´ë¯¸ ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤. memoryless)\nMarkov state \\(s\\)\rë¡œë¶€í„° \\(s'\\)\rìœ¼ë¡œ ë³€ê²½í•˜ëŠ” transition probability ë¥¼ í•˜ëŠ” ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\r\\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s'\\mid S_t = s]\r\\]\rë§¤íŠ¸ë¦­ìŠ¤ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. (í•©ì€ 1) \\[\r\\mathcal{P} = \\left[\r\\begin{array}{ccc}\r\\mathcal{P}_{11} \u0026 \\cdots \u0026 \\mathcal{P}_{1n} \\\\\r\\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r\\mathcal{P}_{n1} \u0026 \\cdots \u0026 \\mathcal{P}_{nn}\r\\end{array}\r\\right]\r\\]\rMPëŠ” tupleë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.\nA Markov Process (or Markov Chain) is a tuple \\(\\langle \\mathcal{S}, \\mathcal{P} \\rangle \\)\r\\(\\mathcal{S}\\)\ris a (finite) set of states \\(\\mathcal{P}\\)\ris a state transition probability matrix,\n\\(\\mathcal{P}_{ss'} = \\mathbb{P} \\left[ S_{t+1} = s' \\mid S_t = s \\right]\\)\rì´ ì˜ˆì œëŠ” ë‹¨ìˆœí™”í•œ state ë³€í™”ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ê³ , MDPë¥¼ ì´ìš©í•œ ì‹¤ì œ ì‚¬ìš©ì€ í›¨ì‹ ë” ë§ì€ stateì™€ probabilityë¥¼ í¬í•¨í•œë‹¤.\nMarkov Reward Process(MRP) ë€?\r#\rMRP ì†ì„±\r#\rrewardê°€ ì¶”ê°€ê°€ ëœ ê²ƒ. MP ì— value judgmentê°€ í¬í•¨ëœ ê²ƒ - ì—¬ê¸°ì„œ judgment ëŠ” ëˆ„ì  rewardê°€ ì–¼ë§ˆë‚˜ ì¢‹ì•„ì§ˆì§€\nA Markov Rewards Process (or Markov Chain) is a tuple \\(\\langle \\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma \\rangle \\)\r\\(\\mathcal{S}\\)\ris a (finite) set of states \\(\\mathcal{P}\\)\ris a state transition probability matrix,\n\\(\\mathcal{P}_{ss'} = \\mathbb{P} \\left[ S_{t+1} = s' \\mid S_t = s \\right]\\)\r\\(\\mathcal{R}\\)\ris a reward function, \\(\\mathcal{R}_s = \\mathbb{E} \\left[ R_{t+1} \\mid S_t = s \\right]\\)\r\\(\\gamma\\)\ris a discount factor, \\({\\gamma \\in [0, 1]}\\)\rtimestep tì— ëŒ€í•œ goal ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤. ê°ë§ˆëŠ” ë¯¸ë˜ì— ëŒ€í•œ discount factorì´ë‹¤. ì´ê²ƒì´ í•„ìš”í•œ ì´ìœ ëŠ”\nìš°ë¦¬ëŠ” í¼íŒ©í•œ ëª¨ë¸ì´ ì—†ê¸° ë•Œë¬¸ì— ìˆ˜í•™ì  max ë°”ìš´ë“œ(ìˆ˜í•™ì  í¸ì˜ì„±ì„ ìœ„í•´) MPì˜ ë¬´í•œ ë£¨í”„ë¥¼ í•˜ì§€ í”¼í•˜ê¸° ìœ„í•´ ê·¼ì ‘ ë¯¸ë˜ì˜ ê°€ì¹˜ê°€ ë¹„ê·¼ì ‘(ë¨¼ë¯¸ë˜) ë¯¸ë˜ì˜ ê°€ì¹˜ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ ì‹œí€€ìŠ¤ì˜ ëì´ ë³´ì¥ëœë‹¤ë©´ discount factorë¥¼ ì•ˆì“¸ìˆ˜ë„ ìˆë‹¤ \\[\rG_t = R_{t+1} + \\gamma R_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\r\\]\rValue Function\r#\rí˜„ì¬ìƒíƒœ(s)ì—ì„œì˜ terminated ìƒíƒœì—ì„œì˜ Expected return ì´ê²ƒì€ Expectation ì´ë‹¤. ì™œëƒí•˜ë©´ environmentëŠ” stochastic ì´ë‹ˆê¹Œ \\[\rv(s) = \\mathbb{E} [ G_t \\mid S_t = s ]\r\\]\rì´ëŠ” ë°¸ë§ë°©ì •ì‹ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.\nBellman Equation for MRP\r#\rValue Functionì€ í¬ê²Œ ë‘ê°€ì§€ ì»´í¬ë„ŒíŠ¸ë¡œ ë‚˜ëˆŒìˆ˜ ìˆë‹¤.\ní˜„ì¬ì˜ ë¦¬ì›Œë“œ \\( R_(t+1)\\)\rë‹¤ìŒê³„ìŠ¹stateì˜ discounted ìƒíƒœ \\(\\gamma v(S_{t+1})\\)\r\\[\r\\begin{aligned}\rv(s) \u0026= \\mathbb{E} \\left[ G_t \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma \\left( R_{t+2} + \\gamma R_{t+3} + \\cdots \\right) \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma G_{t+1} \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s \\right]\r\\end{aligned}\r\\]\r\\[\rv(s) = \\mathbb{E} \\left[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s \\right]\r\\]\r\\[\rv(s) = \\mathcal{R}_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'} v(s')\r\\]\rì´ë¥¼ ë²¡í„° ë§¤íŠ¸ë¦­ìŠ¤ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\\[\r\\begin{bmatrix}\rv(1) \\\\\r\\vdots \\\\\rv(n)\r\\end{bmatrix}\r=\r\\begin{bmatrix}\r\\mathcal{R}_1 \\\\\r\\vdots \\\\\r\\mathcal{R}_n\r\\end{bmatrix}\r+\r\\gamma\r\\begin{bmatrix}\r\\mathcal{P}_{11} \u0026 \\cdots \u0026 \\mathcal{P}_{1n} \\\\\r\\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r\\mathcal{P}_{n1} \u0026 \\cdots \u0026 \\mathcal{P}_{nn}\r\\end{bmatrix}\r\\begin{bmatrix}\rv(1) \\\\\r\\vdots \\\\\rv(n)\r\\end{bmatrix}\r\\]\rMarkov Decision Process(MDP) ë€?\r#\rA Markov Decision Process (or Markov Chain) is a tuple \\(\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma \\rangle \\)\r\\(\\mathcal{S}\\)\ris a (finite) set of states \\(\\mathcal{A}\\)\ris a (finite) set of actions \\(\\mathcal{P}\\)\ris a state transition probability matrix,\n\\(\\mathcal{P}^a_{ss'} = \\mathbb{P} \\left[ S_{t+1} = s' \\mid S_t = s, A_t = a \\right]\\)\r\\(\\mathcal{R}\\)\ris a reward function, \\(\\mathcal{R}^a_s = \\mathbb{E} \\left[ R_{t+1} \\mid S_t = s , A_t =a \\right]\\)\r\\(\\gamma\\)\ris a discount factor, \\({\\gamma \\in [0, 1]}\\)\rPolicy\r#\rì •ì±…(Policy \\(\\pi\\)\r) ëŠ” ì£¼ì–´ì§„ stateì— ëŒ€í•œ actionì˜ ë¶„í¬ \\[\r\\pi(a \\mid s) = \\mathbb{P}[A_t = a \\mid S_t = s]\r\\]\rë§ˆí¬ë¡œí”„ ì†ì„±ì— ì˜í•´ì„œ í˜„ì¬ stateëŠ” rewardë¥¼ fully characterize í•œê²ƒì´ê¸° ë•Œë¬¸ì— ìˆ˜ì‹ì— rewardê°€ ì—†ë‹¤\nstate ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ í´ë¦¬ì‹œë¥¼ ë„£ìœ¼ë©´ Markov Processì´ê³ , state ì‹œí€€ì— ë¦¬ì›Œë“œë¥¼ ë„£ìœ¼ë©´ Markov reward process ì´ë‹¤.\në§ˆë¥´ì½”í”„ reward process ì— ëŒ€í•´ì„œ MDP ìˆ˜ì‹ìœ¼ë¡œ (actionìœ¼ë¡œë¶€í„°) ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤. (MDP ìˆ˜ì‹ìœ¼ë¡œ(policy-actionì´ í¬í•¨ëœ ë²„ì „ìœ¼ë¡œ) MPì™€ MRPë¥¼ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤)\n\\[\r\\mathcal{P}^\\pi_{s, s'} = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathcal{P}^{a}_{s s'} \\\\\r\\mathcal{R}^\\pi_s = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathcal{R}^a_s\r\\]\rì´ëŠ” ëª¨ë“  actionì— ê°ˆìˆ˜ ìˆëŠ” probë¥¼ averageë¡œ(íŒŒì´ëŠ” 0~1ì˜ ê°’ì´ë¯€ë¡œ) ì´í•´ë¥¼ ì‰½ê²Œí•˜ê¸° Pì™€ Rì„ í‘œí˜„í•œ ê²ƒ.\nMDPì— ëŒ€í•œ value functionì€ stage-value í‘ì…˜ê³¼, action-value function ë‘ê°€ì§€ ë°©ì‹ì´ ìˆë‹¤.\nstate-value function\r#\rë‹¤ìŒê³¼ ê°™ê³  ì´ëŠ” í˜„ì¬ stateì¼ë•Œ pi í´ë¦¬ì‹œë¥¼ ë”°ë¥¼ë•Œ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. (ì–¼ë§ˆë‚˜ ë¦¬ì›Œë“œë¥¼ ì–»ì„ì§€) \\[\rv_\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s \\right]\r\\]\rì—¬ê¸°ì„¸ E_piëŠ” ëª¨ë“  ìƒ˜í”Œì•¡ì…˜ì— ëŒ€í•œ expectation\naction-value function\r#\rì´ë¥¼ action-value function q_pi ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” í˜„ì¬ stateì—ì„œ ì–´ë–¤ë–¤actionì„ ì„ íƒí–ˆì„ë•Œ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. (ì–¼ë§ˆë‚˜ ë¦¬ì›Œë“œë¥¼ ì–»ì„ì§€) \\[\rq_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s, A_t = a \\right]\r\\]\rstate-value functionê³¼ action-value functionì˜ Bellman Expectation ë°©ì •ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤. \\[\rv_\\pi(s) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s \\right]\r\\]\r\\[\rq_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a \\right]\r\\]\rstate-value-function ì™€ action-value function ì¤‘ ì–´ë–¤ ê²ƒì„ ì¤‘ì ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ”ì§€ì— ë”°ë¥¸ í•™ìŠµë°©ë²•ì´ ë‹¬ë¼ì§€ëŠ” ê²ƒ ê°™ë‹¤. ì™¼ìª½ì€ state-value í‘ì…˜ê´€ì ì—ì„œì˜ ê·¸ë¦¼ê³¼ ìˆ˜ì‹í‘œí˜„ì´ê³ , ì˜¤ë¥¸ìª½ì€ action-value í‘ì…˜ê´€ì ì—ì„œ ìˆ˜ì‹ê³¼ í‘œí˜„ì´ë‹¤. action-value í‘ì…˜ì— ëŒ€í•´ì„œëŠ”, actionì„ ì„ íƒí•¨ìœ¼ë¡œì¨ rewardë¥¼ í†µí•´ì„œ state-value í‘ì…˜ìœ¼ë¡œ ë‹¤ì‹œ ë„˜ì–´ê°€ëŠ” ê²ƒì„ ë³¼ìˆ˜ ìˆë‹¤.\nì´ë‘ê°œì˜ ê·¸ë˜í”„ë¥¼ í•©ì¹˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ì™¼ìª½ì€ stae-value í‘ì…˜ ê´€ì ì—ì„œì˜ ìˆ˜ì‹ì´ê³ , ì˜¤ë¥¸ìª½ì€ action-value í‘ì…˜ ê´€ì ì—ì„œì˜ ìˆ˜ì‹ì´ë‹¤.\nì´ê²ƒë“¤ì€ ì•ì—ì„œ ì–¸ê¸‰í–ˆë˜ê²ƒì²˜ëŸ¼ ë‘ê°€ì§€ íŒŒíŠ¸ë¡œ ë‚˜ëˆŒìˆ˜ ìˆê³ , (ì´ë²ˆ ìŠ¤í…ì—ì„œì˜ ë¦¬ì›Œë“œì™€ ë¯¸ë˜ì˜ value function ë¦¬í„´ê°’ê°’) ë‹¤ìŒ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì§„ë‹¤ (maxtrix-form). ì¶”ê°€ë¡œ ëª¨ë“  MDPëŠ” MRPë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤. \\[\rv_\\pi = \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_\\pi\r\\]\r\\[\rv_\\pi = \\left( I - \\gamma \\mathcal{P}^\\pi \\right)^{-1} \\mathcal{R}^\\pi\r\\]\rìš°ë¦¬ëŠ” ì´ë¡œë¶€í„°(state-value, action-value functionìœ¼ë¡œë¶€í„°) Optimal Value Function ì„ ì°¾ëŠ”ë‹¤\noptimal value function\r#\rmdpì—ì„œì˜ ìµœì  í–‰ë™ì„ ì°¾ëŠ” ë°©ë²•ì€ optimal state-value function v_(s) ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì€ ëª¨ë“  policy ì— ëŒ€í•´ì„œ value functionì„ ìµœëŒ€í™” í•˜ëŠ”ê²ƒì´ë‹¤. optimal action-value function q_(s,a) ì˜ ê²½ìš° ì•„ë˜ì™€ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\\[\rv_*(s) = \\max_\\pi v_\\pi(s)\r\\]\r\\[\rq_*(s, a) = \\max_\\pi q_\\pi(s, a)\r\\]\roptimal policy ëŠ” q_* ë¥¼ ìµœëŒ€í™” í•¨ìœ¼ë¡œì¨ ì–»ì„ ìˆ˜ ìˆë‹¤. \\[\r\\pi_*(a \\mid s) = \\begin{cases}\r1 \u0026 \\text{if } a = \\arg\\max\\limits_{a \\in \\mathcal{A}} q_*(s, a) \\\\\r0 \u0026 \\text{otherwise}\r\\end{cases}\r\\]\rì˜µí‹°ë©€í•œ í•´ëŠ” ìœ„ì— êµ¬í•œ ë„ì‹ê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì•„ë˜ ëª¨í˜•ê³¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\nSolving Bellman Optimality Equation\r#\rBellman Optimality Equationì€ non-linear í•˜ê³  ë³´í†µ No closed form ìœ¼ë¡œ ì œê³µë¨. ì•„ë˜ì™€ ê°™ì€ solving method ë“¤ì´ ìˆìŒ\nvalue iteration : Iteratively updates value estimates using the Bellman optimality equation until convergence. policy interation : Alternates between policy evaluation and policy improvement until the policy becomes stable. Q-learning : Off-policy method that directly learns the optimal action-value function from experience. Sarsa(State-Action-Reward-State-Action) : On-policy method that updates action-values based on the action actually taken by the current policy. Extensions of MDPs\r#\rinfinite and continous MDPs ë¬´í•œí•œ ê¸°ì¡´ ë°©ë²•ì„ ê·¸ëŒ€ë¡œ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤(Straightforward). ì—°ì†ì ì¸ ìˆ«ìë¼ë©´ í¸ë¯¸ë¶„ í•´ì•¼í•œë‹¤. Partially observable MDPs (finite sets of Observation:O ì™€ Observation function:Z) ì¶”ê°€ìš”ì†Œê°€ ìˆìœ¼ë©°. ìƒíƒœë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ìœ¼ë‹ˆ ê´€ì¸¡ê°’ìœ¼ë¡œë¶€í„° ì¶”ì •í•˜ëŠ” hidden stageê°€ ìˆëŠ” MDPë¡œ í•´ê²° Undiscounted, average reward MDPs ergodic markvo process ë¡œ ì²˜ë¦¬í• ìˆ˜ ìˆë‹¤. ergodicì€ Recuurent: ê° stateëŠ” ë¬´í•œ ì‹œê°„ ë™ì•ˆì— ë°©ë¬¸, Aperiodic : ì–´ë–¤ ì£¼ê¸°ì„± ì—†ì´ ë°©ë¬¸ í•˜ëŠ” ì†ì„±ì„ ê°€ì§€ê³  ìˆë‹¤.ì´ê²ƒì€ average reward MDPì´ë‹¤ (discount ë˜ì§€ ì•Šìœ¼ë‹ˆ, í°ìˆ˜ì˜ ë²•ì¹™ì— ì˜í•´ì„œ). ë”°ëŸ¬ì„œ average bellman equation ì„ í’€ë©´ ëœë‹¤. ì°¸ì¡°\r#\rhttps://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-2-mdp.pdf\nhttps://www.youtube.com/watch?v=lfHX2hHRMVQ\nhttps://trivia-starage.tistory.com/280\n"},{"id":6,"href":"/reinforcement-learning/rl-dp/","title":"temp : 3. Planning by Dynamic Programming","section":"Reinforcement Learning / ê°•í™”í•™ìŠµ","content":"\r3. Planning by Dynamic Programming\r#\r// NOTE: ì´ í˜ì´ì§€ëŠ” ì„ì‹œë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\rMDPë¥¼ í‘¸ëŠ” ë°©ì‹ë“¤ì´ ì—¬ëŸ¬ê°œê°€ ìˆë‹¤.\nPolicy evaluation, Policy iteration, value iteration ë“±ì´ ìˆê³ , ì´ê²ƒë“¤ì€ DPê°€ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤.\nDynamic Programming(DP) ì´ë€?\r#\rì •ì˜ : // ì´ë¶€ë¶„ì€ ìƒëµ\nMDPëŠ” DPë¡œ ë¬¸ì œë¥¼ í’€ê¸°ì— í•„ìš”í•œ ì¡°ê±´ë“¤ì„ ë§Œì¡±í•œë‹¤.\në²¨ë§Œ ë°©ì •ì‹(Bellman equation) ì€ ì¬ê·€ì  decomposition value function ì€ ê°’ì„ ì €ì¥í•˜ê³ , ì¬ì‚¬ìš©í•œë‹¤. prediction\npolicy evaluation\r#\rí´ë¦¬ì‹œê°€ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ í‰ê°€(mdpë¡œ ì–¼ë§ˆë‚˜ ì–¼ë§ˆë‚˜ ë§ì€ rewardë¥¼ ì–»ì„ìˆ˜ ìˆëŠ”ì§€?)\npolicy iteration\r#\rí´ë¦¬ì‹œë¥¼ inner loopì—ì„œ í‰ê°€í•˜ë©´ì„œ policyê°€ betterí•˜ë„ë¡ í•˜ëŠ” ë°©ì‹ policy evaluation policy improvement\nvalue interation\r#\rbellan equationì„ í†µí•´ì„œ value functionì´ better í•˜ë„ë¡ í•˜ëŠ” ë°©ì‹\nì •ì±… ë°˜ë³µë³´ë‹¤ ê³„ì‚°ëŸ‰ì´ ì ê³ , ìˆ˜ë ´ ì†ë„ê°€ ë¹ ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nSynchronous Dynamic Programing\r#\rì•„ë˜ ë„í‘œì™€ ê°™ì´ ì²˜ë¦¬í•˜ë©´ ëœë‹¤ Asynchronous Dynamic Programing\r#\rì •ì˜: ëª¨ë“  ìƒíƒœë¥¼ ë™ì‹œì— ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šê³ , ì¼ë¶€ ìƒíƒœë§Œ ì„ íƒì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì¥ì : ê³„ì‚° íš¨ìœ¨ì„±ì´ ë†’ì•„ì§€ê³ , íŠ¹ì • ìƒíƒœì— ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nasynchronous dynamic programmingì„ í•˜ê¸°ìœ„í•œ 3ê°€ì§€ ideaë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤\nIn-place dynamic programming Prioritised sweeping Real-time dynamic programming 4. model-free prediction/control\r#\repisode : ì—ì´ì „íŠ¸ê°€ ì‹œì‘ ìƒíƒœì—ì„œ í–‰ë™ì„ ì‹œì‘í•´ì„œ, ì–´ë–¤ ì¢…ë£Œ ì¡°ê±´(End state)ì— ë„ë‹¬í•  ë•Œê¹Œì§€ì˜ ì „ì²´ ê³¼ì •\nMonte Carlo : í•œ ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° í›„, ê·¸ ì „ì²´ ë¦¬í„´ ê°’ì„ ì´ìš©í•˜ì—¬ valueë¥¼ ì—…ë°ì´íŠ¸\nTemporal Difference : ì—í”¼ì†Œë“œê°€ ëë‚˜ì§€ ì•Šì•„ë„, ë‹¤ìŒ ìƒíƒœì˜ í˜„ì¬ ì¶”ì • ê°’ì„ ì‚¬ìš©í•´ ë°”ë¡œ ì—…ë°ì´íŠ¸\nMonte-Carlo Reinforcement Learning ì€ model-free ì´ë‹¤. ì™œëƒí•˜ë©´ MDP Transition ì— ëŒ€í•œ (rewardì— ëŒ€í•œ) ì§€ì‹ì´ ì—†ê¸° ë•Œë¬¸.\nTemporal-Difference Learning ì€ model-free\non-policy/off-policy\nÏ€ = Target Policy Âµ = Behavior Policy\nOff-policy learning : â€œLook over someoneâ€™s shoulderâ€ Learn about policy Ï€ from experience sampled from Âµ re-use experience generated from old policy Q-Learning : e-greedy ë°©ì‹ìœ¼ë¡œ íƒí—˜í•˜ì§€ë§Œ í•™ìŠµì—ëŠ” ë°˜ì˜ ì•ˆí• ìˆ˜ ìˆìŒ(ìµœì ì˜ í–‰ë™ë§Œ ì—…ë°ì´íŠ¸)\nOn-policy learning : â€œLearn on the jobâ€ Learn about policy Ï€ from experience sampled from Ï€\nSalsa :on-policy Q-learning. í˜„ì¬ í–‰ë™ì„ ê·¸ëŒ€ë¡œ ë”°ë¼ê°€ë©° í•™ìŠµ\nì°¸ì¡°\r#\rhttps://wnthqmffhrm.tistory.com/10\nhttps://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-5-model-free-control-.pdf\n"}]