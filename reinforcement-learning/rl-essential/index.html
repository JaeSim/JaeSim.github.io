<!DOCTYPE html>
<html lang="ko-kr" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  ê°•í™”í•™ìŠµì— ëŒ€í•œ ê¸°ì´ˆ ë‚´ìš©
  #


  ë“¤ì–´ê°€ê¸° ì•ì„œ
  #

ê°•í™”í•™ìŠµì˜ ê¸°ì´ˆì ì¸ ë‚´ìš©ì„ í•™ìŠµí•œ ë’¤ ì •ë¦¬í•œ ê²ƒìœ¼ë¡œ,
ë‚¨ë“¤ì—ê²Œ ë³´ì—¬ì£¼ê¸° ë³´ë‹¤ëŠ” ë³¸ì¸ì˜ ì´í•´ì™€ ê¸°ì–µì„ ìœ„í•´ì„œ ê¸°ìˆ í•œ ê²ƒì…ë‹ˆë‹¤.
ë‚˜ë§Œì˜ ë°©ì‹ìœ¼ë¡œ ì´í•´í•œ ê²ƒì´ê¸° ë•Œë¬¸ì—, ì£¼ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” ë¶€ë¶„ì´ ë‹¤ë¥¼ìˆ˜ ìˆìœ¼ë©° ìƒëµë˜ê±°ë‚˜ ë†“ì¹œ ë¶€ë¶„ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤.

  ê°•í™”í•™ìŠµ(RL: Reinforcement Learning) ì´ë€?
  #


  Definition
  #


â€œReinforcement learning is learning what to doâ€”how to map situations to actionsâ€”so as to maximize a numerical reward signal.â€
â€” Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction (2nd ed), p.1">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://JaeSim.github.io/reinforcement-learning/rl-essential/">
  <meta property="og:site_name" content="JaeSim&#39;s Workspace">
  <meta property="og:title" content="1. Reinforcement Learning Essential">
  <meta property="og:description" content="ê°•í™”í•™ìŠµì— ëŒ€í•œ ê¸°ì´ˆ ë‚´ìš©#ë“¤ì–´ê°€ê¸° ì•ì„œ#ê°•í™”í•™ìŠµì˜ ê¸°ì´ˆì ì¸ ë‚´ìš©ì„ í•™ìŠµí•œ ë’¤ ì •ë¦¬í•œ ê²ƒìœ¼ë¡œ, ë‚¨ë“¤ì—ê²Œ ë³´ì—¬ì£¼ê¸° ë³´ë‹¤ëŠ” ë³¸ì¸ì˜ ì´í•´ì™€ ê¸°ì–µì„ ìœ„í•´ì„œ ê¸°ìˆ í•œ ê²ƒì…ë‹ˆë‹¤.
ë‚˜ë§Œì˜ ë°©ì‹ìœ¼ë¡œ ì´í•´í•œ ê²ƒì´ê¸° ë•Œë¬¸ì—, ì£¼ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” ë¶€ë¶„ì´ ë‹¤ë¥¼ìˆ˜ ìˆìœ¼ë©° ìƒëµë˜ê±°ë‚˜ ë†“ì¹œ ë¶€ë¶„ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤.
ê°•í™”í•™ìŠµ(RL: Reinforcement Learning) ì´ë€?#Definition#â€œReinforcement learning is learning what to doâ€”how to map situations to actionsâ€”so as to maximize a numerical reward signal.â€
â€” Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction (2nd ed), p.1">
  <meta property="og:locale" content="ko_kr">
  <meta property="og:type" content="article">
    <meta property="article:section" content="reinforcement-learning">
    <meta property="article:published_time" content="2025-05-22T11:13:00+09:00">
    <meta property="article:modified_time" content="2025-05-22T11:13:00+09:00">
<title>1. Reinforcement Learning Essential | JaeSim&#39;s Workspace</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://JaeSim.github.io/reinforcement-learning/rl-essential/">
<link rel="stylesheet" href="/book.min.e9f68c3fff3d8236a489d16a9caf6de5e4d1a29c20eb4b5524e42cd30be4d319.css" integrity="sha256-6faMP/89gjakidFqnK9t5eTRopwg60tVJOQs0wvk0xk=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.986fbac5607514d6baad970ba26752bcc5fea171651e0f1a47b7ce7d3f51d3f2.js" integrity="sha256-mG&#43;6xWB1FNa6rZcLomdSvMX&#43;oXFlHg8aR7fOfT9R0/I=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>JaeSim&#39;s Workspace</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/aboutme/" class="">About Me</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>Development / ê°œë°œ</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/development/firstpost/" class="">ì²«ë²ˆì§¸ ê¸€ì…ë‹ˆë‹¤.</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>Reinforcement Learning / ê°•í™”í•™ìŠµ</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/secondpost/" class="">ë‘ë²ˆì§¸ ê¸€ì…ë‹ˆë‹¤. ë¸”ë¡œê·¸ ì‹¤í—˜ìš© í…ŒìŠ¤íŠ¸ ê¸€ì…ë‹ˆë‹¤.</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-essential/" class="active">1. Reinforcement Learning Essential</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>1. Reinforcement Learning Essential</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#ë“¤ì–´ê°€ê¸°-ì•ì„œ"><strong>ë“¤ì–´ê°€ê¸° ì•ì„œ</strong></a></li>
    <li><a href="#ê°•í™”í•™ìŠµrl-reinforcement-learning-ì´ë€"><strong>ê°•í™”í•™ìŠµ(RL: Reinforcement Learning) ì´ë€?</strong></a>
      <ul>
        <li><a href="#definition"><strong>Definition</strong></a></li>
        <li><a href="#state-and-mdp"><strong>State and MDP</strong></a></li>
        <li><a href="#policy-value-function-model"><strong>Policy, Value Function, Model</strong></a></li>
      </ul>
    </li>
    <li><a href="#ë¨¸ì‹ ëŸ¬ë‹mlê³¼-ë”¥ëŸ¬ë‹dlê³¼ì˜-ê´€ê³„"><strong>ë¨¸ì‹ ëŸ¬ë‹(ML)ê³¼ ë”¥ëŸ¬ë‹(DL)ê³¼ì˜ ê´€ê³„</strong></a></li>
    <li><a href="#ì°¸ê³ "><strong>ì°¸ê³ </strong></a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="ê°•í™”í•™ìŠµì—-ëŒ€í•œ-ê¸°ì´ˆ-ë‚´ìš©">
  <strong>ê°•í™”í•™ìŠµì— ëŒ€í•œ ê¸°ì´ˆ ë‚´ìš©</strong>
  <a class="anchor" href="#%ea%b0%95%ed%99%94%ed%95%99%ec%8a%b5%ec%97%90-%eb%8c%80%ed%95%9c-%ea%b8%b0%ec%b4%88-%eb%82%b4%ec%9a%a9">#</a>
</h1>
<h2 id="ë“¤ì–´ê°€ê¸°-ì•ì„œ">
  <strong>ë“¤ì–´ê°€ê¸° ì•ì„œ</strong>
  <a class="anchor" href="#%eb%93%a4%ec%96%b4%ea%b0%80%ea%b8%b0-%ec%95%9e%ec%84%9c">#</a>
</h2>
<p>ê°•í™”í•™ìŠµì˜ ê¸°ì´ˆì ì¸ ë‚´ìš©ì„ í•™ìŠµí•œ ë’¤ ì •ë¦¬í•œ ê²ƒìœ¼ë¡œ,
ë‚¨ë“¤ì—ê²Œ ë³´ì—¬ì£¼ê¸° ë³´ë‹¤ëŠ” ë³¸ì¸ì˜ ì´í•´ì™€ ê¸°ì–µì„ ìœ„í•´ì„œ ê¸°ìˆ í•œ ê²ƒì…ë‹ˆë‹¤.</p>
<p>ë‚˜ë§Œì˜ ë°©ì‹ìœ¼ë¡œ ì´í•´í•œ ê²ƒì´ê¸° ë•Œë¬¸ì—, ì£¼ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” ë¶€ë¶„ì´ ë‹¤ë¥¼ìˆ˜ ìˆìœ¼ë©° ìƒëµë˜ê±°ë‚˜ ë†“ì¹œ ë¶€ë¶„ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤.</p>
<h2 id="ê°•í™”í•™ìŠµrl-reinforcement-learning-ì´ë€">
  <strong>ê°•í™”í•™ìŠµ(RL: Reinforcement Learning) ì´ë€?</strong>
  <a class="anchor" href="#%ea%b0%95%ed%99%94%ed%95%99%ec%8a%b5rl-reinforcement-learning-%ec%9d%b4%eb%9e%80">#</a>
</h2>
<h3 id="definition">
  <strong>Definition</strong>
  <a class="anchor" href="#definition">#</a>
</h3>
<blockquote>
<p>â€œReinforcement learning is learning what to doâ€”how to map <strong>situations</strong> to <strong>actions</strong>â€”so as to maximize a numerical <strong>reward</strong> signal.â€<br>
â€” <em>Richard S. Sutton and Andrew G. Barto</em>, <em>Reinforcement Learning: An Introduction</em> (2nd ed), p.1</p></blockquote>
<p>situationsì„ stateë¡œ í‘œê¸°í•˜ì—¬</p>
<p>í†µìƒì ìœ¼ë¡œ action, stateì™€ reward ê°€ RLì˜ í•µì‹¬ ìš”ì†Œì´ë‹¤.</p>
<!-- ![figure](/images/rl-action-state-reward.png) -->
<img src="/images/rl-action-state-reward.png" alt="test" style="width:80%;" />
<p>ìš”ì•½í•˜ë©´,
<strong>ì£¼ì–´ì§„ ìƒíƒœ(State)ì—ì„œ ë³´ìƒ(Reward)ì„ ìµœëŒ€í™” í•  ìˆ˜ ìˆëŠ” í–‰ë™(Action)ì„ í•™ìŠµí•˜ëŠ” ê²ƒ</strong></p>
<p>ì´ëŠ” Reward Hypothesisë¥¼ ê¸°ë°˜ìœ¼ë¡œí•œë‹¤.</p>
<blockquote>
<p><strong>Reward Hypothesis</strong><br>
All goals can be described by the maximisation of expected cumulative reward</p></blockquote>
<p>RewardsëŠ” Scalar feedback signalì´ë‹¤
AgentëŠ” ë¯¸ë˜ì— ê¸°ëŒ€ë˜ëŠ” cumulative rewardê°€ ìµœëŒ€í™” ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ í•œë‹¤</p>
<h3 id="state-and-mdp">
  <strong>State and MDP</strong>
  <a class="anchor" href="#state-and-mdp">#</a>
</h3>
<p>Envrionment State, Agent State ê°€ ìˆê³ 
ê°ê° Envrionment, Agentê´€ì ì—ì„œì˜ ìˆ˜ì‹ì /ë‚´ë¶€ì  í‘œí˜„ì´ë‹¤.</p>
<p>Envrionment StateëŠ” Envrionment ê´€ì ì—ì„œ ë‹¤ìŒ stepì—ì„œ Envrionmentê°€ ì–´ë–»ê²Œ ë³€í™”í• ì§€ë¥¼(ì–´ë–¤ Stateë¡œ ë³€í™”í• ì§€) ë‚˜íƒ€ë‚¸ë‹¤.
Envrionmentì—ì„œëŠ” microsecond ì—ì„œë„ ìˆ˜ë§ì€ ì •ë³´ê°€ ì˜¤ê¸° ë•Œë¬¸ì— ë¶ˆí•„ìš”í•œ ì •ë³´ë“¤ë„ ë§ë‹¤.
Envrionment StateëŠ” ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ë§Œë“œëŠ”ë° ìœ ìš©í•˜ì§„ ì•Šë‹¤. ì™œëƒí•˜ë©´ Agentì˜ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸. (ìš°ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ì€ Agentì— ìˆì„í…Œë‹ˆ ë¼ê³  ì´í•´í•¨)</p>
<p>Agent StateëŠ” ë‹¤ìŒ step ì—ì„œ Agentê°€ ì–´ë–¤ í–‰ë™ì„ ì„ íƒí• ì§€ë¥¼ ë‚˜íƒ€ë‚¸ ìˆ˜ì‹/í‘œí˜„ì´ë‹¤.</p>
<p>Information StateëŠ” ê³¼ê±° historyë¶€í„° ëª¨ë“  ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•œ ìˆ˜í•™ì  ì •ì˜ë¥¼ ê°€ì§„ Stateì´ë‹¤. ì£¼ë¡œ Markov Stateë¼ ë¶€ë¥¸ë‹¤. (Markov ì†ì„±ì„ ë§Œì¡±í•œë‹¤)</p>
<ul>
<li>Markov Properties : <br>
ì´ì „ì˜ ëª¨ë“  ìŠ¤í…Œì´íŠ¸ì •ë³´ë¥¼ ì´ìš©í•´ì„œ ë‹¤ìŒ Stateë¥¼ ì„ íƒí•˜ëŠ”ê²ƒì´, í˜„ì¬Stateë§Œ ë³´ê³  í•˜ëŠ”ê²ƒê³¼ ê°™ë‹¤
<span>
  \[
\mathbb{P}[S_{t+1} \mid S_t] = \mathbb{P}[S_{t+1} \mid S_1, \dots, S_t]
\]
</span>
</li>
</ul>
<p>ì´ë¥¼ ì´ìš©í•˜ë©´, ë¯¸ë˜(future) ëŠ” ê³¼ê±°ì— ë¬´ì—‡ì´ ì£¼ì–´ì¡Œë“ ì§€ ë…ë¦½ì ì´ë‹¤.
(=The future is independent of the past given the present)
ë‹¬ë¦¬ë§í•˜ë©´, í˜„ì¬ <span>
  \(S_t\)
</span>
 ë§Œ ì €ì¥í•´ë„ ëœë‹¤
<span>
  \[
H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\infty}
\]
</span>

í˜„ì¬ì˜ Stateê°€ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì´ë¯¸ ë‹´ê³  ìˆë‹¤ê³ ë„ ë³¼ ìˆ˜ ì‡ë‹¤.</p>
<details ><summary>Appendix</summary>
  <div class="markdown-inner">
<ul>
<li>history ëŠ” Observationê³¼ actions, rewardsì˜ ì—°ì†ì´ë‹¤</li>
</ul>
<div style="text-align: center;">
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \[
\quad \quad H_t = O_1, R_1, A_1, ..., A_{tâˆ’1}, O_t, R_t
\]
</span>
</div>
<ul>
<li>State ëŠ” ë‹¤ìŒ actionì„ ê²°ì •í•˜ê¸° ìœ„í•œ ì •ë³´ì´ë‹¤</li>
</ul>
<div style="text-align: center;">
<span>
  \[
\quad \quad S_t = f(H_t)
\\
\]
</span>
</div>
<ul>
<li>ì´ì „ stateëŠ”</li>
</ul>
<div style="text-align: center;">
<span>
  \[
\mathbb{P}[S_{t+1} \mid S_t] = \mathbb{P}[S_{t+1} \mid S_1, \dots, S_t]
\]
</span>
</div>
<ul>
<li>The future is independent of the past given the present</li>
<li>ëª¨ë“  ì´ì „ Stateë¥¼ ì•Œì§€ ì•Šì•„ë„ ì§ì „ Stateë§Œ ë³´ê³  ê²°ì • í•  ìˆ˜ ìˆë‹¤</li>
</ul>
<div style="text-align: center;">
<span>
  \[
H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\infty}
\]
</span>
</div>
  </div>
</details>
<p><strong>Fully Observable Envrionments</strong> ëŠ” Agentê°€ envrionmentì—ì„œ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ë°”ë¡œ ê´€ì¸¡ì´ ê°€ëŠ¥í•¨ì„ ë‚˜íƒ€ë‚´ê³ ,
ê²°ê³¼ì ìœ¼ë¡œ Envrionment State = Information State = Agent State ìƒíƒœì´ë‹¤.
ì´ë¥¼ <strong>Markov Desicion Process(MDP)</strong> ë¼ê³  í•œë‹¤</p>
<p><strong>Partial Observabable Envrionments</strong> ëŠ” ì¢€ë” í˜„ì‹¤ì ì¸ í™˜ê²½. ë¡œë´‡ì´ ì¹´ë©”ë¼ë¥¼ í†µí•´ì„œ í™”ë©´ì„ ë³´ì§€ë§Œ í˜„ì¬ ìê¸°ì˜ ìœ„ì¹˜ë¥¼ ëª¨ë¥´ëŠ” ê²ƒì²˜ëŸ¼.
ì¦‰, Agent State <span>
  \( \ne \)
</span>
 Environment State ì´ë‹¤.
<strong>Partially Observable Markov Decision Process(POMDP)</strong> ë¡œ ìˆ˜ì‹ì´ í‘œí˜„ëœë‹¤.</p>
<p>ìš°ë¦¬ëŠ” í™˜ê²½ì— ëŒ€í•´ì„œ ì˜ ëª¨ë¥´ì§€ë§Œ</p>
<ul>
<li>ì´ì „ Historyë¥¼ ì´ìš©í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ìˆê³ ,
<span>
  \[S_t^a = H_t\]
</span>
</li>
<li>Probability ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•ì´ ìˆê³ ,
<span>
  \[S_t^a = \left( \mathbb{P}[S_t^e = s_1], \dots, \mathbb{P}[S_t^e = s_n] \right)\]
</span>
</li>
<li>ìˆœí™˜ì‹ ê²½ë§(Recurrent neural network) ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•ë„ ìˆë‹¤.
<span>
  \[S_t^a = \sigma(S_{t-1}^a W_s + O_t W_o)\]
</span>
</li>
</ul>
<h3 id="policy-value-function-model">
  <strong>Policy, Value Function, Model</strong>
  <a class="anchor" href="#policy-value-function-model">#</a>
</h3>
<p>RLì€ ì•„ë˜ componentsë¥¼ í•œê°œ ì´ìƒ í¬í•¨í•œë‹¤.</p>
<blockquote>
<p><strong>Policy</strong>ëŠ” <strong><em>Agentê°€ ì–´ë–»ê²Œ Actionì„ ì„ íƒí•˜ëŠ”ì§€(=behavior function)</em></strong> ì´ë‹¤.</p></blockquote>
<p>Stateë¡œë¶€í„° function íŒŒì´ë¥¼ ì´ìš©í•´ì„œ(policy) ë¥¼ í†µí•´ actionì„ ê²°ì •í•œë‹¤.</p>
<ul>
<li>Deterministic policy: <span>
  \( a = \pi(s)\)
</span>
</li>
</ul>
<p>probabilityë¡œ policyë¥¼ í‘œí˜„í•˜ê³ ì í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ul>
<li>Stochastic policy: <span>
  \( \pi(a \mid s) = \mathbb{P}[A_t = a \mid S_t = s]\)
</span>
</li>
</ul>
<blockquote>
<p><strong>Value Function</strong>ì€ <strong><em>Stateë‚˜ Actionì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€(ê¸°ëŒ€ë˜ëŠ” ë¯¸ë˜ì˜ rewardê°€ ì–¼ë§ˆì¼ì§€ ì˜ˆì¸¡)</em></strong> ì„ ë‚˜íƒ€ë‚¸ë‹¤.</p></blockquote>
<p>State oneê³¼ State two,  action oneê³¼ action twoë¥¼ ì„ íƒí• ë•Œ ìµœì¢… rewardê°€ ë” ì¢‹ì€ìª½ìœ¼ë¡œ ì„ íƒí•œë‹¤.</p>
<p>ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ë˜ëŠ”ë° <span>
  \( R_{t+1}, R_{t+2}, R_{t+3} \)
</span>
 ë¥¼ ë”í•˜ëŠ” ê²ƒê³¼ ê°™ì´ ë‹¤ìŒ(ë¯¸ë˜)ì˜ rewardì˜ í•©ì˜ ê¸°ëŒ€ê°’
<span>
  \[
v_{\pi}(s) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \mid S_t = s \right]
\]
</span>

gamma(<span>
  \( \gamma\)
</span>
) ëŠ” ë‹¤ìŒ ìŠ¤íƒ­ì— ëŒ€í•œ discounting factor (ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ê²ƒì´ë‹ˆ ë„£ì€ ë³€ìˆ˜ë¡œ ì´í•´í•¨)</p>
<blockquote>
<p><strong>Model</strong>ì€ <strong><em>Agentê´€ì ì—ì„œ Envrionmentê°€ ì–´ë–»ê²Œ ë™ì‘í• ì§€ ìƒê°í•˜ëŠ” ê²ƒ</em></strong> ì„ ë‚˜íƒ€ë‚¸ë‹¤.</p></blockquote>
<p>transitions model, rewards model ì „í†µì ìœ¼ë¡œ ë‘ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤</p>
<p>transtions ëª¨ë¸ì€ directly ë‹¤ìŒ stateë¥¼ ì˜ˆì¸¡í•œë‹¤.
<span>
  \[\mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1} = s' \mid S_t = s, A_t = a]\]
</span>

Rewards ëª¨ë¸ì€ rewardë¥¼ ì˜ˆì¸¡í•œë‹¤.
<span>
  \[\mathcal{R}_s^a = \mathbb{E}[R_{t+1} \mid S_t = s, A_t = a]\]
</span>
</p>
<h4 id="rl-agentì˜-ë¶„ë¥˜">
  RL agentì˜ ë¶„ë¥˜
  <a class="anchor" href="#rl-agent%ec%9d%98-%eb%b6%84%eb%a5%98">#</a>
</h4>
<img src="/images/rl-agent-taxonomy.png" alt="test" style="width:80%;" />
<p>ì–´ë–¤ key componentë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ì— ë”°ë¼ì„œ RLì„ ë¶„ë¥˜í•œë‹¤</p>
<ul>
<li>value-based RLì€ value functionì„ ê°€ì§€ê³  ìˆë‹¤.</li>
<li>policy-based RLì€ policyë¥¼ ê°€ì§€ê³  ìˆë‹¤.</li>
<li>actor-criticì€ value functionê³¼ policyë¥¼ ê°€ì§€ê³  ì‡ë‹¤.</li>
</ul>
<p>Model baseë¡œ êµ¬ë¶„í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.</p>
<ul>
<li>model freeëŠ” ëª¨ë¸ì´ ì—†ì§€ë§Œ(=í™˜ê²½ì— ëŒ€í•œ representationì´ ì—†ì§€ë§Œ) value function + policyë¡œ êµ¬ì„±ëœ RL</li>
<li>model basedëŠ” value function + policy, model ì´ ì¡´ì¬</li>
</ul>
<h4 id="sequential-decision-makingì˜-ë‘ê°€ì§€">
  Sequential decision makingì˜ ë‘ê°€ì§€
  <a class="anchor" href="#sequential-decision-making%ec%9d%98-%eb%91%90%ea%b0%80%ec%a7%80">#</a>
</h4>
<ul>
<li>Reinforcement Learningì€ í™˜ê²½(Environment)ì„ ëª¨ë¥´ê³  ìƒí˜¸ì‘ìš©í•˜ë©´ì„œ rewardê°€ ìµœëŒ€ê°€ ë˜ë„ë¡ í•™ìŠµ</li>
<li>Planningì€ í™˜ê²½ì„ ì•Œê³ (í™˜ê²½ì— í•´ë‹¹í•˜ëŠ” modelì„ ì£¼ê³ ) agentê°€ ê³„ì‚°í•˜ëŠ” ê²ƒ</li>
</ul>
<h4 id="exploration-and-exploitation">
  Exploration and Exploitation
  <a class="anchor" href="#exploration-and-exploitation">#</a>
</h4>
<p>Exploration ì™€ Exploitation ëŠ” trade-off</p>
<ul>
<li>Exploration ì€ í™˜ê²½ì— ëŒ€í•œ ì •ë³´ë¥¼ ë” ì°¾ëŠ”ê²ƒ</li>
<li>Exploitation ì€ ì•Œê³  ìˆëŠ” ì •ë³´ë¥¼ í™œìš©í•´ì„œ rewardë¥¼ ìµœëŒ€í™” í•˜ëŠ”ê²ƒ</li>
</ul>
<p>ì˜ˆ) ìƒˆë¡œìš´ ë ˆìŠ¤í† ë‘ ì°¾ê¸° vs ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ë ˆìŠ¤í† ë‘ ì¬ë°©ë¬¸</p>
<hr>
<h2 id="ë¨¸ì‹ ëŸ¬ë‹mlê³¼-ë”¥ëŸ¬ë‹dlê³¼ì˜-ê´€ê³„">
  <strong>ë¨¸ì‹ ëŸ¬ë‹(ML)ê³¼ ë”¥ëŸ¬ë‹(DL)ê³¼ì˜ ê´€ê³„</strong>
  <a class="anchor" href="#%eb%a8%b8%ec%8b%a0%eb%9f%ac%eb%8b%9dml%ea%b3%bc-%eb%94%a5%eb%9f%ac%eb%8b%9ddl%ea%b3%bc%ec%9d%98-%ea%b4%80%ea%b3%84">#</a>
</h2>
<p>ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì¸ê³µì§€ëŠ¥(AI)ì˜ ê°œë…ìœ¼ë¡œì¨, í•™ìŠµì„ í†µí•´ ì˜ˆì¸¡(ë˜ëŠ” ë¶„ë¥˜)ë¥¼ í•˜ëŠ” ê²ƒ</p>
<p>ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ê°œë…ìœ¼ë¡œì¨, ì¸ê³µì‹ ê²½ë§(Neural Network)ë¥¼ ì´ìš©í•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒ</p>
<p>ê°•í™” í•™ìŠµì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œê°ˆë˜ë¡œì¨, ë³´ìƒì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œ í–‰ë™ í•™ìŠµí•˜ëŠ” ê²ƒ</p>
<pre tabindex="0"><code>AI
â”œâ”€â”€ Machine Learning
â”‚   â”œâ”€â”€ Supervised / Unsupervised
â”‚   â”œâ”€â”€ Reinforcement Learning
â”‚   â””â”€â”€ Deep Learning
â”‚       â””â”€â”€ Deep Reinforcement Learning (e.g., DQN, PPO)
</code></pre><h2 id="ì°¸ê³ ">
  <strong>ì°¸ê³ </strong>
  <a class="anchor" href="#%ec%b0%b8%ea%b3%a0">#</a>
</h2>
<p>David Silver ì˜ RL ê°•ì¢Œ
<a href="https://davidstarsilver.wordpress.com/teaching">https://davidstarsilver.wordpress.com/teaching</a></p>
<p>í•œê¸€ ìœ íŠœë¸Œ + ë¸”ë¡œê·¸
<a href="https://smj990203.tistory.com/2">https://smj990203.tistory.com/2</a></p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#ë“¤ì–´ê°€ê¸°-ì•ì„œ"><strong>ë“¤ì–´ê°€ê¸° ì•ì„œ</strong></a></li>
    <li><a href="#ê°•í™”í•™ìŠµrl-reinforcement-learning-ì´ë€"><strong>ê°•í™”í•™ìŠµ(RL: Reinforcement Learning) ì´ë€?</strong></a>
      <ul>
        <li><a href="#definition"><strong>Definition</strong></a></li>
        <li><a href="#state-and-mdp"><strong>State and MDP</strong></a></li>
        <li><a href="#policy-value-function-model"><strong>Policy, Value Function, Model</strong></a></li>
      </ul>
    </li>
    <li><a href="#ë¨¸ì‹ ëŸ¬ë‹mlê³¼-ë”¥ëŸ¬ë‹dlê³¼ì˜-ê´€ê³„"><strong>ë¨¸ì‹ ëŸ¬ë‹(ML)ê³¼ ë”¥ëŸ¬ë‹(DL)ê³¼ì˜ ê´€ê³„</strong></a></li>
    <li><a href="#ì°¸ê³ "><strong>ì°¸ê³ </strong></a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  <script>
  document.addEventListener('DOMContentLoaded', () => {
    document.querySelectorAll('pre > code[class^="language-"]').forEach(codeBlock => {
      const pre = codeBlock.parentElement;

      
      const button = document.createElement('button');
      button.innerText = 'ğŸ“‹';
      button.title = 'Copy code';
      button.style = `
        position: absolute;
        top: 0.5em;
        right: 0.5em;
        padding: 2px 6px;
        font-size: 0.8rem;
        background: #f5f5f5;
        border: 1px solid #ccc;
        border-radius: 4px;
        cursor: pointer;
        z-index: 10;
      `;

      
      button.addEventListener('click', () => {
        navigator.clipboard.writeText(codeBlock.innerText);
        button.innerText = 'âœ”';
        setTimeout(() => button.innerText = 'ğŸ“‹', 1000);
      });

      
      pre.style.position = 'relative';
      pre.appendChild(button);
    });
  });
</script>

</body>
</html>












