<!DOCTYPE html>
<html lang="ko-kr" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  2. Markov Decision Process
  #


  Markov Process(MP) ë€?
  #


  MP ì†ì„±
  #

MDPëŠ” í™˜ê²½ì— ëŒ€í•´ì„œ Reinforcement Learningì´ ì´í•´ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì‹í™”í•œë‹¤
ê±°ì˜ ëª¨ë“  RL ê´€ë ¨ ë¬¸ì œë“¤ì€ MDPë¡œ ìˆ˜ì‹í™” í•  ìˆ˜ ìˆë‹¤(Fully observableì´ë‚˜ Partially observationì´ë‚˜) 
Markov Propertyë¥¼ ì´ìš©í•˜ëŠ”ë°, &ndash;ì´ì „ ê°•ì˜ì°¸ì¡°&ndash;
ìš”ì•½í•˜ë©´, í˜„ì¬ stateë§Œìœ¼ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•´ë„ ëœë‹¤ëŠ” ì†ì„±ì´ë‹¤.
(ë‹¤ë¥´ê²Œ ë§í•˜ë©´, í˜„ì¬ stateê°€ ì´ë¯¸ ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤. = memoryless)
Markov state 



  \(s\)

ë¡œë¶€í„° 
  \(s&#39;\)

 ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” transition probability ë¥¼ í•˜ëŠ” ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

  \[
\mathcal{P}_{ss&#39;}  = \mathbb{P}[S_{t&#43;1} = s&#39;\mid S_t = s]
\]


ë§¤íŠ¸ë¦­ìŠ¤ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. (í•©ì€ 1)

  \[
\mathcal{P} =  \left[
\begin{array}{ccc}
\mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\
\vdots & \ddots & \vdots \\
\mathcal{P}_{n1} & \cdots & \mathcal{P}_{nn}
\end{array}
\right]
\]

">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://JaeSim.github.io/reinforcement-learning/rl-mdp/">
  <meta property="og:site_name" content="JaeSim&#39;s Workspace">
  <meta property="og:title" content="2. Markov Decision Process">
  <meta property="og:description" content="2. Markov Decision Process#Markov Process(MP) ë€?#MP ì†ì„±#MDPëŠ” í™˜ê²½ì— ëŒ€í•´ì„œ Reinforcement Learningì´ ì´í•´ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì‹í™”í•œë‹¤
ê±°ì˜ ëª¨ë“  RL ê´€ë ¨ ë¬¸ì œë“¤ì€ MDPë¡œ ìˆ˜ì‹í™” í•  ìˆ˜ ìˆë‹¤(Fully observableì´ë‚˜ Partially observationì´ë‚˜) Markov Propertyë¥¼ ì´ìš©í•˜ëŠ”ë°, â€“ì´ì „ ê°•ì˜ì°¸ì¡°â€“
ìš”ì•½í•˜ë©´, í˜„ì¬ stateë§Œìœ¼ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•´ë„ ëœë‹¤ëŠ” ì†ì„±ì´ë‹¤. (ë‹¤ë¥´ê²Œ ë§í•˜ë©´, í˜„ì¬ stateê°€ ì´ë¯¸ ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤. = memoryless)
Markov state \(s\)ë¡œë¶€í„° \(s&#39;\)ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” transition probability ë¥¼ í•˜ëŠ” ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \[\mathcal{P}_{ss&#39;} = \mathbb{P}[S_{t&#43;1} = s&#39;\mid S_t = s]\]ë§¤íŠ¸ë¦­ìŠ¤ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. (í•©ì€ 1) \[\mathcal{P} = \left[\begin{array}{ccc}\mathcal{P}_{11} &amp; \cdots &amp; \mathcal{P}_{1n} \\\vdots &amp; \ddots &amp; \vdots \\\mathcal{P}_{n1} &amp; \cdots &amp; \mathcal{P}_{nn}\end{array}\right]\]">
  <meta property="og:locale" content="ko_kr">
  <meta property="og:type" content="article">
    <meta property="article:section" content="reinforcement-learning">
    <meta property="article:published_time" content="2025-05-27T16:43:57+09:00">
    <meta property="article:modified_time" content="2025-05-27T16:43:57+09:00">
    <meta property="article:tag" content="Definition">
    <meta property="article:tag" content="Markov">
    <meta property="article:tag" content="Markov Process">
    <meta property="article:tag" content="Markov Decision Process">
    <meta property="article:tag" content="Reinforcement Learning">
<title>2. Markov Decision Process | JaeSim&#39;s Workspace</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://JaeSim.github.io/reinforcement-learning/rl-mdp/">
<link rel="stylesheet" href="/book.min.e9f68c3fff3d8236a489d16a9caf6de5e4d1a29c20eb4b5524e42cd30be4d319.css" integrity="sha256-6faMP/89gjakidFqnK9t5eTRopwg60tVJOQs0wvk0xk=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.f3d73cc38a9307efbe81ed599d6507776c1c7f0b6c6aa6c1c6c1b33132f2c3a2.js" integrity="sha256-89c8w4qTB&#43;&#43;&#43;ge1ZnWUHd2wcfwtsaqbBxsGzMTLyw6I=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>JaeSim&#39;s Workspace</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/aboutme/" class="">About Me</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>Development / ê°œë°œ</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/development/secondpost/" class="">ë‘ë²ˆì§¸ ê¸€ì…ë‹ˆë‹¤. ë¸”ë¡œê·¸ ì‹¤í—˜ìš© í…ŒìŠ¤íŠ¸ ê¸€ì…ë‹ˆë‹¤.</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/development/firstpost/" class="">í• ì¼</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/development/blogsetup/" class="">Github pages ë¥¼ ì´ìš©í•œ blog Setting (hugo &#43; hugo-book theme &#43; giscus</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>Reinforcement Learning / ê°•í™”í•™ìŠµ</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-essential/" class="">1. Reinforcement Learning Essential</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-mdp/" class="active">2. Markov Decision Process</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-dp/" class="">temp : 3. Planning by Dynamic Programming</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>2. Markov Decision Process</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#markov-processmp-ë€"><strong>Markov Process(MP) ë€?</strong></a>
      <ul>
        <li><a href="#mp-ì†ì„±"><strong>MP ì†ì„±</strong></a></li>
      </ul>
    </li>
    <li><a href="#markov-reward-processmrp-ë€"><strong>Markov Reward Process(MRP) ë€?</strong></a>
      <ul>
        <li><a href="#mrp-ì†ì„±"><strong>MRP ì†ì„±</strong></a></li>
      </ul>
    </li>
    <li><a href="#markov-decision-processmdp-ë€"><strong>Markov Decision Process(MDP) ë€?</strong></a>
      <ul>
        <li><a href="#policy"><strong>Policy</strong></a></li>
        <li><a href="#state-value-function"><strong>state-value function</strong></a></li>
        <li><a href="#action-value-q-function"><strong>action-value (q) function</strong></a></li>
        <li><a href="#optimal-value-function"><strong>Optimal Value Function</strong></a></li>
        <li><a href="#solving-bellman-optimality-equation"><strong>Solving Bellman Optimality Equation</strong></a></li>
        <li><a href="#extensions-of-mdps"><strong>Extensions of MDPs</strong></a></li>
      </ul>
    </li>
    <li><a href="#ì°¸ì¡°"><strong>ì°¸ì¡°</strong></a></li>
  </ul>
</nav>

  <div class="post-tags">
    <strong>Tags:</strong>
    
      <a href="/tags/definition" class="tag">Definition</a>
    
      <a href="/tags/markov" class="tag">Markov</a>
    
      <a href="/tags/markov-process" class="tag">Markov Process</a>
    
      <a href="/tags/markov-decision-process" class="tag">Markov Decision Process</a>
    
      <a href="/tags/reinforcement-learning" class="tag">Reinforcement Learning</a>
    
  </div>



  <div class="post-categories">
    <strong>Categories:</strong>
    
      <a href="/categories/reinforcement-learning" class="category">Reinforcement Learning</a>
    
  </div>





  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="2-markov-decision-process">
  <strong>2. Markov Decision Process</strong>
  <a class="anchor" href="#2-markov-decision-process">#</a>
</h1>
<h2 id="markov-processmp-ë€">
  <strong>Markov Process(MP) ë€?</strong>
  <a class="anchor" href="#markov-processmp-%eb%9e%80">#</a>
</h2>
<h3 id="mp-ì†ì„±">
  <strong>MP ì†ì„±</strong>
  <a class="anchor" href="#mp-%ec%86%8d%ec%84%b1">#</a>
</h3>
<p>MDPëŠ” í™˜ê²½ì— ëŒ€í•´ì„œ Reinforcement Learningì´ ì´í•´ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì‹í™”í•œë‹¤</p>
<p><U>ê±°ì˜ ëª¨ë“  RL ê´€ë ¨ ë¬¸ì œë“¤ì€ MDPë¡œ ìˆ˜ì‹í™” í•  ìˆ˜ ìˆë‹¤(Fully observableì´ë‚˜ Partially observationì´ë‚˜) </U></p>
<p>Markov Propertyë¥¼ ì´ìš©í•˜ëŠ”ë°, &ndash;ì´ì „ ê°•ì˜ì°¸ì¡°&ndash;</p>
<p>ìš”ì•½í•˜ë©´, í˜„ì¬ stateë§Œìœ¼ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•´ë„ ëœë‹¤ëŠ” ì†ì„±ì´ë‹¤.
(ë‹¤ë¥´ê²Œ ë§í•˜ë©´, í˜„ì¬ stateê°€ ì´ë¯¸ ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤. = memoryless)</p>
<p>Markov state 
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \(s\)
</span>
ë¡œë¶€í„° <span>
  \(s'\)
</span>
 ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” transition probability ë¥¼ í•˜ëŠ” ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
<span>
  \[
\mathcal{P}_{ss'}  = \mathbb{P}[S_{t+1} = s'\mid S_t = s]
\]
</span>

ë§¤íŠ¸ë¦­ìŠ¤ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. (í•©ì€ 1)
<span>
  \[
\mathcal{P} =  \left[
\begin{array}{ccc}
\mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\
\vdots & \ddots & \vdots \\
\mathcal{P}_{n1} & \cdots & \mathcal{P}_{nn}
\end{array}
\right]
\]
</span>
</p>
<p>MPëŠ” tupleë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.</p>
<blockquote>
<p>A Markov Process (or Markov Chain) is a tuple <span>
  \(\langle \mathcal{S}, \mathcal{P} \rangle \)
</span>
</p>
<ul>
<li><span>
  \(\mathcal{S}\)
</span>
 is a (finite) set of states</li>
<li><span>
  \(\mathcal{P}\)
</span>
 is a state transition probability matrix,<br>
<span>
  \(\mathcal{P}_{ss'} = \mathbb{P} \left[ S_{t+1} = s' \mid S_t = s \right]\)
</span>
</li>
</ul></blockquote>
<p>ì´ ì˜ˆì œëŠ” ë‹¨ìˆœí™”í•œ state ë³€í™”ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ê³ ,
MDPë¥¼ ì´ìš©í•œ ì‹¤ì œ ì‚¬ìš©ì€ í›¨ì‹ ë” ë§ì€ stateì™€ probabilityë¥¼ í¬í•¨í•œë‹¤.</p>
<img src="/images/rl-mp-example.png" alt="mp-example" style="width:100%;" />
<h2 id="markov-reward-processmrp-ë€">
  <strong>Markov Reward Process(MRP) ë€?</strong>
  <a class="anchor" href="#markov-reward-processmrp-%eb%9e%80">#</a>
</h2>
<h3 id="mrp-ì†ì„±">
  <strong>MRP ì†ì„±</strong>
  <a class="anchor" href="#mrp-%ec%86%8d%ec%84%b1">#</a>
</h3>
<p>rewardê°€ ì¶”ê°€ê°€ ëœ ê²ƒ. MP ì— value judgmentê°€ í¬í•¨ëœ ê²ƒ - ì—¬ê¸°ì„œ judgment ëŠ” ëˆ„ì  rewardê°€ ì–¼ë§ˆë‚˜ ì¢‹ì•„ì§ˆì§€</p>
<blockquote>
<p>A Markov Rewards Process (or Markov Chain) is a tuple <span>
  \(\langle \mathcal{S}, \mathcal{P}, \textcolor{red}{\mathcal{R}, \gamma} \rangle \)
</span>
</p>
<ul>
<li><span>
  \(\mathcal{S}\)
</span>
 is a (finite) set of states</li>
<li><span>
  \(\mathcal{P}\)
</span>
 is a state transition probability matrix,<br>
<span>
  \(\mathcal{P}_{ss'} = \mathbb{P} \left[ S_{t+1} = s' \mid S_t = s \right]\)
</span>
</li>
<li><span style="color:red"><span>
  \(\mathcal{R}\)
</span>
 is a reward function, <span>
  \(\mathcal{R}_s = \mathbb{E} \left[ R_{t+1} \mid S_t = s \right]\)
</span>
 </span></li>
<li><span style="color:red"><span>
  \(\gamma\)
</span>
 is a discount factor, <span>
  \({\gamma \in [0, 1]}\)
</span>
 </span></li>
</ul></blockquote>
<p>timestep tì— ëŒ€í•œ goal ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.
ê°ë§ˆëŠ” ë¯¸ë˜ì— ëŒ€í•œ discount factorì´ë‹¤. ì´ê²ƒì´ í•„ìš”í•œ ì´ìœ ëŠ”</p>
<ul>
<li>ìš°ë¦¬ëŠ” í¼íŒ©í•œ ëª¨ë¸ì´ ì—†ê¸° ë•Œë¬¸ì—</li>
<li>ìˆ˜í•™ì  max ë°”ìš´ë“œ(ìˆ˜í•™ì  í¸ì˜ì„±ì„ ìœ„í•´)</li>
<li>MPì˜ ë¬´í•œ ë£¨í”„ë¥¼ í•˜ì§€ í”¼í•˜ê¸° ìœ„í•´</li>
<li>ê·¼ì ‘ ë¯¸ë˜ì˜ ê°€ì¹˜ê°€ ë¹„ê·¼ì ‘(ë¨¼ë¯¸ë˜) ë¯¸ë˜ì˜ ê°€ì¹˜ë³´ë‹¤ í¬ê¸° ë•Œë¬¸</li>
<li>ì‹œí€€ìŠ¤ì˜ ëì´ ë³´ì¥ëœë‹¤ë©´ discount factorë¥¼ ì•ˆì“¸ìˆ˜ë„ ìˆë‹¤</li>
</ul>
<span>
  \[
G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]
</span>

<p>MPëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ í™•ë¥ ì— ì˜í•´ì„œ stateê°€ ë³€ê²½ë˜ëŠ” ê²ƒì´ê³ .<br>
MRP ì—ì„œëŠ” ê·¸ ë³€í™”ëœ ì‹œê°„ì—ì„œ stateì— ë„ë‹¬í• ë•Œë§ˆë‹¤ rewardê°€ íšë“ëœë‹¤ê³  ì´í•´í–ˆë‹¤.<br>
ì´ë¡œì¨ í˜„ì¬ state ì—ì„œ ë°”ë¼ë³¸ë‹¤ë©´ ì•ìœ¼ë¡œ ë‚˜ì˜ ë¯¸ë˜ total rewardë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.(discount factor 0~1)
<img src="/images/rl-mrp-example.png" alt="mp-example" style="width:80%;" /></p>
<h4 id="value-function">
  <strong>Value Function</strong>
  <a class="anchor" href="#value-function">#</a>
</h4>
<p>í˜„ì¬ìƒíƒœ(s)ì—ì„œì˜ terminated ìƒíƒœì—ì„œì˜ Expected return
ì´ê²ƒì€ Expectation ì´ë‹¤. ì™œëƒí•˜ë©´ environmentëŠ” stochastic ì´ë‹ˆê¹Œ
<span>
  \[
v(s) = \mathbb{E} [ G_t \mid S_t = s ]
\]
</span>
</p>
<p>ì´ëŠ” ë°¸ë§ë°©ì •ì‹ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.</p>
<h4 id="bellman-equation-for-mrp">
  <strong>Bellman Equation for MRP</strong>
  <a class="anchor" href="#bellman-equation-for-mrp">#</a>
</h4>
<p>Value Functionì€ í¬ê²Œ ë‘ê°€ì§€ ì»´í¬ë„ŒíŠ¸ë¡œ ë‚˜ëˆŒìˆ˜ ìˆë‹¤.</p>
<ul>
<li>í˜„ì¬ì˜ ë¦¬ì›Œë“œ <span>
  \( R_{t+1}\)
</span>
</li>
<li>ë‹¤ìŒê³„ìŠ¹ stateì˜ discounted ìƒíƒœ <span>
  \(\gamma v(S_{t+1})\)
</span>
</li>
</ul>
<span>
  \[
\begin{aligned}
v(s) &= \mathbb{E} \left[ G_t \mid S_t = s \right] \\
     &= \mathbb{E} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \mid S_t = s \right] \\
     &= \mathbb{E} \left[ R_{t+1} + \gamma \left( R_{t+2} + \gamma R_{t+3} + \cdots \right) \mid S_t = s \right] \\
     &= \mathbb{E} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \\
     &= \mathbb{E} \left[ \textcolor{red}{R_{t+1} + \gamma v(S_{t+1})} \mid S_t = s \right]
\end{aligned}
\]
</span>

<span>
  \[
v(s) = \mathbb{E} \left[ R_{t+1} + \gamma v(S_{t+1}) \mid S_t = s \right]
\]
</span>

<span>
  \[
v(s) = \mathcal{R}_s + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'} v(s')
\]
</span>

<p>ì´ë¥¼ ë²¡í„° ë§¤íŠ¸ë¦­ìŠ¤ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>
<span>
  \[
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}
=
\begin{bmatrix}
\mathcal{R}_1 \\
\vdots \\
\mathcal{R}_n
\end{bmatrix}
+
\gamma
\begin{bmatrix}
\mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\
\vdots & \ddots & \vdots \\
\mathcal{P}_{n1} & \cdots & \mathcal{P}_{nn}
\end{bmatrix}
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}
\]
</span>

<h4 id="solving-the-bellman-equation">
  <strong>Solving the Bellman Equation</strong>
  <a class="anchor" href="#solving-the-bellman-equation">#</a>
</h4>
<p>ë²¨ë§Œ ë°©ì •ì‹ì€ linear equation ì´ì§€ë§Œ O(n^3) ë³µì¡ë„ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ì‘ì€ê²ƒë§Œ í’€ìˆ˜ ìˆë‹¤.<br>
Large MRPë¥¼ í’€ê¸°ìœ„í•´ì„œ</p>
<ul>
<li>Dynamic Programing ì´ë‚˜</li>
<li>Monte-Carlo evaluation ì´ë‚˜</li>
<li>Temporal-Difference Learningì´ ìˆë‹¤.</li>
</ul>
<h2 id="markov-decision-processmdp-ë€">
  <strong>Markov Decision Process(MDP) ë€?</strong>
  <a class="anchor" href="#markov-decision-processmdp-%eb%9e%80">#</a>
</h2>
<blockquote>
<p>A Markov Decision Process (or Markov Chain) is a tuple <span>
  \(\langle \mathcal{S}, \textcolor{red}{\mathcal{A}}, \mathcal{P}, \mathcal{R}, \gamma \rangle \)
</span>
</p>
<ul>
<li><span>
  \(\mathcal{S}\)
</span>
 is a (finite) set of states</li>
<li><span style="color:red"><span>
  \(\mathcal{A}\)
</span>
 is a (finite) set of actions </span></li>
<li><span>
  \(\mathcal{P}\)
</span>
 is a state transition probability matrix,<br>
<span>
  \(\mathcal{P}^a_{ss'} = \mathbb{P} \left[ S_{t+1} = s' \mid S_t = s, A_t = \textcolor{red}a \right]\)
</span>
</li>
<li><span>
  \(\mathcal{R}\)
</span>
 is a reward function, <span>
  \(\mathcal{R}^a_s = \mathbb{E} \left[ R_{t+1} \mid S_t = s , A_t = \textcolor{red}a \right]\)
</span>
</li>
<li><span>
  \(\gamma\)
</span>
 is a discount factor, <span>
  \({\gamma \in [0, 1]}\)
</span>
</li>
</ul></blockquote>
<p>ìœ„ MP MRP ì˜ˆì œì™€ ë‹¤ë¥´ê²Œ actionì´ ì¶”ê°€ë¨. <br>
ê·¸ë¦¼ì—ì„œëŠ” ì˜ ì•ˆí‘œí˜„ë˜ì–´ìˆì§€ë§Œ, ì•¡ì…˜ì„ í•˜ë©´. ê·¸ ì•¡ì…˜ìœ¼ë¡œì¸í•´ íŠ¹ì • stateë¡œ ì „ì´ë˜ëŠ” ê²ƒì€ í™•ë¥ ì´ë‹¤.<br>
ë°‘ì— pubì— ê°€ëŠ”ê²ƒ ì•¡ì…˜ì„ ìˆ˜í–‰í•˜ë©´ í™•ë¥ ì ìœ¼ë¡œ class1, class2, class3ì— ë„ë‹¬í•œë‹¤.
<img src="/images/rl-mdp-example.png" alt="mp-example" style="width:80%;" /></p>
<h3 id="policy">
  <strong>Policy</strong>
  <a class="anchor" href="#policy">#</a>
</h3>
<p>ì •ì±…(Policy <span>
  \(\pi\)
</span>
) ëŠ” ì£¼ì–´ì§„ stateì— ëŒ€í•œ actionì˜ ë¶„í¬
<span>
  \[
\pi(a \mid s) = \mathbb{P}[A_t = a \mid S_t = s]
\]
</span>

<em>ë§ˆí¬ë¡œí”„ ì†ì„±ì— ì˜í•´ì„œ í˜„ì¬ stateëŠ” rewardë¥¼ fully characterize í•œê²ƒì´ê¸° ë•Œë¬¸ì— ìœ„ ìˆ˜ì‹ì— rewardê°€ ì—†ë‹¤</em></p>
<p>state ì‹œí€€ìŠ¤(ìƒíƒœì „ì˜)ì— ëŒ€í•´ì„œ policyë¥¼ ë„£ìœ¼ë©´ Markov Processì´ê³ ,
state ì‹œí€€ì— Rewardë¥¼ ë„£ìœ¼ë©´ Markov Reward process ì´ë‹¤.</p>
<p>Markov Reward Process ì— ëŒ€í•´ì„œ
MDP ìˆ˜ì‹ìœ¼ë¡œ  (Actionìœ¼ë¡œë¶€í„°) ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.
(MDP ìˆ˜ì‹ìœ¼ë¡œ(policy-actionì´ í¬í•¨ëœ ë²„ì „ìœ¼ë¡œ) MPì™€ MRPë¥¼ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤)</p>
<p><span>
  \[
\mathcal{P}^\pi_{s, s'} = \sum_{a \in \mathcal{A}} \pi(a \mid s) \mathcal{P}^{a}_{s s'} \\
\mathcal{R}^\pi_s = \sum_{a \in \mathcal{A}} \pi(a \mid s) \mathcal{R}^a_s
\]
</span>

ì´ëŠ” ëª¨ë“  actionì— ê°ˆìˆ˜ ìˆëŠ” probë¥¼ averageë¡œ(<span>
  \(\pi\)
</span>
ëŠ” 0~1ì˜ ê°’ì´ë¯€ë¡œ) ì´í•´ë¥¼ ì‰½ê²Œí•˜ê¸° Pì™€ Rì„ í‘œí˜„í•œ ê²ƒ.</p>
<p>MDPì— ëŒ€í•œ value functionì€
state-value í‘ì…˜ê³¼, action-value function ë‘ê°€ì§€ ë°©ì‹ì´ ìˆë‹¤.</p>
<h3 id="state-value-function">
  <strong>state-value function</strong>
  <a class="anchor" href="#state-value-function">#</a>
</h3>
<p>ë‹¤ìŒê³¼ ê°™ê³  ì´ëŠ” <strong>í˜„ì¬ stateì¼ë•Œ <span>
  \(\pi\)
</span>
 policyë¥¼ ë”°ë¥¼ë•Œ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€</strong>ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. (ì–¼ë§ˆë‚˜ Rewardë¥¼ ì–»ì„ì§€)
<span>
  \[
v_\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]
\]
</span>

ì—¬ê¸°ì„œ <span>
  \(\mathbb{E}_\pi\)
</span>
ëŠ” ëª¨ë“  ìƒ˜í”Œì•¡ì…˜ì— ëŒ€í•œ expectation</p>
<h3 id="action-value-q-function">
  <strong>action-value (q) function</strong>
  <a class="anchor" href="#action-value-q-function">#</a>
</h3>
<p>ì´ë¥¼ action-value function <span>
  \(q_\pi\)
</span>
 ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.
ì´ëŠ” <strong>í˜„ì¬ stateì—ì„œ ì–´ë–¤actionì„ ì„ íƒí–ˆì„ë•Œ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€</strong>ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. (ì–¼ë§ˆë‚˜ Rewardë¥¼ ì–»ì„ì§€)
<span>
  \[
q_\pi(s, a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right]
\]
</span>
</p>
<p>state-value functionê³¼ action-value functionì˜ Bellman Expectation ë°©ì •ì‹ì„
ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤.
<span>
  \[
v_\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s \right]
\]
</span>
</p>
<span>
  \[
q_\pi(s, a) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a \right]
\]
</span>

<p>state-value-function ì™€ action-value function ì¤‘ ì–´ë–¤ ê²ƒì„ ì¤‘ì ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ”ì§€ì— ë”°ë¥¸ í•™ìŠµë°©ë²•ì´ ë‹¬ë¼ì§€ëŠ” ê²ƒ ê°™ë‹¤.<br>
<strong>ì™¼ìª½</strong>ì€ state-value functionê´€ì ì—ì„œì˜ ê·¸ë¦¼ê³¼ ìˆ˜ì‹í‘œí˜„ì´ê³ , <strong>ì˜¤ë¥¸ìª½</strong>ì€ action-value í‘ì…˜ê´€ì ì—ì„œ ìˆ˜ì‹ê³¼ í‘œí˜„ì´ë‹¤.
action-value functionì— ëŒ€í•´ì„œëŠ”, actionì„ ì„ íƒí•¨ìœ¼ë¡œì¨ rewardë¥¼ í†µí•´ì„œ state-value functionìœ¼ë¡œ ë‹¤ì‹œ ë„˜ì–´ê°€ëŠ” ê²ƒì„ ë³¼ìˆ˜ ìˆë‹¤.</p>
<div style="display: flex; gap: 20px;">
  <img src="/images/rl-mdp-bellman-state-value.png" alt="state-value" style="width: 50%;" />
  <img src="/images/rl-mdp-bellman-action-value.png" alt="action-value" style="width: 50%;" />
</div>
<p>ì´ ë‘ê°œì˜ ê·¸ë˜í”„ë¥¼ í•©ì¹˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
ì™¼ìª½ì€ state-value function ê´€ì ì—ì„œì˜ ìˆ˜ì‹ì´ê³ , ì˜¤ë¥¸ìª½ì€ action-value function ê´€ì ì—ì„œì˜ ìˆ˜ì‹ì´ë‹¤.</p>
<div style="display: flex; gap: 20px;">
  <img src="/images/rl-mdp-bellman-state-value-merged.png" alt="state-value-merged" style="width: 50%;" />
  <img src="/images/rl-mdp-bellman-action-value-merged.png" alt="action-value-merged" style="width: 50%;" />
</div>
<p>ì´ê²ƒë“¤ì€ ì•ì—ì„œ ì–¸ê¸‰í–ˆë˜ê²ƒì²˜ëŸ¼ ë‘ ê°€ì§€ Junkë¡œ ë‚˜ëˆŒìˆ˜ ìˆê³ , (ì´ë²ˆ Stepì—ì„œì˜ Rewardì™€ ë¯¸ë˜ì˜ value function ë¦¬í„´ê°’)
ë‹¤ìŒ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì§„ë‹¤ (matric-form).
ì¶”ê°€ë¡œ ëª¨ë“  MDPëŠ” MRPë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.
<span>
  \[
v_\pi = \mathcal{R}^\pi + \gamma \mathcal{P}^\pi v_\pi
\]
</span>

<span>
  \[
v_\pi = \left( I - \gamma \mathcal{P}^\pi \right)^{-1} \mathcal{R}^\pi
\]
</span>
</p>
<p>ìš°ë¦¬ëŠ” ì´ë¡œë¶€í„°(state-value, action-value functionìœ¼ë¡œë¶€í„°) <strong>Optimal Value Function</strong> ì„ ì°¾ëŠ”ë‹¤</p>
<h3 id="optimal-value-function">
  <strong>Optimal Value Function</strong>
  <a class="anchor" href="#optimal-value-function">#</a>
</h3>
<p>MDPì—ì„œì˜ ìµœì  í–‰ë™ì„ ì°¾ëŠ” ë°©ë²•ì€ optimal state-value function <span>
  \(v_*(s)\)
</span>
 ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. <br>
ì´ê²ƒì€ ëª¨ë“  policy ì— ëŒ€í•´ì„œ value functionì„ ìµœëŒ€í™” í•˜ëŠ”ê²ƒì´ë‹¤. (ì¥ê¸°ì ìœ¼ë¡œ ìµœëŒ€ ë³´ìƒì„ ì–»ê¸° ìœ„í•´ì„œ) <br>
optimal action-value function <span>
  \(q_*(s,a) \)
</span>
ì˜ ê²½ìš° ì•„ë˜ì™€ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.</p>
<p><span>
  \[
v_*(s) = \max_\pi v_\pi(s)
\]
</span>

<span>
  \[
q_*(s, a) = \max_\pi q_\pi(s, a)
\]
</span>
</p>
<p>optimal policy (<span>
  \(\pi_*\)
</span>
) ëŠ” <span>
  \(q_*\)
</span>
 ë¥¼ ìµœëŒ€í™” í•¨ìœ¼ë¡œì¨ ì–»ì„ ìˆ˜ ìˆë‹¤.
<span>
  \[
\pi_*(a \mid s) = 
\begin{cases}
1 & \text{if } a = \arg\max\limits_{a \in \mathcal{A}} q_*(s, a) \\
0 & \text{otherwise}
\end{cases}
\]
</span>
</p>
<p>ì˜µí‹°ë©€í•œ í•´ëŠ” ìœ„ì— êµ¬í•œ ë„ì‹ê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì•„ë˜ ëª¨í˜•ê³¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</p>
<div style="display: flex; gap: 20px;">
  <img src="/images/rl-mdp-bellman-state-value-opt-merged.png" alt="state-value-merged" style="width: 50%;" />
  <img src="/images/rl-mdp-bellman-action-value-opt-merged.png" alt="action-value-merged" style="width: 50%;" />
</div>
<h3 id="solving-bellman-optimality-equation">
  <strong>Solving Bellman Optimality Equation</strong>
  <a class="anchor" href="#solving-bellman-optimality-equation">#</a>
</h3>
<p>Bellman Optimality Equationì€ non-linear í•˜ê³  ë³´í†µ No closed form ìœ¼ë¡œ ì œê³µë¨.
ì•„ë˜ì™€ ê°™ì€ solving method ë“¤ì´ ìˆìŒ</p>
<ul>
<li><strong>value iteration</strong> : Iteratively updates value estimates using the Bellman optimality equation until convergence.</li>
<li><strong>policy iteration</strong> : Alternates between policy evaluation and policy improvement until the policy becomes stable.</li>
<li><strong>Q-learning</strong> : Off-policy method that directly learns the optimal action-value function from experience.</li>
<li><strong>Sarsa</strong>(State-Action-Reward-State-Action) : On-policy method that updates action-values based on the action actually taken by the current policy.</li>
</ul>
<h3 id="extensions-of-mdps">
  <strong>Extensions of MDPs</strong>
  <a class="anchor" href="#extensions-of-mdps">#</a>
</h3>
<ul>
<li>infinite and continous MDPs
<ul>
<li>ë¬´í•œí•œ ê¸°ì¡´ ë°©ë²•ì„ ê·¸ëŒ€ë¡œ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤(Straightforward). ì—°ì†ì ì¸ ìˆ«ìë¼ë©´ í¸ë¯¸ë¶„ í•´ì•¼í•œë‹¤.</li>
</ul>
</li>
<li>Partially observable MDPs
<ul>
<li>(finite sets of Observation:O ì™€ Observation function:Z) ì¶”ê°€ìš”ì†Œê°€ ìˆìœ¼ë©°. ìƒíƒœë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ìœ¼ë‹ˆ ê´€ì¸¡ê°’ìœ¼ë¡œë¶€í„° ì¶”ì •í•˜ëŠ” hidden stageê°€ ìˆëŠ” MDPë¡œ í•´ê²°</li>
</ul>
</li>
<li>Undiscounted, average reward MDPs
<ul>
<li>ergodic markvo process ë¡œ ì²˜ë¦¬í• ìˆ˜ ìˆë‹¤. ergodicì€ Recuurent: ê° stateëŠ” ë¬´í•œ ì‹œê°„ ë™ì•ˆì— ë°©ë¬¸, Aperiodic : ì–´ë–¤ ì£¼ê¸°ì„± ì—†ì´ ë°©ë¬¸ í•˜ëŠ” ì†ì„±ì„ ê°€ì§€ê³  ìˆë‹¤.ì´ê²ƒì€ average reward MDPì´ë‹¤ (discount ë˜ì§€ ì•Šìœ¼ë‹ˆ, í°ìˆ˜ì˜ ë²•ì¹™ì— ì˜í•´ì„œ). ë”°ëŸ¬ì„œ average bellman equation ì„ í’€ë©´ ëœë‹¤.</li>
</ul>
</li>
</ul>
<h2 id="ì°¸ì¡°">
  <strong>ì°¸ì¡°</strong>
  <a class="anchor" href="#%ec%b0%b8%ec%a1%b0">#</a>
</h2>
<p><a href="https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-2-mdp.pdf">https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-2-mdp.pdf</a></p>
<p><a href="https://www.youtube.com/watch?v=lfHX2hHRMVQ">https://www.youtube.com/watch?v=lfHX2hHRMVQ</a></p>
<p><a href="https://trivia-starage.tistory.com/280">https://trivia-starage.tistory.com/280</a></p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">
<script src="https://giscus.app/client.js"
        data-repo="JaeSim/JaeSim.github.io"
        data-repo-id="R_kgDOOtVVtw"
        data-category="Comment"
        data-category-id="DIC_kwDOOtVVt84Cqcxg"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="ko"
        crossorigin="anonymous"
        async>
</script>

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#markov-processmp-ë€"><strong>Markov Process(MP) ë€?</strong></a>
      <ul>
        <li><a href="#mp-ì†ì„±"><strong>MP ì†ì„±</strong></a></li>
      </ul>
    </li>
    <li><a href="#markov-reward-processmrp-ë€"><strong>Markov Reward Process(MRP) ë€?</strong></a>
      <ul>
        <li><a href="#mrp-ì†ì„±"><strong>MRP ì†ì„±</strong></a></li>
      </ul>
    </li>
    <li><a href="#markov-decision-processmdp-ë€"><strong>Markov Decision Process(MDP) ë€?</strong></a>
      <ul>
        <li><a href="#policy"><strong>Policy</strong></a></li>
        <li><a href="#state-value-function"><strong>state-value function</strong></a></li>
        <li><a href="#action-value-q-function"><strong>action-value (q) function</strong></a></li>
        <li><a href="#optimal-value-function"><strong>Optimal Value Function</strong></a></li>
        <li><a href="#solving-bellman-optimality-equation"><strong>Solving Bellman Optimality Equation</strong></a></li>
        <li><a href="#extensions-of-mdps"><strong>Extensions of MDPs</strong></a></li>
      </ul>
    </li>
    <li><a href="#ì°¸ì¡°"><strong>ì°¸ì¡°</strong></a></li>
  </ul>
</nav>

  <div class="post-tags">
    <strong>Tags:</strong>
    
      <a href="/tags/definition" class="tag">Definition</a>
    
      <a href="/tags/markov" class="tag">Markov</a>
    
      <a href="/tags/markov-process" class="tag">Markov Process</a>
    
      <a href="/tags/markov-decision-process" class="tag">Markov Decision Process</a>
    
      <a href="/tags/reinforcement-learning" class="tag">Reinforcement Learning</a>
    
  </div>



  <div class="post-categories">
    <strong>Categories:</strong>
    
      <a href="/categories/reinforcement-learning" class="category">Reinforcement Learning</a>
    
  </div>




 
      </div>
    </aside>
    
  </main>

  <script>
  document.addEventListener('DOMContentLoaded', () => {
    document.querySelectorAll('pre > code[class^="language-"]').forEach(codeBlock => {
      const pre = codeBlock.parentElement;

      
      const button = document.createElement('button');
      button.innerText = 'ğŸ“‹';
      button.title = 'Copy code';
      button.style = `
        position: absolute;
        top: 0.5em;
        right: 0.5em;
        padding: 2px 6px;
        font-size: 0.8rem;
        background: #f5f5f5;
        border: 1px solid #ccc;
        border-radius: 4px;
        cursor: pointer;
        z-index: 10;
      `;

      
      button.addEventListener('click', () => {
        navigator.clipboard.writeText(codeBlock.innerText);
        button.innerText = 'âœ”';
        setTimeout(() => button.innerText = 'ğŸ“‹', 1000);
      });

      
      pre.style.position = 'relative';
      pre.appendChild(button);
    });
  });
</script>

</body>
</html>












