<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning / 강화학습 on JaeSim&#39;s Workspace</title>
    <link>https://JaeSim.github.io/reinforcement-learning/</link>
    <description>Recent content in Reinforcement Learning / 강화학습 on JaeSim&#39;s Workspace</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Thu, 29 May 2025 12:21:05 +0900</lastBuildDate>
    <atom:link href="https://JaeSim.github.io/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1. Reinforcement Learning Essential</title>
      <link>https://JaeSim.github.io/reinforcement-learning/rl-essential/</link>
      <pubDate>Thu, 22 May 2025 11:13:00 +0900</pubDate>
      <guid>https://JaeSim.github.io/reinforcement-learning/rl-essential/</guid>
      <description>&lt;h1 id=&#34;1-강화학습에-대한-기초-내용&#34;&gt;&#xD;&#xA;  &lt;strong&gt;1. 강화학습에 대한 기초 내용&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1-%ea%b0%95%ed%99%94%ed%95%99%ec%8a%b5%ec%97%90-%eb%8c%80%ed%95%9c-%ea%b8%b0%ec%b4%88-%eb%82%b4%ec%9a%a9&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h1&gt;&#xD;&#xA;&lt;h2 id=&#34;들어가기-앞서&#34;&gt;&#xD;&#xA;  &lt;strong&gt;들어가기 앞서&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%eb%93%a4%ec%96%b4%ea%b0%80%ea%b8%b0-%ec%95%9e%ec%84%9c&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h2&gt;&#xD;&#xA;&lt;p&gt;강화학습의 기초적인 내용을 학습한 뒤 정리한 것으로,&#xA;남들에게 보여주기 보다는 본인의 이해와 기억을 위해서 기술한 것입니다.&lt;/p&gt;&#xA;&lt;p&gt;나만의 방식으로 이해한 것이기 때문에, 주요하다고 생각하는 부분이 다를수 있으며 생략되거나 놓친 부분이 많이 있습니다.&lt;/p&gt;&#xA;&lt;h2 id=&#34;강화학습rl-reinforcement-learning-이란&#34;&gt;&#xD;&#xA;  &lt;strong&gt;강화학습(RL: Reinforcement Learning) 이란?&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%ea%b0%95%ed%99%94%ed%95%99%ec%8a%b5rl-reinforcement-learning-%ec%9d%b4%eb%9e%80&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h2&gt;&#xD;&#xA;&lt;h3 id=&#34;definition&#34;&gt;&#xD;&#xA;  &lt;strong&gt;Definition&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#definition&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h3&gt;&#xD;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;“Reinforcement learning is learning what to do—how to map &lt;strong&gt;situations&lt;/strong&gt; to &lt;strong&gt;actions&lt;/strong&gt;—so as to maximize a numerical &lt;strong&gt;reward&lt;/strong&gt; signal.”&lt;br&gt;&#xA;— &lt;em&gt;Richard S. Sutton and Andrew G. Barto&lt;/em&gt;, &lt;em&gt;Reinforcement Learning: An Introduction&lt;/em&gt; (2nd ed), p.1&lt;/p&gt;</description>
    </item>
    <item>
      <title>2. Markov Decision Process</title>
      <link>https://JaeSim.github.io/reinforcement-learning/rl-mdp/</link>
      <pubDate>Tue, 27 May 2025 16:43:57 +0900</pubDate>
      <guid>https://JaeSim.github.io/reinforcement-learning/rl-mdp/</guid>
      <description>&lt;h1 id=&#34;2-markov-decision-process&#34;&gt;&#xD;&#xA;  &lt;strong&gt;2. Markov Decision Process&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2-markov-decision-process&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h1&gt;&#xD;&#xA;&lt;h2 id=&#34;markov-processmp-란&#34;&gt;&#xD;&#xA;  &lt;strong&gt;Markov Process(MP) 란?&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#markov-processmp-%eb%9e%80&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h2&gt;&#xD;&#xA;&lt;h3 id=&#34;mp-속성&#34;&gt;&#xD;&#xA;  &lt;strong&gt;MP 속성&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mp-%ec%86%8d%ec%84%b1&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h3&gt;&#xD;&#xA;&lt;p&gt;MDP는 환경에 대해서 Reinforcement Learning이 이해가능하도록 수식화한다&lt;/p&gt;&#xA;&lt;p&gt;거의 모든 RL 관련 문제들은 MDP로 수식화 할 수 있다(Fully observable이나 Partially ovservable이나)&lt;/p&gt;&#xA;&lt;p&gt;Markov Property를 이용하는데, &amp;ndash;이전 강의참조&amp;ndash;&lt;/p&gt;&#xA;&lt;p&gt;요약하면, 현재 state만으로 미래를 예측해도 된다는 속성이다.&#xA;(다르게 말하면, 현재 state가 이미 유용한 정보를 포함하고 있다. = memoryless)&lt;/p&gt;&#xA;&lt;p&gt;Markov state &#xD;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://JaeSim.github.io/katex/katex.min.css&#34; /&gt;&#xD;&#xA;&lt;script defer src=&#34;https://JaeSim.github.io/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xD;&#xA;&lt;script defer src=&#34;https://JaeSim.github.io/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xD;&#xA;  \(s\)&#xD;&#xA;&lt;/span&gt;&#xD;&#xA;로부터 &lt;span&gt;&#xD;&#xA;  \(s&#39;\)&#xD;&#xA;&lt;/span&gt;&#xD;&#xA; 으로 변경하는 transition probability 를 하는 수식은 다음과 같다.&#xA;&lt;span&gt;&#xD;&#xA;  \[&#xD;&#xA;\mathcal{P}_{ss&#39;}  = \mathbb{P}[S_{t+1} = s&#39;\mid S_t = s]&#xD;&#xA;\]&#xD;&#xA;&lt;/span&gt;&#xD;&#xA;&#xA;매트릭스로 표현하면 다음과 같다. (합은 1)&#xA;&lt;span&gt;&#xD;&#xA;  \[&#xD;&#xA;\mathcal{P} =  \left[&#xD;&#xA;\begin{array}{ccc}&#xD;&#xA;\mathcal{P}_{11} &amp; \cdots &amp; \mathcal{P}_{1n} \\&#xD;&#xA;\vdots &amp; \ddots &amp; \vdots \\&#xD;&#xA;\mathcal{P}_{n1} &amp; \cdots &amp; \mathcal{P}_{nn}&#xD;&#xA;\end{array}&#xD;&#xA;\right]&#xD;&#xA;\]&#xD;&#xA;&lt;/span&gt;&#xD;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>temp : 3. Planning by Dynamic Programming</title>
      <link>https://JaeSim.github.io/reinforcement-learning/rl-dp/</link>
      <pubDate>Thu, 29 May 2025 12:21:05 +0900</pubDate>
      <guid>https://JaeSim.github.io/reinforcement-learning/rl-dp/</guid>
      <description>&lt;h1 id=&#34;3-planning-by-dynamic-programming&#34;&gt;&#xD;&#xA;  &lt;strong&gt;3. Planning by Dynamic Programming&lt;/strong&gt;&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#3-planning-by-dynamic-programming&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h1&gt;&#xD;&#xA;&lt;blockquote class=&#34;book-hint warning&#34;&gt;&#xD;&#xA;  // NOTE: 이 페이지는 임시로 작성되었습니다.&#xD;&#xA;&lt;/blockquote&gt;&#xD;&#xA;&lt;p&gt;MDP를 푸는 방식들이 여러개가 있다.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Policy evaluation, Policy iteration, value iteration&lt;/strong&gt; 등이 있고, 이것들은 &lt;strong&gt;환경을 정확하게 안다면&lt;/strong&gt; DP가 적용이 가능하다&lt;/p&gt;&#xA;&lt;p&gt;먼저 간략하게 언급하자면&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;policy를 평가하고 iteration 하면서 발전해나가는 방식과  (policy evaluation + policy iteration)&lt;/li&gt;&#xA;&lt;li&gt;value function을 iteration하면서 옵티멀을 찾아가는 방법 (value iteration)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;이 있다.&lt;/p&gt;&#xA;&lt;p&gt;이번 섹션은 DP로 known MDP를 푸는 방법에 대한것이고, 이것은 강화학습의 flow 와 수식 간의 이해를 위한 섹션이다.&#xA;4장부터 unknown MDP를 푸는 방법이 기술되어 있다.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
