<!DOCTYPE html>
<html lang="ko-kr" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  4. model-free prediction
  #


  // NOTE: 이 페이지는 임시로 작성되었습니다.

3장 DP에 있는것처럼, Model-free prediction 하고, Model-free control 하는 순서로 진행된다.
episode : 에이전트가 시작 상태에서 행동을 시작해서, 어떤 종료 조건(End state)에 도달할 때까지의 전체 과정
MC와 TD는 major model-free algo.

Monte Carlo (MC) : 한 에피소드가 끝날 때까지 기다린 후, 그 전체 리턴 값을 이용하여 value function을 업데이트
MC는 하나의 에피소드 전체(시작 ~ 종료)를 관찰한 뒤, 
실제로 받은 reward들을 기반으로 학습합니다. 
환경 모델 없이, 경험만으로 value function이나 policy를 추정합니다. 
Monte-Carlo policy evaluation uses empirical mean return
instead of expected return 
높은 variance와 zero bias를 가짐

즉, 가장 손쉬운 방법으로 epside를 돌려보고 가능서들의 mean 값으로 처리 
방문할때마다 횟수와 토탈 return을 늘리고, 이것의 평균을 통해 value function을 estimate한다.
lecture-4, 7 page




  \[
V(S_t) \leftarrow V(S_t) &#43; \frac{1}{N(S_t)} \left(G_t - V(S_t)\right)
\]

">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://JaeSim.github.io/reinforcement-learning/rl-model-free-prediction/">
  <meta property="og:site_name" content="JaeSim&#39;s Workspace">
  <meta property="og:title" content="temp. 4. Model Free Prediction">
  <meta property="og:description" content="4. model-free prediction#// NOTE: 이 페이지는 임시로 작성되었습니다.3장 DP에 있는것처럼, Model-free prediction 하고, Model-free control 하는 순서로 진행된다.
episode : 에이전트가 시작 상태에서 행동을 시작해서, 어떤 종료 조건(End state)에 도달할 때까지의 전체 과정
MC와 TD는 major model-free algo.
Monte Carlo (MC) : 한 에피소드가 끝날 때까지 기다린 후, 그 전체 리턴 값을 이용하여 value function을 업데이트
MC는 하나의 에피소드 전체(시작 ~ 종료)를 관찰한 뒤, 실제로 받은 reward들을 기반으로 학습합니다. 환경 모델 없이, 경험만으로 value function이나 policy를 추정합니다. Monte-Carlo policy evaluation uses empirical mean return instead of expected return 높은 variance와 zero bias를 가짐 즉, 가장 손쉬운 방법으로 epside를 돌려보고 가능서들의 mean 값으로 처리 방문할때마다 횟수와 토탈 return을 늘리고, 이것의 평균을 통해 value function을 estimate한다. lecture-4, 7 page \[V(S_t) \leftarrow V(S_t) &#43; \frac{1}{N(S_t)} \left(G_t - V(S_t)\right)\]">
  <meta property="og:locale" content="ko_kr">
  <meta property="og:type" content="article">
    <meta property="article:section" content="reinforcement-learning">
    <meta property="article:published_time" content="2025-06-12T10:57:55+09:00">
    <meta property="article:modified_time" content="2025-06-12T10:57:55+09:00">
    <meta property="article:tag" content="Definition">
    <meta property="article:tag" content="Model Free">
    <meta property="article:tag" content="Monte Carlo">
    <meta property="article:tag" content="Temporal Difference">
    <meta property="article:tag" content="Reinforcement Learning">
<title>temp. 4. Model Free Prediction | JaeSim&#39;s Workspace</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://JaeSim.github.io/reinforcement-learning/rl-model-free-prediction/">
<link rel="stylesheet" href="/book.min.e9f68c3fff3d8236a489d16a9caf6de5e4d1a29c20eb4b5524e42cd30be4d319.css" integrity="sha256-6faMP/89gjakidFqnK9t5eTRopwg60tVJOQs0wvk0xk=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.cd8fdad82ebc0a9fd91674c3f25021931c8438f3460432ca8cee03d70aa602e2.js" integrity="sha256-zY/a2C68Cp/ZFnTD8lAhkxyEOPNGBDLKjO4D1wqmAuI=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>JaeSim&#39;s Workspace</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/aboutme/" class="">About Me</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>Development / 개발</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/development/secondpost/" class="">두번째 글입니다. 블로그 실험용 테스트 글입니다.</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/development/firstpost/" class="">할일</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/development/blogsetup/" class="">Github pages 를 이용한 blog Setting (hugo &#43; hugo-book theme &#43; giscus</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>Reinforcement Learning / 강화학습</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-essential/" class="">1. Reinforcement Learning Essential</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-mdp/" class="">2. Markov Decision Process</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-dp/" class="">3. Planning by Dynamic Programming</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-model-free-prediction/" class="active">temp. 4. Model Free Prediction</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>Learned Query Optimizer</span>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>temp. 4. Model Free Prediction</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#on-policyoff-policy-intro"><strong>on-policy/off-policy intro</strong></a></li>
    <li><a href="#on-policy"><strong>on-policy</strong></a>
      <ul>
        <li><a href="#monte-carlo-iteration"><strong>Monte-carlo iteration</strong></a></li>
        <li><a href="#ε-greedy"><strong>ε-greedy</strong></a></li>
        <li><a href="#monte-carlo-control"><strong>Monte-carlo Control</strong></a></li>
        <li><a href="#sarsa"><strong>Sarsa</strong></a></li>
      </ul>
    </li>
    <li><a href="#off-policy"><strong>off-policy</strong></a>
      <ul>
        <li><a href="#q-learning"><strong>Q-learning</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#action-value-function-approximation"><strong>Action-value function Approximation</strong></a></li>
    <li><a href="#batch-method"><strong>Batch Method</strong></a>
      <ul>
        <li><a href="#experience-replay"><strong>Experience Replay</strong></a></li>
        <li><a href="#prioritized-experience-replay-per"><strong>Prioritized Experience Replay (PER)</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#actor-critic"><strong>Actor-Critic</strong></a>
      <ul>
        <li><a href="#proximal-policy-optimization-ppo"><strong>Proximal Policy Optimization (PPO)</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#model-based-rl"><strong>Model-based RL</strong></a></li>
    <li><a href="#simulation-based-search"><strong>Simulation-based Search</strong></a></li>
  </ul>

  <ul>
    <li><a href="#model-agnostic-meta-learningmaml"><strong>Model-Agnostic Meta-Learning(MAML)</strong></a></li>
    <li><a href="#per"><strong>PER</strong></a></li>
    <li><a href="#참조">참조</a></li>
  </ul>
</nav>

  <div class="post-tags">
    <strong>Tags:</strong>
    
      <a href="/tags/definition" class="tag">Definition</a>
    
      <a href="/tags/model-free" class="tag">Model Free</a>
    
      <a href="/tags/monte-carlo" class="tag">Monte Carlo</a>
    
      <a href="/tags/temporal-difference" class="tag">Temporal Difference</a>
    
      <a href="/tags/reinforcement-learning" class="tag">Reinforcement Learning</a>
    
  </div>



  <div class="post-categories">
    <strong>Categories:</strong>
    
      <a href="/categories/reinforcement-learning" class="category">Reinforcement Learning</a>
    
  </div>





  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="4-model-free-prediction">
  <strong>4. model-free prediction</strong>
  <a class="anchor" href="#4-model-free-prediction">#</a>
</h1>
<blockquote class="book-hint warning">
  // NOTE: 이 페이지는 임시로 작성되었습니다.
</blockquote>
<p>3장 DP에 있는것처럼, Model-free prediction 하고, Model-free control 하는 순서로 진행된다.</p>
<p>episode : 에이전트가 시작 상태에서 행동을 시작해서, 어떤 종료 조건(End state)에 도달할 때까지의 전체 과정</p>
<p>MC와 TD는 major model-free algo.</p>
<ul>
<li><strong>Monte Carlo (MC)</strong> : 한 에피소드가 끝날 때까지 기다린 후, 그 전체 리턴 값을 이용하여 value function을 업데이트<br>
MC는 <strong>하나의 에피소드 전체</strong>(시작 ~ 종료)를 관찰한 뒤, <br>
실제로 받은 reward들을 기반으로 학습합니다. <br>
환경 모델 없이, 경험만으로 value function이나 policy를 추정합니다. <br>
Monte-Carlo policy evaluation uses <em>empirical mean</em> return
instead of expected return <br>
높은 variance와 zero bias를 가짐</li>
</ul>
<p>즉, 가장 손쉬운 방법으로 epside를 돌려보고 가능서들의 mean 값으로 처리 <br>
방문할때마다 횟수와 토탈 return을 늘리고, 이것의 평균을 통해 value function을 estimate한다.
lecture-4, 7 page

<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \[
V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)} \left(G_t - V(S_t)\right)
\]
</span>
</p>
<p>Blackjack 예제처럼,
확률이나, 분포 그런것 전혀 없이 episode 로 부터 value function을 만들어냈다. (~500,000반복하면서)</p>
<ul>
<li><strong>Temporal Difference (TD)</strong> : 에피소드가 끝나지 않아도, 다음 상태의 현재 추정 값을 사용해 바로 업데이트 <br>
incomplete episodes 를 bootstraping을 통해서 업데이트 <br>
아직 에피소드가 끝나지 않아서 나머지 예상되는 reward를 포함해서 value function을 업데이트함 <br>
따라서 bias가 있음 + 낮은 variance를 가짐<br>
이것은 Markov property를 활용한다.</li>
</ul>
<span>
  \[
V(S_t) \leftarrow V(S_t) + \alpha \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)
\]
</span>

<ul>
<li><strong>TD(<span>
  \(\lambda\)
</span>
)</strong> : 여러 step + 가중합으로 업데이트 ?  MC와 TD의 중간<br>
MC &lt;-&gt; TD는 전체 에피소드를 보느냐, 일부분만 보느냐의 차이.<br>
TD의 step을 0~n (n이 되면 MC와 같음) 사이를 <span>
  \(\lambda\)
</span>
로 가중치를 구해 사용하는것<br>
Monte-Carlo Reinforcement Learning 은 model-free 이다.
왜냐하면 MDP Transition 에 대한 (reward에 대한) 지식이 없기 때문.</li>
</ul>
<p>Temporal-Difference Learning 은 model-free</p>
<img src="/images/rl-backup-category.png" alt="rl-essential" style="width:80%;" />
<h1 id="5-model-free-control">
  <strong>5. model-free control</strong>
  <a class="anchor" href="#5-model-free-control">#</a>
</h1>
<h2 id="on-policyoff-policy-intro">
  <strong>on-policy/off-policy intro</strong>
  <a class="anchor" href="#on-policyoff-policy-intro">#</a>
</h2>
<p>π = Target Policy
µ = Behavior Policy</p>
<p>이두개가 같으면 on-policy 다르면 off-policy.</p>
<ul>
<li>
<p>Off-policy learning : “Look over someone’s shoulder” <br>
Learn about policy π from experience sampled from µ <br>
<strong>re-use</strong> experience generated from old policy <br>
<strong>Q-Learning</strong> : <span>
  \(\varepsilon\)
</span>
-greedy 방식으로 탐험하지만 학습에는 반영 안할수 있음(최적의 행동만 업데이트)</p>
</li>
<li>
<p>On-policy learning : “Learn on the job” <br>
Learn about policy π from experience sampled from π<br>
<strong>Salsa</strong> :on-policy Q-learning. 현재 행동을 그대로 따라가며 학습</p>
</li>
</ul>
<h2 id="on-policy">
  <strong>on-policy</strong>
  <a class="anchor" href="#on-policy">#</a>
</h2>
<h3 id="monte-carlo-iteration">
  <strong>Monte-carlo iteration</strong>
  <a class="anchor" href="#monte-carlo-iteration">#</a>
</h3>
<p>Monte-Carlo 방법을 통해서 Policy Evaluation은 가능.(=Monte-Carlo Evaluation)<br>
greedy 하게 policy improvement는 action-value 펑션에 대해서만 가능하다.
state-value function을 하려면, 모델에 대해 알아야만 가능하다.<br></p>
<p><span>
  \[
\pi'(s) = \arg\max_{a \in \mathcal{A}} \left( \mathcal{R}_s^a + \sum_{s'} \mathcal{P}_{ss'}^a V(s') \right)
\]
</span>

위와 대비되게 action-value function(Q 펑션) 은 model-free해서 알수 있다.</p>
<span>
  \[
\pi'(s) = \arg\max_{a \in \mathcal{A}} Q(s, a)
\]
</span>

<p>이렇게 알게된 policy를 아래 <span>
  \(\varepsilon\)
</span>
-greedy  방식으로 improvement 한다.</p>
<h3 id="ε-greedy">
  <strong>ε-greedy</strong>
  <a class="anchor" href="#%ce%b5-greedy">#</a>
</h3>
<p><strong><span>
  \(\varepsilon\)
</span>
-greedy Algo</strong></p>
<p>항상 최고의 행동만 고르면 탐험이 부족하고, 항상 무작위로 고르면 성능이 낮다. → 둘 사이를 적절히 섞자!</p>
<p>예시 )<br>
ε=0.1 (10%) <br>
90% 확률로 현재 최적의 행동 <br>
10% 확률로 랜덤 행동 <br></p>
<p>이것은 수학적으로 policy가 점차 좋아지는것을 나타내고 수렴한다는 계산 증명이 가능하다</p>
<h3 id="monte-carlo-control">
  <strong>Monte-carlo Control</strong>
  <a class="anchor" href="#monte-carlo-control">#</a>
</h3>
<p>Monde-Carlo Policy iteration 은 이전 섹션에서 설명한것</p>
<p>Monde-Carlo Control 은 하나씩의 episode 가 끝난후에 policy를 업데이트하는것 (episode 단위로 policy improvement)<br>
이렇게 해도 되는 이유는(수렴하는이유는) <code>Greedy in the Limit with Infinite Exploration</code> 성질을 만족하기 때문이다..<br>
(policy를 업데이트하기 위한 충분한 정보를 이미 가지고 있다 라고 볼수도 있다.) <br>
[화살표 그림 추가예정]</p>
<p>GLIE 성질에 대한 설명은 추후 업데이트.</p>
<h3 id="sarsa">
  <strong>Sarsa</strong>
  <a class="anchor" href="#sarsa">#</a>
</h3>
<p>MC Control은 episode가 끝날때마다 정책을 개선하는데
Salsa는 매 step마다 정책을 개선.</p>
<h2 id="off-policy">
  <strong>off-policy</strong>
  <a class="anchor" href="#off-policy">#</a>
</h2>
<p><strong>behaviour policy µ를 통해서 수집하고, target policy π 를 학습하는것</strong> <br>
이것은 다른 분포로부터 학습하는 성질을 이용
<span>
  \[
\mathbb{E}_{X \sim P}[f(X)]
= \sum P(X) f(X)
= \sum Q(X) \frac{P(X)}{Q(X)} f(X)
= \mathbb{E}_{X \sim Q} \left[ \frac{P(X)}{Q(X)} f(X) \right]
\]
</span>
</p>
<p>이것은 다음과 같이 value function 계산에 주입된다.
<span>
  \[
G_t^{\pi / \mu} =
\frac{\pi(A_t \mid S_t)}{\mu(A_t \mid S_t)}
\frac{\pi(A_{t+1} \mid S_{t+1})}{\mu(A_{t+1} \mid S_{t+1})}
\cdots
\frac{\pi(A_T \mid S_T)}{\mu(A_T \mid S_T)}
G_t
\]
</span>

<span>
  \[
V(S_t) \leftarrow V(S_t) + \alpha \left( G_t^{\pi / \mu} - V(S_t) \right)
\]
</span>
</p>
<h3 id="q-learning">
  <strong>Q-learning</strong>
  <a class="anchor" href="#q-learning">#</a>
</h3>
<p>action-value Q(s,a) 의 off-policy learning <br>
다음 action을 선택할때 behaviour policy로부터 고르고, <span>
  \(A_{t+1} \sim \mu(\cdot \mid S_t)\)
</span>
 <br>
학습은 target policy 기반으로 진행. <span>
  \(A' \sim \pi(\cdot \mid S_t)\)
</span>
<br>
<strong>위와 같이 진행해도, Q-learing은 결국에는 <br>
옵티멀한 action-value (q) function에 수렴한다는 성질을 이용한것.</strong> <br></p>
<p>밑에는 improvement 하는 수식.
<span>
  \[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( R_{t+1} + \gamma Q(S_{t+1}, A') - Q(S_t, A_t) \right)
\]
</span>
</p>
<p>이렇게 하면 복잡한 weight 계산식 을 하지 않아도 된다.</p>
<p>[마지막 랩업 표 추가 예정]</p>
<h1 id="6-value-function-approximation">
  <strong>6. value function approximation</strong>
  <a class="anchor" href="#6-value-function-approximation">#</a>
</h1>
<p>large MDP를 풀수 없으니 (too many state and action) value function(state-value/action-value function)을 어떻게 근사하게 구하는가?</p>
<ul>
<li>Liner combinations of feature</li>
<li>Neural network</li>
</ul>
<p>여기서부터 Gradient Descent 가 나온다.</p>
<blockquote>
<ol>
<li>정책 π 고정</li>
<li>Q-function을 gradient descent로 근사 (policy evaluation)</li>
<li>근사된 Q 기반으로 정책 개선 (policy improvement)</li>
<li>다시 반복 (⇒ 점진적으로 최적 정책에 수렴)</li>
</ol></blockquote>
<p>value function을 근사해서 사용하므로 <strong>value-based</strong> 라고도 한다.</p>
<h2 id="action-value-function-approximation">
  <strong>Action-value function Approximation</strong>
  <a class="anchor" href="#action-value-function-approximation">#</a>
</h2>
<p>아래와 같이 action value function은 approximate를 통해서 표현될수 있고.
델타 W를 작게하므로써 근사를 구할 수 있다.
<span>
  \[
J(\mathbf{w}) = \mathbb{E}_{\pi} \left[ \left( q_{\pi}(S, A) - \hat{q}(S, A, \mathbf{w}) \right)^2 \right]
\]
</span>
</p>
<p><span>
  \[
- \frac{1}{2} \nabla_{\mathbf{w}} J(\mathbf{w}) 
= \left( q_{\pi}(S, A) - \hat{q}(S, A, \mathbf{w}) \right) \nabla_{\mathbf{w}} \hat{q}(S, A, \mathbf{w})
\]
</span>

<span>
  \[
\Delta \mathbf{w} = \alpha \left( q_{\pi}(S, A) - \hat{q}(S, A, \mathbf{w}) \right) \nabla_{\mathbf{w}} \hat{q}(S, A, \mathbf{w})
\]
</span>
</p>
<h2 id="batch-method">
  <strong>Batch Method</strong>
  <a class="anchor" href="#batch-method">#</a>
</h2>
<p>위의 gradient descent 할때 sampling을 효율적으로 하기 위한 여러가지 방법들</p>
<h3 id="experience-replay">
  <strong>Experience Replay</strong>
  <a class="anchor" href="#experience-replay">#</a>
</h3>
<p>과거의 transition들을 버리지 않고 buffer에 저장해 두었다가,
학습할 때마다 랜덤하게 샘플링해서 사용</p>
<h3 id="prioritized-experience-replay-per">
  <strong>Prioritized Experience Replay (PER)</strong>
  <a class="anchor" href="#prioritized-experience-replay-per">#</a>
</h3>
<p>모든 transition을 균등하게 샘플하지 않고,
TD-error가 큰 transition에 더 높은 확률을 부여하여 학습</p>
<p>직관: TD-error가 클수록 더 학습이 필요한 &ldquo;중요한&rdquo; 경험</p>
<h1 id="7-policy-gradient-method">
  <strong>7. policy gradient Method</strong>
  <a class="anchor" href="#7-policy-gradient-method">#</a>
</h1>
<p>이전섹션에서는 action-value function(Q 펑션)을 근사해서 옵티멀한 폴리시를 찾아갔다면, <br>
policy gradient는 Q 없이 policy parameter를 직접 업데이트 한다. <strong>policy-based</strong></p>
<ul>
<li>Softmax Policy : 이산행동일때, softmax over logits으로 정책을 정하고, policy에 대한 gradient 값 계산<br>
이를 통해서 현재 iteration 에 대한 policy 파라미터를 업데이트한다.</li>
<li>Gaussian Policy : 연속행동일때,  가우시안 분포를 정책으로 정하고, policy에 대한 gradient 값 계산<br>
이를 통해서 현재 iteration 에 대한 policy 파라미터를 업데이트한다.</li>
</ul>
<blockquote class="book-hint warning">
  <p>policy-based 에서도 replay buffer를 쓸수 있는가?</p>
<ol>
<li>
<p>Policy-based에서 Replay Buffer의 어려움
문제: Policy가 계속 변하기 때문에
​</p>
</li>
<li>
<p>PPO, TRPO [	❌ 또는 제한적 사용 ]	최근의 데이터만 사용 (very short buffer)</p>
</li>
<li>
<p>SAC (Soft Actor-Critic) [ 확률적 policy 사용 ] (policy gradient 기반) <br>
하지만 전체 구조는 off-policy 로써 replay buffer 적극 사용</p>
</li>
</ol>
<p>by GPT</p>
<p><strong>추가 research 요망</strong></p>
</blockquote>
<h2 id="actor-critic">
  <strong>Actor-Critic</strong>
  <a class="anchor" href="#actor-critic">#</a>
</h2>
<p>policy-based + value-based</p>
<p>Actor는 행동을 생성하고 <br>
Critic은 그 행동의 &ldquo;좋음&quot;을 평가해서 Advantage를 추정 <br>
이를 바탕으로 Actor의 policy gradient를 계산 <br></p>
<p>lacture-7 24page</p>
<p>간단한 actor-critic 구조는 actuion-value를 critic 하는것이다.
Critic은 TD(0)을 통해서 Q-Function을 학습하고
Actor는 위 학습된 Q-function을 기반으로 policy gradient를 수행하는것이다.</p>
<h3 id="proximal-policy-optimization-ppo">
  <strong>Proximal Policy Optimization (PPO)</strong>
  <a class="anchor" href="#proximal-policy-optimization-ppo">#</a>
</h3>
<p>Actor-Critic 구조에서
Clipped Objective를 도입해서, policy가 너무 크게 바뀌지 않도록 제약하는것.</p>
<p>아래는 PPO surrogate objective 함수(목적함수). 이를 gradient ascent(최대화) 하는 파라미터를 현재 policy에 업데이트하면 policy는 옵티멀을 향해간다.
<span>
  \[
L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta) \hat{A}_t,\; \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
\]
</span>
</p>
<h1 id="8-integration-learning-and-planning">
  <strong>8. Integration Learning and Planning</strong>
  <a class="anchor" href="#8-integration-learning-and-planning">#</a>
</h1>
<h2 id="model-based-rl">
  <strong>Model-based RL</strong>
  <a class="anchor" href="#model-based-rl">#</a>
</h2>
<p><strong>[생략]</strong></p>
<h2 id="simulation-based-search">
  <strong>Simulation-based Search</strong>
  <a class="anchor" href="#simulation-based-search">#</a>
</h2>
<p>&ldquo;전체 상태 공간을 직접 학습하기는 너무 비싸다. 대신, 유망한 부분만 집중적으로 시뮬레이션하면서 거기서 경험한 정보로 Q값을 점점 더 정확하게 만든다.&rdquo;</p>
<p>//note: balsa simulation learning 과는 개념이 조금 다름</p>
<p><strong>[미완]</strong></p>
<h1 id="9-exploration-and-exploitation">
  <strong>9. Exploration and Exploitation</strong>
  <a class="anchor" href="#9-exploration-and-exploitation">#</a>
</h1>
<ul>
<li><strong>Exploitation</strong> : Make the best decision given current information</li>
<li><strong>Exploration</strong> : Gather more information</li>
</ul>
<p>너무 탐험만 하면: 성능이 낮은 행동도 계속 시도 → 학습은 느리고, 보상은 낮음 <br>
너무 이용만 하면: 더 나은 행동을 아예 시도하지 않음 → **지역 최적해(local optimum)**에 갇힘 <br>
→ 따라서, 단기 보상 vs 장기 학습 사이의 균형을 잡는 것이 중요합니다. <br></p>
<p>이 섹션에서는 여러 방법들을 제시하고 있습니다
[미완]</p>
<h1 id="10-case-study-rl-in-classic-games">
  <strong>10. Case Study: RL in Classic Games</strong>
  <a class="anchor" href="#10-case-study-rl-in-classic-games">#</a>
</h1>
<p>[미완]</p>
<hr>
<h1 id="999-deep-rl">
  <strong>999. Deep RL</strong>
  <a class="anchor" href="#999-deep-rl">#</a>
</h1>
<p>RL과 딥러닝의 결합 <br>
딥러닝이 사용하는 위치</p>
<ul>
<li><strong>Policy Network</strong> : 현재 상태에서 행동 분포를 출력</li>
<li><strong>Value Network</strong> : 상태나 상태-행동의 가치를 출력</li>
<li><strong>Q-network</strong> : Q(s, a)를 직접 추정</li>
<li><strong>Model Network</strong> : 환경 dynamics (transition, reward)를 예측 (model-based RL에서만 사용)</li>
</ul>
<h1 id="a-balsa">
  <strong>a. Balsa</strong>
  <a class="anchor" href="#a-balsa">#</a>
</h1>
<p>Balsa는 쿼리 플랜을 순차적으로 구성하는 문제를 <strong>Markov Decision Process (MDP)</strong> 으로 보고,
이를 강화학습으로 해결</p>
<ul>
<li>State s = 현재까지 만들어진 partial query plan</li>
<li>Action a = 다음에 어떤 테이블을 조인할지 결정</li>
<li>Reward r = 쿼리 플랜의 실행 비용 또는 latency</li>
<li>Environment = DB 쿼리 시뮬레이터 or Costmodel</li>
</ul>
<p>추가적으로</p>
<ul>
<li>simulation phase(step) 을 가져서 재앙적 plan을 탐험하지 않게하고,</li>
<li>Timeout을 둬서 Safe Execution 시간을 보장했다. (재앙적 plan이 선택되더라도 timeout으로 하한보장)</li>
<li>value network를 simple tree convolution networks 로 구성</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 여기에서 모델은 강화학습의 environment의 모델이 아니라, value function을 근사할(계산할) treeconv 모델을 의미함</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">MakeModel</span>(p, exp, dataset):
</span></span><span style="display:flex;"><span>    dev <span style="color:#f92672">=</span> GetDevice()
</span></span><span style="display:flex;"><span>    num_label_bins <span style="color:#f92672">=</span> int(
</span></span><span style="display:flex;"><span>        dataset<span style="color:#f92672">.</span>costs<span style="color:#f92672">.</span>max()<span style="color:#f92672">.</span>item()) <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>  <span style="color:#75715e"># +1 for 0, +1 for ceil(max cost).</span>
</span></span><span style="display:flex;"><span>    query_feat_size <span style="color:#f92672">=</span> len(exp<span style="color:#f92672">.</span>query_featurizer(exp<span style="color:#f92672">.</span>nodes[<span style="color:#ae81ff">0</span>]))
</span></span><span style="display:flex;"><span>    batch <span style="color:#f92672">=</span> exp<span style="color:#f92672">.</span>featurizer(exp<span style="color:#f92672">.</span>nodes[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> batch<span style="color:#f92672">.</span>ndim <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    plan_feat_size <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>tree_conv:
</span></span><span style="display:flex;"><span>        labels <span style="color:#f92672">=</span> num_label_bins <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>cross_entropy <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> TreeConvolution(feature_size<span style="color:#f92672">=</span>query_feat_size,
</span></span><span style="display:flex;"><span>                               plan_size<span style="color:#f92672">=</span>plan_feat_size,
</span></span><span style="display:flex;"><span>                               label_size<span style="color:#f92672">=</span>labels,
</span></span><span style="display:flex;"><span>                               version<span style="color:#f92672">=</span>p<span style="color:#f92672">.</span>tree_conv_version)<span style="color:#f92672">.</span>to(dev)
</span></span></code></pre></div><h1 id="b-logger">
  <strong>b. LOGGER</strong>
  <a class="anchor" href="#b-logger">#</a>
</h1>
<ul>
<li>e-beam search 소개 [Exploration and exploitation]</li>
<li>loss function reward weighting을 통해서 poor operator에 의한 fluctuation 방지</li>
<li>log transformation 을 통해서 reward의 범위를 압축 (재앙적 plan의 영향도를 감쇄)</li>
<li>ROSS Restricted Operator Search Space. (최적을 찾지 않고 최악이 안골라지게 해서 효율적)</li>
<li>policy nertwork (GNN + LSTM)</li>
</ul>
<h1 id="c-reload">
  <strong>c. RELOAD</strong>
  <a class="anchor" href="#c-reload">#</a>
</h1>
<p>Balsa + MAML + PER</p>
<h2 id="model-agnostic-meta-learningmaml">
  <strong>Model-Agnostic Meta-Learning(MAML)</strong>
  <a class="anchor" href="#model-agnostic-meta-learningmaml">#</a>
</h2>
<p>모든 task에 잘 작동하는 하나의 모델을 학습하는 것&quot;이 아니라, <br>
조금만 fine-tuning 하면 각 task에 잘 작동할 수 있는 초기 모델&quot;을 학습하는 것.</p>
<h2 id="per">
  <strong>PER</strong>
  <a class="anchor" href="#per">#</a>
</h2>
<p>위에 언급 [ 생략 ]</p>
<h2 id="참조">
  참조
  <a class="anchor" href="#%ec%b0%b8%ec%a1%b0">#</a>
</h2>
<p><a href="https://davidstarsilver.wordpress.com/teaching/">https://davidstarsilver.wordpress.com/teaching/</a></p>
<p><a href="https://wnthqmffhrm.tistory.com/10">https://wnthqmffhrm.tistory.com/10</a></p>
<p><a href="https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-5-model-free-control-.pdf">https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-5-model-free-control-.pdf</a></p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">
<script src="https://giscus.app/client.js"
        data-repo="JaeSim/JaeSim.github.io"
        data-repo-id="R_kgDOOtVVtw"
        data-category="Comment"
        data-category-id="DIC_kwDOOtVVt84Cqcxg"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="ko"
        crossorigin="anonymous"
        async>
</script>

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#on-policyoff-policy-intro"><strong>on-policy/off-policy intro</strong></a></li>
    <li><a href="#on-policy"><strong>on-policy</strong></a>
      <ul>
        <li><a href="#monte-carlo-iteration"><strong>Monte-carlo iteration</strong></a></li>
        <li><a href="#ε-greedy"><strong>ε-greedy</strong></a></li>
        <li><a href="#monte-carlo-control"><strong>Monte-carlo Control</strong></a></li>
        <li><a href="#sarsa"><strong>Sarsa</strong></a></li>
      </ul>
    </li>
    <li><a href="#off-policy"><strong>off-policy</strong></a>
      <ul>
        <li><a href="#q-learning"><strong>Q-learning</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#action-value-function-approximation"><strong>Action-value function Approximation</strong></a></li>
    <li><a href="#batch-method"><strong>Batch Method</strong></a>
      <ul>
        <li><a href="#experience-replay"><strong>Experience Replay</strong></a></li>
        <li><a href="#prioritized-experience-replay-per"><strong>Prioritized Experience Replay (PER)</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#actor-critic"><strong>Actor-Critic</strong></a>
      <ul>
        <li><a href="#proximal-policy-optimization-ppo"><strong>Proximal Policy Optimization (PPO)</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#model-based-rl"><strong>Model-based RL</strong></a></li>
    <li><a href="#simulation-based-search"><strong>Simulation-based Search</strong></a></li>
  </ul>

  <ul>
    <li><a href="#model-agnostic-meta-learningmaml"><strong>Model-Agnostic Meta-Learning(MAML)</strong></a></li>
    <li><a href="#per"><strong>PER</strong></a></li>
    <li><a href="#참조">참조</a></li>
  </ul>
</nav>

  <div class="post-tags">
    <strong>Tags:</strong>
    
      <a href="/tags/definition" class="tag">Definition</a>
    
      <a href="/tags/model-free" class="tag">Model Free</a>
    
      <a href="/tags/monte-carlo" class="tag">Monte Carlo</a>
    
      <a href="/tags/temporal-difference" class="tag">Temporal Difference</a>
    
      <a href="/tags/reinforcement-learning" class="tag">Reinforcement Learning</a>
    
  </div>



  <div class="post-categories">
    <strong>Categories:</strong>
    
      <a href="/categories/reinforcement-learning" class="category">Reinforcement Learning</a>
    
  </div>




 
      </div>
    </aside>
    
  </main>

  <script>
  document.addEventListener('DOMContentLoaded', () => {
    document.querySelectorAll('pre > code[class^="language-"]').forEach(codeBlock => {
      const pre = codeBlock.parentElement;

      
      const button = document.createElement('button');
      button.innerText = '📋';
      button.title = 'Copy code';
      button.style = `
        position: absolute;
        top: 0.5em;
        right: 0.5em;
        padding: 2px 6px;
        font-size: 0.8rem;
        background: #f5f5f5;
        border: 1px solid #ccc;
        border-radius: 4px;
        cursor: pointer;
        z-index: 10;
      `;

      
      button.addEventListener('click', () => {
        navigator.clipboard.writeText(codeBlock.innerText);
        button.innerText = '✔';
        setTimeout(() => button.innerText = '📋', 1000);
      });

      
      pre.style.position = 'relative';
      pre.appendChild(button);
    });
  });
</script>

</body>
</html>












