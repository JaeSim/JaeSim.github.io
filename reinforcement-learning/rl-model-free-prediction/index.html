<!DOCTYPE html>
<html lang="ko-kr" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  4. model-free prediction
  #


  // NOTE: ì´ í˜ì´ì§€ëŠ” ì„ì‹œë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

3ì¥ DPì— ìˆëŠ”ê²ƒì²˜ëŸ¼, Model-free prediction í•˜ê³ , Model-free control í•˜ëŠ” ìˆœì„œë¡œ ì§„í–‰ëœë‹¤.
episode : ì—ì´ì „íŠ¸ê°€ ì‹œì‘ ìƒíƒœì—ì„œ í–‰ë™ì„ ì‹œì‘í•´ì„œ, ì–´ë–¤ ì¢…ë£Œ ì¡°ê±´(End state)ì— ë„ë‹¬í•  ë•Œê¹Œì§€ì˜ ì „ì²´ ê³¼ì •
MCì™€ TDëŠ” major model-free algo.

Monte Carlo (MC) : í•œ ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° í›„, ê·¸ ì „ì²´ ë¦¬í„´ ê°’ì„ ì´ìš©í•˜ì—¬ value functionì„ ì—…ë°ì´íŠ¸
MCëŠ” í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œ ì „ì²´(ì‹œì‘ ~ ì¢…ë£Œ)ë¥¼ ê´€ì°°í•œ ë’¤, 
ì‹¤ì œë¡œ ë°›ì€ rewardë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. 
í™˜ê²½ ëª¨ë¸ ì—†ì´, ê²½í—˜ë§Œìœ¼ë¡œ value functionì´ë‚˜ policyë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. 
Monte-Carlo policy evaluation uses empirical mean return
instead of expected return 
ë†’ì€ varianceì™€ zero biasë¥¼ ê°€ì§

ì¦‰, ê°€ì¥ ì†ì‰¬ìš´ ë°©ë²•ìœ¼ë¡œ epsideë¥¼ ëŒë ¤ë³´ê³  ê°€ëŠ¥ì„œë“¤ì˜ mean ê°’ìœ¼ë¡œ ì²˜ë¦¬ 
ë°©ë¬¸í• ë•Œë§ˆë‹¤ íšŸìˆ˜ì™€ í† íƒˆ returnì„ ëŠ˜ë¦¬ê³ , ì´ê²ƒì˜ í‰ê· ì„ í†µí•´ value functionì„ estimateí•œë‹¤.
lecture-4, 7 page




  \[
V(S_t) \leftarrow V(S_t) &#43; \frac{1}{N(S_t)} \left(G_t - V(S_t)\right)
\]

">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://JaeSim.github.io/reinforcement-learning/rl-model-free-prediction/">
  <meta property="og:site_name" content="JaeSim&#39;s Workspace">
  <meta property="og:title" content="temp. 4. Model Free Prediction">
  <meta property="og:description" content="4. model-free prediction#// NOTE: ì´ í˜ì´ì§€ëŠ” ì„ì‹œë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.3ì¥ DPì— ìˆëŠ”ê²ƒì²˜ëŸ¼, Model-free prediction í•˜ê³ , Model-free control í•˜ëŠ” ìˆœì„œë¡œ ì§„í–‰ëœë‹¤.
episode : ì—ì´ì „íŠ¸ê°€ ì‹œì‘ ìƒíƒœì—ì„œ í–‰ë™ì„ ì‹œì‘í•´ì„œ, ì–´ë–¤ ì¢…ë£Œ ì¡°ê±´(End state)ì— ë„ë‹¬í•  ë•Œê¹Œì§€ì˜ ì „ì²´ ê³¼ì •
MCì™€ TDëŠ” major model-free algo.
Monte Carlo (MC) : í•œ ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° í›„, ê·¸ ì „ì²´ ë¦¬í„´ ê°’ì„ ì´ìš©í•˜ì—¬ value functionì„ ì—…ë°ì´íŠ¸
MCëŠ” í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œ ì „ì²´(ì‹œì‘ ~ ì¢…ë£Œ)ë¥¼ ê´€ì°°í•œ ë’¤, ì‹¤ì œë¡œ ë°›ì€ rewardë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. í™˜ê²½ ëª¨ë¸ ì—†ì´, ê²½í—˜ë§Œìœ¼ë¡œ value functionì´ë‚˜ policyë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. Monte-Carlo policy evaluation uses empirical mean return instead of expected return ë†’ì€ varianceì™€ zero biasë¥¼ ê°€ì§ ì¦‰, ê°€ì¥ ì†ì‰¬ìš´ ë°©ë²•ìœ¼ë¡œ epsideë¥¼ ëŒë ¤ë³´ê³  ê°€ëŠ¥ì„œë“¤ì˜ mean ê°’ìœ¼ë¡œ ì²˜ë¦¬ ë°©ë¬¸í• ë•Œë§ˆë‹¤ íšŸìˆ˜ì™€ í† íƒˆ returnì„ ëŠ˜ë¦¬ê³ , ì´ê²ƒì˜ í‰ê· ì„ í†µí•´ value functionì„ estimateí•œë‹¤. lecture-4, 7 page \[V(S_t) \leftarrow V(S_t) &#43; \frac{1}{N(S_t)} \left(G_t - V(S_t)\right)\]">
  <meta property="og:locale" content="ko_kr">
  <meta property="og:type" content="article">
    <meta property="article:section" content="reinforcement-learning">
    <meta property="article:published_time" content="2025-06-12T10:57:55+09:00">
    <meta property="article:modified_time" content="2025-06-12T10:57:55+09:00">
    <meta property="article:tag" content="Definition">
    <meta property="article:tag" content="Model Free">
    <meta property="article:tag" content="Monte Carlo">
    <meta property="article:tag" content="Temporal Difference">
    <meta property="article:tag" content="Reinforcement Learning">
<title>temp. 4. Model Free Prediction | JaeSim&#39;s Workspace</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://JaeSim.github.io/reinforcement-learning/rl-model-free-prediction/">
<link rel="stylesheet" href="/book.min.e9f68c3fff3d8236a489d16a9caf6de5e4d1a29c20eb4b5524e42cd30be4d319.css" integrity="sha256-6faMP/89gjakidFqnK9t5eTRopwg60tVJOQs0wvk0xk=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.cd8fdad82ebc0a9fd91674c3f25021931c8438f3460432ca8cee03d70aa602e2.js" integrity="sha256-zY/a2C68Cp/ZFnTD8lAhkxyEOPNGBDLKjO4D1wqmAuI=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>JaeSim&#39;s Workspace</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/aboutme/" class="">About Me</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>Development / ê°œë°œ</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/development/secondpost/" class="">ë‘ë²ˆì§¸ ê¸€ì…ë‹ˆë‹¤. ë¸”ë¡œê·¸ ì‹¤í—˜ìš© í…ŒìŠ¤íŠ¸ ê¸€ì…ë‹ˆë‹¤.</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/development/firstpost/" class="">í• ì¼</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/development/blogsetup/" class="">Github pages ë¥¼ ì´ìš©í•œ blog Setting (hugo &#43; hugo-book theme &#43; giscus</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>Reinforcement Learning / ê°•í™”í•™ìŠµ</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-essential/" class="">1. Reinforcement Learning Essential</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-mdp/" class="">2. Markov Decision Process</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-dp/" class="">3. Planning by Dynamic Programming</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/reinforcement-learning/rl-model-free-prediction/" class="active">temp. 4. Model Free Prediction</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>Learned Query Optimizer</span>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>temp. 4. Model Free Prediction</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#on-policyoff-policy-intro"><strong>on-policy/off-policy intro</strong></a></li>
    <li><a href="#on-policy"><strong>on-policy</strong></a>
      <ul>
        <li><a href="#monte-carlo-iteration"><strong>Monte-carlo iteration</strong></a></li>
        <li><a href="#Îµ-greedy"><strong>Îµ-greedy</strong></a></li>
        <li><a href="#monte-carlo-control"><strong>Monte-carlo Control</strong></a></li>
        <li><a href="#sarsa"><strong>Sarsa</strong></a></li>
      </ul>
    </li>
    <li><a href="#off-policy"><strong>off-policy</strong></a>
      <ul>
        <li><a href="#q-learning"><strong>Q-learning</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#action-value-function-approximation"><strong>Action-value function Approximation</strong></a></li>
    <li><a href="#batch-method"><strong>Batch Method</strong></a>
      <ul>
        <li><a href="#experience-replay"><strong>Experience Replay</strong></a></li>
        <li><a href="#prioritized-experience-replay-per"><strong>Prioritized Experience Replay (PER)</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#actor-critic"><strong>Actor-Critic</strong></a>
      <ul>
        <li><a href="#proximal-policy-optimization-ppo"><strong>Proximal Policy Optimization (PPO)</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#model-based-rl"><strong>Model-based RL</strong></a></li>
    <li><a href="#simulation-based-search"><strong>Simulation-based Search</strong></a></li>
  </ul>

  <ul>
    <li><a href="#model-agnostic-meta-learningmaml"><strong>Model-Agnostic Meta-Learning(MAML)</strong></a></li>
    <li><a href="#per"><strong>PER</strong></a></li>
    <li><a href="#ì°¸ì¡°">ì°¸ì¡°</a></li>
  </ul>
</nav>

  <div class="post-tags">
    <strong>Tags:</strong>
    
      <a href="/tags/definition" class="tag">Definition</a>
    
      <a href="/tags/model-free" class="tag">Model Free</a>
    
      <a href="/tags/monte-carlo" class="tag">Monte Carlo</a>
    
      <a href="/tags/temporal-difference" class="tag">Temporal Difference</a>
    
      <a href="/tags/reinforcement-learning" class="tag">Reinforcement Learning</a>
    
  </div>



  <div class="post-categories">
    <strong>Categories:</strong>
    
      <a href="/categories/reinforcement-learning" class="category">Reinforcement Learning</a>
    
  </div>





  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="4-model-free-prediction">
  <strong>4. model-free prediction</strong>
  <a class="anchor" href="#4-model-free-prediction">#</a>
</h1>
<blockquote class="book-hint warning">
  // NOTE: ì´ í˜ì´ì§€ëŠ” ì„ì‹œë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
</blockquote>
<p>3ì¥ DPì— ìˆëŠ”ê²ƒì²˜ëŸ¼, Model-free prediction í•˜ê³ , Model-free control í•˜ëŠ” ìˆœì„œë¡œ ì§„í–‰ëœë‹¤.</p>
<p>episode : ì—ì´ì „íŠ¸ê°€ ì‹œì‘ ìƒíƒœì—ì„œ í–‰ë™ì„ ì‹œì‘í•´ì„œ, ì–´ë–¤ ì¢…ë£Œ ì¡°ê±´(End state)ì— ë„ë‹¬í•  ë•Œê¹Œì§€ì˜ ì „ì²´ ê³¼ì •</p>
<p>MCì™€ TDëŠ” major model-free algo.</p>
<ul>
<li><strong>Monte Carlo (MC)</strong> : í•œ ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° í›„, ê·¸ ì „ì²´ ë¦¬í„´ ê°’ì„ ì´ìš©í•˜ì—¬ value functionì„ ì—…ë°ì´íŠ¸<br>
MCëŠ” <strong>í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œ ì „ì²´</strong>(ì‹œì‘ ~ ì¢…ë£Œ)ë¥¼ ê´€ì°°í•œ ë’¤, <br>
ì‹¤ì œë¡œ ë°›ì€ rewardë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. <br>
í™˜ê²½ ëª¨ë¸ ì—†ì´, ê²½í—˜ë§Œìœ¼ë¡œ value functionì´ë‚˜ policyë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. <br>
Monte-Carlo policy evaluation uses <em>empirical mean</em> return
instead of expected return <br>
ë†’ì€ varianceì™€ zero biasë¥¼ ê°€ì§</li>
</ul>
<p>ì¦‰, ê°€ì¥ ì†ì‰¬ìš´ ë°©ë²•ìœ¼ë¡œ epsideë¥¼ ëŒë ¤ë³´ê³  ê°€ëŠ¥ì„œë“¤ì˜ mean ê°’ìœ¼ë¡œ ì²˜ë¦¬ <br>
ë°©ë¬¸í• ë•Œë§ˆë‹¤ íšŸìˆ˜ì™€ í† íƒˆ returnì„ ëŠ˜ë¦¬ê³ , ì´ê²ƒì˜ í‰ê· ì„ í†µí•´ value functionì„ estimateí•œë‹¤.
lecture-4, 7 page

<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \[
V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)} \left(G_t - V(S_t)\right)
\]
</span>
</p>
<p>Blackjack ì˜ˆì œì²˜ëŸ¼,
í™•ë¥ ì´ë‚˜, ë¶„í¬ ê·¸ëŸ°ê²ƒ ì „í˜€ ì—†ì´ episode ë¡œ ë¶€í„° value functionì„ ë§Œë“¤ì–´ëƒˆë‹¤. (~500,000ë°˜ë³µí•˜ë©´ì„œ)</p>
<ul>
<li><strong>Temporal Difference (TD)</strong> : ì—í”¼ì†Œë“œê°€ ëë‚˜ì§€ ì•Šì•„ë„, ë‹¤ìŒ ìƒíƒœì˜ í˜„ì¬ ì¶”ì • ê°’ì„ ì‚¬ìš©í•´ ë°”ë¡œ ì—…ë°ì´íŠ¸ <br>
incomplete episodes ë¥¼ bootstrapingì„ í†µí•´ì„œ ì—…ë°ì´íŠ¸ <br>
ì•„ì§ ì—í”¼ì†Œë“œê°€ ëë‚˜ì§€ ì•Šì•„ì„œ ë‚˜ë¨¸ì§€ ì˜ˆìƒë˜ëŠ” rewardë¥¼ í¬í•¨í•´ì„œ value functionì„ ì—…ë°ì´íŠ¸í•¨ <br>
ë”°ë¼ì„œ biasê°€ ìˆìŒ + ë‚®ì€ varianceë¥¼ ê°€ì§<br>
ì´ê²ƒì€ Markov propertyë¥¼ í™œìš©í•œë‹¤.</li>
</ul>
<span>
  \[
V(S_t) \leftarrow V(S_t) + \alpha \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)
\]
</span>

<ul>
<li><strong>TD(<span>
  \(\lambda\)
</span>
)</strong> : ì—¬ëŸ¬ step + ê°€ì¤‘í•©ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ?  MCì™€ TDì˜ ì¤‘ê°„<br>
MC &lt;-&gt; TDëŠ” ì „ì²´ ì—í”¼ì†Œë“œë¥¼ ë³´ëŠëƒ, ì¼ë¶€ë¶„ë§Œ ë³´ëŠëƒì˜ ì°¨ì´.<br>
TDì˜ stepì„ 0~n (nì´ ë˜ë©´ MCì™€ ê°™ìŒ) ì‚¬ì´ë¥¼ <span>
  \(\lambda\)
</span>
ë¡œ ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•´ ì‚¬ìš©í•˜ëŠ”ê²ƒ<br>
Monte-Carlo Reinforcement Learning ì€ model-free ì´ë‹¤.
ì™œëƒí•˜ë©´ MDP Transition ì— ëŒ€í•œ (rewardì— ëŒ€í•œ) ì§€ì‹ì´ ì—†ê¸° ë•Œë¬¸.</li>
</ul>
<p>Temporal-Difference Learning ì€ model-free</p>
<img src="/images/rl-backup-category.png" alt="rl-essential" style="width:80%;" />
<h1 id="5-model-free-control">
  <strong>5. model-free control</strong>
  <a class="anchor" href="#5-model-free-control">#</a>
</h1>
<h2 id="on-policyoff-policy-intro">
  <strong>on-policy/off-policy intro</strong>
  <a class="anchor" href="#on-policyoff-policy-intro">#</a>
</h2>
<p>Ï€ = Target Policy
Âµ = Behavior Policy</p>
<p>ì´ë‘ê°œê°€ ê°™ìœ¼ë©´ on-policy ë‹¤ë¥´ë©´ off-policy.</p>
<ul>
<li>
<p>Off-policy learning : â€œLook over someoneâ€™s shoulderâ€ <br>
Learn about policy Ï€ from experience sampled from Âµ <br>
<strong>re-use</strong> experience generated from old policy <br>
<strong>Q-Learning</strong> : <span>
  \(\varepsilon\)
</span>
-greedy ë°©ì‹ìœ¼ë¡œ íƒí—˜í•˜ì§€ë§Œ í•™ìŠµì—ëŠ” ë°˜ì˜ ì•ˆí• ìˆ˜ ìˆìŒ(ìµœì ì˜ í–‰ë™ë§Œ ì—…ë°ì´íŠ¸)</p>
</li>
<li>
<p>On-policy learning : â€œLearn on the jobâ€ <br>
Learn about policy Ï€ from experience sampled from Ï€<br>
<strong>Salsa</strong> :on-policy Q-learning. í˜„ì¬ í–‰ë™ì„ ê·¸ëŒ€ë¡œ ë”°ë¼ê°€ë©° í•™ìŠµ</p>
</li>
</ul>
<h2 id="on-policy">
  <strong>on-policy</strong>
  <a class="anchor" href="#on-policy">#</a>
</h2>
<h3 id="monte-carlo-iteration">
  <strong>Monte-carlo iteration</strong>
  <a class="anchor" href="#monte-carlo-iteration">#</a>
</h3>
<p>Monte-Carlo ë°©ë²•ì„ í†µí•´ì„œ Policy Evaluationì€ ê°€ëŠ¥.(=Monte-Carlo Evaluation)<br>
greedy í•˜ê²Œ policy improvementëŠ” action-value í‘ì…˜ì— ëŒ€í•´ì„œë§Œ ê°€ëŠ¥í•˜ë‹¤.
state-value functionì„ í•˜ë ¤ë©´, ëª¨ë¸ì— ëŒ€í•´ ì•Œì•„ì•¼ë§Œ ê°€ëŠ¥í•˜ë‹¤.<br></p>
<p><span>
  \[
\pi'(s) = \arg\max_{a \in \mathcal{A}} \left( \mathcal{R}_s^a + \sum_{s'} \mathcal{P}_{ss'}^a V(s') \right)
\]
</span>

ìœ„ì™€ ëŒ€ë¹„ë˜ê²Œ action-value function(Q í‘ì…˜) ì€ model-freeí•´ì„œ ì•Œìˆ˜ ìˆë‹¤.</p>
<span>
  \[
\pi'(s) = \arg\max_{a \in \mathcal{A}} Q(s, a)
\]
</span>

<p>ì´ë ‡ê²Œ ì•Œê²Œëœ policyë¥¼ ì•„ë˜ <span>
  \(\varepsilon\)
</span>
-greedy  ë°©ì‹ìœ¼ë¡œ improvement í•œë‹¤.</p>
<h3 id="Îµ-greedy">
  <strong>Îµ-greedy</strong>
  <a class="anchor" href="#%ce%b5-greedy">#</a>
</h3>
<p><strong><span>
  \(\varepsilon\)
</span>
-greedy Algo</strong></p>
<p>í•­ìƒ ìµœê³ ì˜ í–‰ë™ë§Œ ê³ ë¥´ë©´ íƒí—˜ì´ ë¶€ì¡±í•˜ê³ , í•­ìƒ ë¬´ì‘ìœ„ë¡œ ê³ ë¥´ë©´ ì„±ëŠ¥ì´ ë‚®ë‹¤. â†’ ë‘˜ ì‚¬ì´ë¥¼ ì ì ˆíˆ ì„ì!</p>
<p>ì˜ˆì‹œ )<br>
Îµ=0.1 (10%) <br>
90% í™•ë¥ ë¡œ í˜„ì¬ ìµœì ì˜ í–‰ë™ <br>
10% í™•ë¥ ë¡œ ëœë¤ í–‰ë™ <br></p>
<p>ì´ê²ƒì€ ìˆ˜í•™ì ìœ¼ë¡œ policyê°€ ì ì°¨ ì¢‹ì•„ì§€ëŠ”ê²ƒì„ ë‚˜íƒ€ë‚´ê³  ìˆ˜ë ´í•œë‹¤ëŠ” ê³„ì‚° ì¦ëª…ì´ ê°€ëŠ¥í•˜ë‹¤</p>
<h3 id="monte-carlo-control">
  <strong>Monte-carlo Control</strong>
  <a class="anchor" href="#monte-carlo-control">#</a>
</h3>
<p>Monde-Carlo Policy iteration ì€ ì´ì „ ì„¹ì…˜ì—ì„œ ì„¤ëª…í•œê²ƒ</p>
<p>Monde-Carlo Control ì€ í•˜ë‚˜ì”©ì˜ episode ê°€ ëë‚œí›„ì— policyë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ”ê²ƒ (episode ë‹¨ìœ„ë¡œ policy improvement)<br>
ì´ë ‡ê²Œ í•´ë„ ë˜ëŠ” ì´ìœ ëŠ”(ìˆ˜ë ´í•˜ëŠ”ì´ìœ ëŠ”) <code>Greedy in the Limit with Infinite Exploration</code> ì„±ì§ˆì„ ë§Œì¡±í•˜ê¸° ë•Œë¬¸ì´ë‹¤..<br>
(policyë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì´ë¯¸ ê°€ì§€ê³  ìˆë‹¤ ë¼ê³  ë³¼ìˆ˜ë„ ìˆë‹¤.) <br>
[í™”ì‚´í‘œ ê·¸ë¦¼ ì¶”ê°€ì˜ˆì •]</p>
<p>GLIE ì„±ì§ˆì— ëŒ€í•œ ì„¤ëª…ì€ ì¶”í›„ ì—…ë°ì´íŠ¸.</p>
<h3 id="sarsa">
  <strong>Sarsa</strong>
  <a class="anchor" href="#sarsa">#</a>
</h3>
<p>MC Controlì€ episodeê°€ ëë‚ ë•Œë§ˆë‹¤ ì •ì±…ì„ ê°œì„ í•˜ëŠ”ë°
SalsaëŠ” ë§¤ stepë§ˆë‹¤ ì •ì±…ì„ ê°œì„ .</p>
<h2 id="off-policy">
  <strong>off-policy</strong>
  <a class="anchor" href="#off-policy">#</a>
</h2>
<p><strong>behaviour policy Âµë¥¼ í†µí•´ì„œ ìˆ˜ì§‘í•˜ê³ , target policy Ï€ ë¥¼ í•™ìŠµí•˜ëŠ”ê²ƒ</strong> <br>
ì´ê²ƒì€ ë‹¤ë¥¸ ë¶„í¬ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ì„±ì§ˆì„ ì´ìš©
<span>
  \[
\mathbb{E}_{X \sim P}[f(X)]
= \sum P(X) f(X)
= \sum Q(X) \frac{P(X)}{Q(X)} f(X)
= \mathbb{E}_{X \sim Q} \left[ \frac{P(X)}{Q(X)} f(X) \right]
\]
</span>
</p>
<p>ì´ê²ƒì€ ë‹¤ìŒê³¼ ê°™ì´ value function ê³„ì‚°ì— ì£¼ì…ëœë‹¤.
<span>
  \[
G_t^{\pi / \mu} =
\frac{\pi(A_t \mid S_t)}{\mu(A_t \mid S_t)}
\frac{\pi(A_{t+1} \mid S_{t+1})}{\mu(A_{t+1} \mid S_{t+1})}
\cdots
\frac{\pi(A_T \mid S_T)}{\mu(A_T \mid S_T)}
G_t
\]
</span>

<span>
  \[
V(S_t) \leftarrow V(S_t) + \alpha \left( G_t^{\pi / \mu} - V(S_t) \right)
\]
</span>
</p>
<h3 id="q-learning">
  <strong>Q-learning</strong>
  <a class="anchor" href="#q-learning">#</a>
</h3>
<p>action-value Q(s,a) ì˜ off-policy learning <br>
ë‹¤ìŒ actionì„ ì„ íƒí• ë•Œ behaviour policyë¡œë¶€í„° ê³ ë¥´ê³ , <span>
  \(A_{t+1} \sim \mu(\cdot \mid S_t)\)
</span>
 <br>
í•™ìŠµì€ target policy ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰. <span>
  \(A' \sim \pi(\cdot \mid S_t)\)
</span>
<br>
<strong>ìœ„ì™€ ê°™ì´ ì§„í–‰í•´ë„, Q-learingì€ ê²°êµ­ì—ëŠ” <br>
ì˜µí‹°ë©€í•œ action-value (q) functionì— ìˆ˜ë ´í•œë‹¤ëŠ” ì„±ì§ˆì„ ì´ìš©í•œê²ƒ.</strong> <br></p>
<p>ë°‘ì—ëŠ” improvement í•˜ëŠ” ìˆ˜ì‹.
<span>
  \[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( R_{t+1} + \gamma Q(S_{t+1}, A') - Q(S_t, A_t) \right)
\]
</span>
</p>
<p>ì´ë ‡ê²Œ í•˜ë©´ ë³µì¡í•œ weight ê³„ì‚°ì‹ ì„ í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.</p>
<p>[ë§ˆì§€ë§‰ ë©ì—… í‘œ ì¶”ê°€ ì˜ˆì •]</p>
<h1 id="6-value-function-approximation">
  <strong>6. value function approximation</strong>
  <a class="anchor" href="#6-value-function-approximation">#</a>
</h1>
<p>large MDPë¥¼ í’€ìˆ˜ ì—†ìœ¼ë‹ˆ (too many state and action) value function(state-value/action-value function)ì„ ì–´ë–»ê²Œ ê·¼ì‚¬í•˜ê²Œ êµ¬í•˜ëŠ”ê°€?</p>
<ul>
<li>Liner combinations of feature</li>
<li>Neural network</li>
</ul>
<p>ì—¬ê¸°ì„œë¶€í„° Gradient Descent ê°€ ë‚˜ì˜¨ë‹¤.</p>
<blockquote>
<ol>
<li>ì •ì±… Ï€ ê³ ì •</li>
<li>Q-functionì„ gradient descentë¡œ ê·¼ì‚¬ (policy evaluation)</li>
<li>ê·¼ì‚¬ëœ Q ê¸°ë°˜ìœ¼ë¡œ ì •ì±… ê°œì„  (policy improvement)</li>
<li>ë‹¤ì‹œ ë°˜ë³µ (â‡’ ì ì§„ì ìœ¼ë¡œ ìµœì  ì •ì±…ì— ìˆ˜ë ´)</li>
</ol></blockquote>
<p>value functionì„ ê·¼ì‚¬í•´ì„œ ì‚¬ìš©í•˜ë¯€ë¡œ <strong>value-based</strong> ë¼ê³ ë„ í•œë‹¤.</p>
<h2 id="action-value-function-approximation">
  <strong>Action-value function Approximation</strong>
  <a class="anchor" href="#action-value-function-approximation">#</a>
</h2>
<p>ì•„ë˜ì™€ ê°™ì´ action value functionì€ approximateë¥¼ í†µí•´ì„œ í‘œí˜„ë ìˆ˜ ìˆê³ .
ë¸íƒ€ Wë¥¼ ì‘ê²Œí•˜ë¯€ë¡œì¨ ê·¼ì‚¬ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.
<span>
  \[
J(\mathbf{w}) = \mathbb{E}_{\pi} \left[ \left( q_{\pi}(S, A) - \hat{q}(S, A, \mathbf{w}) \right)^2 \right]
\]
</span>
</p>
<p><span>
  \[
- \frac{1}{2} \nabla_{\mathbf{w}} J(\mathbf{w}) 
= \left( q_{\pi}(S, A) - \hat{q}(S, A, \mathbf{w}) \right) \nabla_{\mathbf{w}} \hat{q}(S, A, \mathbf{w})
\]
</span>

<span>
  \[
\Delta \mathbf{w} = \alpha \left( q_{\pi}(S, A) - \hat{q}(S, A, \mathbf{w}) \right) \nabla_{\mathbf{w}} \hat{q}(S, A, \mathbf{w})
\]
</span>
</p>
<h2 id="batch-method">
  <strong>Batch Method</strong>
  <a class="anchor" href="#batch-method">#</a>
</h2>
<p>ìœ„ì˜ gradient descent í• ë•Œ samplingì„ íš¨ìœ¨ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•œ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ë“¤</p>
<h3 id="experience-replay">
  <strong>Experience Replay</strong>
  <a class="anchor" href="#experience-replay">#</a>
</h3>
<p>ê³¼ê±°ì˜ transitionë“¤ì„ ë²„ë¦¬ì§€ ì•Šê³  bufferì— ì €ì¥í•´ ë‘ì—ˆë‹¤ê°€,
í•™ìŠµí•  ë•Œë§ˆë‹¤ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•´ì„œ ì‚¬ìš©</p>
<h3 id="prioritized-experience-replay-per">
  <strong>Prioritized Experience Replay (PER)</strong>
  <a class="anchor" href="#prioritized-experience-replay-per">#</a>
</h3>
<p>ëª¨ë“  transitionì„ ê· ë“±í•˜ê²Œ ìƒ˜í”Œí•˜ì§€ ì•Šê³ ,
TD-errorê°€ í° transitionì— ë” ë†’ì€ í™•ë¥ ì„ ë¶€ì—¬í•˜ì—¬ í•™ìŠµ</p>
<p>ì§ê´€: TD-errorê°€ í´ìˆ˜ë¡ ë” í•™ìŠµì´ í•„ìš”í•œ &ldquo;ì¤‘ìš”í•œ&rdquo; ê²½í—˜</p>
<h1 id="7-policy-gradient-method">
  <strong>7. policy gradient Method</strong>
  <a class="anchor" href="#7-policy-gradient-method">#</a>
</h1>
<p>ì´ì „ì„¹ì…˜ì—ì„œëŠ” action-value function(Q í‘ì…˜)ì„ ê·¼ì‚¬í•´ì„œ ì˜µí‹°ë©€í•œ í´ë¦¬ì‹œë¥¼ ì°¾ì•„ê°”ë‹¤ë©´, <br>
policy gradientëŠ” Q ì—†ì´ policy parameterë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸ í•œë‹¤. <strong>policy-based</strong></p>
<ul>
<li>Softmax Policy : ì´ì‚°í–‰ë™ì¼ë•Œ, softmax over logitsìœ¼ë¡œ ì •ì±…ì„ ì •í•˜ê³ , policyì— ëŒ€í•œ gradient ê°’ ê³„ì‚°<br>
ì´ë¥¼ í†µí•´ì„œ í˜„ì¬ iteration ì— ëŒ€í•œ policy íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.</li>
<li>Gaussian Policy : ì—°ì†í–‰ë™ì¼ë•Œ,  ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ì •ì±…ìœ¼ë¡œ ì •í•˜ê³ , policyì— ëŒ€í•œ gradient ê°’ ê³„ì‚°<br>
ì´ë¥¼ í†µí•´ì„œ í˜„ì¬ iteration ì— ëŒ€í•œ policy íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.</li>
</ul>
<blockquote class="book-hint warning">
  <p>policy-based ì—ì„œë„ replay bufferë¥¼ ì“¸ìˆ˜ ìˆëŠ”ê°€?</p>
<ol>
<li>
<p>Policy-basedì—ì„œ Replay Bufferì˜ ì–´ë ¤ì›€
ë¬¸ì œ: Policyê°€ ê³„ì† ë³€í•˜ê¸° ë•Œë¬¸ì—
â€‹</p>
</li>
<li>
<p>PPO, TRPO [	âŒ ë˜ëŠ” ì œí•œì  ì‚¬ìš© ]	ìµœê·¼ì˜ ë°ì´í„°ë§Œ ì‚¬ìš© (very short buffer)</p>
</li>
<li>
<p>SAC (Soft Actor-Critic) [ í™•ë¥ ì  policy ì‚¬ìš© ] (policy gradient ê¸°ë°˜) <br>
í•˜ì§€ë§Œ ì „ì²´ êµ¬ì¡°ëŠ” off-policy ë¡œì¨ replay buffer ì ê·¹ ì‚¬ìš©</p>
</li>
</ol>
<p>by GPT</p>
<p><strong>ì¶”ê°€ research ìš”ë§</strong></p>
</blockquote>
<h2 id="actor-critic">
  <strong>Actor-Critic</strong>
  <a class="anchor" href="#actor-critic">#</a>
</h2>
<p>policy-based + value-based</p>
<p>ActorëŠ” í–‰ë™ì„ ìƒì„±í•˜ê³  <br>
Criticì€ ê·¸ í–‰ë™ì˜ &ldquo;ì¢‹ìŒ&quot;ì„ í‰ê°€í•´ì„œ Advantageë¥¼ ì¶”ì • <br>
ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Actorì˜ policy gradientë¥¼ ê³„ì‚° <br></p>
<p>lacture-7 24page</p>
<p>ê°„ë‹¨í•œ actor-critic êµ¬ì¡°ëŠ” actuion-valueë¥¼ critic í•˜ëŠ”ê²ƒì´ë‹¤.
Criticì€ TD(0)ì„ í†µí•´ì„œ Q-Functionì„ í•™ìŠµí•˜ê³ 
ActorëŠ” ìœ„ í•™ìŠµëœ Q-functionì„ ê¸°ë°˜ìœ¼ë¡œ policy gradientë¥¼ ìˆ˜í–‰í•˜ëŠ”ê²ƒì´ë‹¤.</p>
<h3 id="proximal-policy-optimization-ppo">
  <strong>Proximal Policy Optimization (PPO)</strong>
  <a class="anchor" href="#proximal-policy-optimization-ppo">#</a>
</h3>
<p>Actor-Critic êµ¬ì¡°ì—ì„œ
Clipped Objectiveë¥¼ ë„ì…í•´ì„œ, policyê°€ ë„ˆë¬´ í¬ê²Œ ë°”ë€Œì§€ ì•Šë„ë¡ ì œì•½í•˜ëŠ”ê²ƒ.</p>
<p>ì•„ë˜ëŠ” PPO surrogate objective í•¨ìˆ˜(ëª©ì í•¨ìˆ˜). ì´ë¥¼ gradient ascent(ìµœëŒ€í™”) í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ í˜„ì¬ policyì— ì—…ë°ì´íŠ¸í•˜ë©´ policyëŠ” ì˜µí‹°ë©€ì„ í–¥í•´ê°„ë‹¤.
<span>
  \[
L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta) \hat{A}_t,\; \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
\]
</span>
</p>
<h1 id="8-integration-learning-and-planning">
  <strong>8. Integration Learning and Planning</strong>
  <a class="anchor" href="#8-integration-learning-and-planning">#</a>
</h1>
<h2 id="model-based-rl">
  <strong>Model-based RL</strong>
  <a class="anchor" href="#model-based-rl">#</a>
</h2>
<p><strong>[ìƒëµ]</strong></p>
<h2 id="simulation-based-search">
  <strong>Simulation-based Search</strong>
  <a class="anchor" href="#simulation-based-search">#</a>
</h2>
<p>&ldquo;ì „ì²´ ìƒíƒœ ê³µê°„ì„ ì§ì ‘ í•™ìŠµí•˜ê¸°ëŠ” ë„ˆë¬´ ë¹„ì‹¸ë‹¤. ëŒ€ì‹ , ìœ ë§í•œ ë¶€ë¶„ë§Œ ì§‘ì¤‘ì ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•˜ë©´ì„œ ê±°ê¸°ì„œ ê²½í—˜í•œ ì •ë³´ë¡œ Qê°’ì„ ì ì  ë” ì •í™•í•˜ê²Œ ë§Œë“ ë‹¤.&rdquo;</p>
<p>//note: balsa simulation learning ê³¼ëŠ” ê°œë…ì´ ì¡°ê¸ˆ ë‹¤ë¦„</p>
<p><strong>[ë¯¸ì™„]</strong></p>
<h1 id="9-exploration-and-exploitation">
  <strong>9. Exploration and Exploitation</strong>
  <a class="anchor" href="#9-exploration-and-exploitation">#</a>
</h1>
<ul>
<li><strong>Exploitation</strong> : Make the best decision given current information</li>
<li><strong>Exploration</strong> : Gather more information</li>
</ul>
<p>ë„ˆë¬´ íƒí—˜ë§Œ í•˜ë©´: ì„±ëŠ¥ì´ ë‚®ì€ í–‰ë™ë„ ê³„ì† ì‹œë„ â†’ í•™ìŠµì€ ëŠë¦¬ê³ , ë³´ìƒì€ ë‚®ìŒ <br>
ë„ˆë¬´ ì´ìš©ë§Œ í•˜ë©´: ë” ë‚˜ì€ í–‰ë™ì„ ì•„ì˜ˆ ì‹œë„í•˜ì§€ ì•ŠìŒ â†’ **ì§€ì—­ ìµœì í•´(local optimum)**ì— ê°‡í˜ <br>
â†’ ë”°ë¼ì„œ, ë‹¨ê¸° ë³´ìƒ vs ì¥ê¸° í•™ìŠµ ì‚¬ì´ì˜ ê· í˜•ì„ ì¡ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. <br></p>
<p>ì´ ì„¹ì…˜ì—ì„œëŠ” ì—¬ëŸ¬ ë°©ë²•ë“¤ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤
[ë¯¸ì™„]</p>
<h1 id="10-case-study-rl-in-classic-games">
  <strong>10. Case Study: RL in Classic Games</strong>
  <a class="anchor" href="#10-case-study-rl-in-classic-games">#</a>
</h1>
<p>[ë¯¸ì™„]</p>
<hr>
<h1 id="999-deep-rl">
  <strong>999. Deep RL</strong>
  <a class="anchor" href="#999-deep-rl">#</a>
</h1>
<p>RLê³¼ ë”¥ëŸ¬ë‹ì˜ ê²°í•© <br>
ë”¥ëŸ¬ë‹ì´ ì‚¬ìš©í•˜ëŠ” ìœ„ì¹˜</p>
<ul>
<li><strong>Policy Network</strong> : í˜„ì¬ ìƒíƒœì—ì„œ í–‰ë™ ë¶„í¬ë¥¼ ì¶œë ¥</li>
<li><strong>Value Network</strong> : ìƒíƒœë‚˜ ìƒíƒœ-í–‰ë™ì˜ ê°€ì¹˜ë¥¼ ì¶œë ¥</li>
<li><strong>Q-network</strong> : Q(s, a)ë¥¼ ì§ì ‘ ì¶”ì •</li>
<li><strong>Model Network</strong> : í™˜ê²½ dynamics (transition, reward)ë¥¼ ì˜ˆì¸¡ (model-based RLì—ì„œë§Œ ì‚¬ìš©)</li>
</ul>
<h1 id="a-balsa">
  <strong>a. Balsa</strong>
  <a class="anchor" href="#a-balsa">#</a>
</h1>
<p>BalsaëŠ” ì¿¼ë¦¬ í”Œëœì„ ìˆœì°¨ì ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” ë¬¸ì œë¥¼ <strong>Markov Decision Process (MDP)</strong> ìœ¼ë¡œ ë³´ê³ ,
ì´ë¥¼ ê°•í™”í•™ìŠµìœ¼ë¡œ í•´ê²°</p>
<ul>
<li>State s = í˜„ì¬ê¹Œì§€ ë§Œë“¤ì–´ì§„ partial query plan</li>
<li>Action a = ë‹¤ìŒì— ì–´ë–¤ í…Œì´ë¸”ì„ ì¡°ì¸í• ì§€ ê²°ì •</li>
<li>Reward r = ì¿¼ë¦¬ í”Œëœì˜ ì‹¤í–‰ ë¹„ìš© ë˜ëŠ” latency</li>
<li>Environment = DB ì¿¼ë¦¬ ì‹œë®¬ë ˆì´í„° or Costmodel</li>
</ul>
<p>ì¶”ê°€ì ìœ¼ë¡œ</p>
<ul>
<li>simulation phase(step) ì„ ê°€ì ¸ì„œ ì¬ì•™ì  planì„ íƒí—˜í•˜ì§€ ì•Šê²Œí•˜ê³ ,</li>
<li>Timeoutì„ ë‘¬ì„œ Safe Execution ì‹œê°„ì„ ë³´ì¥í–ˆë‹¤. (ì¬ì•™ì  planì´ ì„ íƒë˜ë”ë¼ë„ timeoutìœ¼ë¡œ í•˜í•œë³´ì¥)</li>
<li>value networkë¥¼ simple tree convolution networks ë¡œ êµ¬ì„±</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># ì—¬ê¸°ì—ì„œ ëª¨ë¸ì€ ê°•í™”í•™ìŠµì˜ environmentì˜ ëª¨ë¸ì´ ì•„ë‹ˆë¼, value functionì„ ê·¼ì‚¬í• (ê³„ì‚°í• ) treeconv ëª¨ë¸ì„ ì˜ë¯¸í•¨</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">MakeModel</span>(p, exp, dataset):
</span></span><span style="display:flex;"><span>    dev <span style="color:#f92672">=</span> GetDevice()
</span></span><span style="display:flex;"><span>    num_label_bins <span style="color:#f92672">=</span> int(
</span></span><span style="display:flex;"><span>        dataset<span style="color:#f92672">.</span>costs<span style="color:#f92672">.</span>max()<span style="color:#f92672">.</span>item()) <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>  <span style="color:#75715e"># +1 for 0, +1 for ceil(max cost).</span>
</span></span><span style="display:flex;"><span>    query_feat_size <span style="color:#f92672">=</span> len(exp<span style="color:#f92672">.</span>query_featurizer(exp<span style="color:#f92672">.</span>nodes[<span style="color:#ae81ff">0</span>]))
</span></span><span style="display:flex;"><span>    batch <span style="color:#f92672">=</span> exp<span style="color:#f92672">.</span>featurizer(exp<span style="color:#f92672">.</span>nodes[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> batch<span style="color:#f92672">.</span>ndim <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    plan_feat_size <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>tree_conv:
</span></span><span style="display:flex;"><span>        labels <span style="color:#f92672">=</span> num_label_bins <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>cross_entropy <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> TreeConvolution(feature_size<span style="color:#f92672">=</span>query_feat_size,
</span></span><span style="display:flex;"><span>                               plan_size<span style="color:#f92672">=</span>plan_feat_size,
</span></span><span style="display:flex;"><span>                               label_size<span style="color:#f92672">=</span>labels,
</span></span><span style="display:flex;"><span>                               version<span style="color:#f92672">=</span>p<span style="color:#f92672">.</span>tree_conv_version)<span style="color:#f92672">.</span>to(dev)
</span></span></code></pre></div><h1 id="b-logger">
  <strong>b. LOGGER</strong>
  <a class="anchor" href="#b-logger">#</a>
</h1>
<ul>
<li>e-beam search ì†Œê°œ [Exploration and exploitation]</li>
<li>loss function reward weightingì„ í†µí•´ì„œ poor operatorì— ì˜í•œ fluctuation ë°©ì§€</li>
<li>log transformation ì„ í†µí•´ì„œ rewardì˜ ë²”ìœ„ë¥¼ ì••ì¶• (ì¬ì•™ì  planì˜ ì˜í–¥ë„ë¥¼ ê°ì‡„)</li>
<li>ROSS Restricted Operator Search Space. (ìµœì ì„ ì°¾ì§€ ì•Šê³  ìµœì•…ì´ ì•ˆê³¨ë¼ì§€ê²Œ í•´ì„œ íš¨ìœ¨ì )</li>
<li>policy nertwork (GNN + LSTM)</li>
</ul>
<h1 id="c-reload">
  <strong>c. RELOAD</strong>
  <a class="anchor" href="#c-reload">#</a>
</h1>
<p>Balsa + MAML + PER</p>
<h2 id="model-agnostic-meta-learningmaml">
  <strong>Model-Agnostic Meta-Learning(MAML)</strong>
  <a class="anchor" href="#model-agnostic-meta-learningmaml">#</a>
</h2>
<p>ëª¨ë“  taskì— ì˜ ì‘ë™í•˜ëŠ” í•˜ë‚˜ì˜ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒ&quot;ì´ ì•„ë‹ˆë¼, <br>
ì¡°ê¸ˆë§Œ fine-tuning í•˜ë©´ ê° taskì— ì˜ ì‘ë™í•  ìˆ˜ ìˆëŠ” ì´ˆê¸° ëª¨ë¸&quot;ì„ í•™ìŠµí•˜ëŠ” ê²ƒ.</p>
<h2 id="per">
  <strong>PER</strong>
  <a class="anchor" href="#per">#</a>
</h2>
<p>ìœ„ì— ì–¸ê¸‰ [ ìƒëµ ]</p>
<h2 id="ì°¸ì¡°">
  ì°¸ì¡°
  <a class="anchor" href="#%ec%b0%b8%ec%a1%b0">#</a>
</h2>
<p><a href="https://davidstarsilver.wordpress.com/teaching/">https://davidstarsilver.wordpress.com/teaching/</a></p>
<p><a href="https://wnthqmffhrm.tistory.com/10">https://wnthqmffhrm.tistory.com/10</a></p>
<p><a href="https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-5-model-free-control-.pdf">https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-5-model-free-control-.pdf</a></p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">
<script src="https://giscus.app/client.js"
        data-repo="JaeSim/JaeSim.github.io"
        data-repo-id="R_kgDOOtVVtw"
        data-category="Comment"
        data-category-id="DIC_kwDOOtVVt84Cqcxg"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="ko"
        crossorigin="anonymous"
        async>
</script>

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#on-policyoff-policy-intro"><strong>on-policy/off-policy intro</strong></a></li>
    <li><a href="#on-policy"><strong>on-policy</strong></a>
      <ul>
        <li><a href="#monte-carlo-iteration"><strong>Monte-carlo iteration</strong></a></li>
        <li><a href="#Îµ-greedy"><strong>Îµ-greedy</strong></a></li>
        <li><a href="#monte-carlo-control"><strong>Monte-carlo Control</strong></a></li>
        <li><a href="#sarsa"><strong>Sarsa</strong></a></li>
      </ul>
    </li>
    <li><a href="#off-policy"><strong>off-policy</strong></a>
      <ul>
        <li><a href="#q-learning"><strong>Q-learning</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#action-value-function-approximation"><strong>Action-value function Approximation</strong></a></li>
    <li><a href="#batch-method"><strong>Batch Method</strong></a>
      <ul>
        <li><a href="#experience-replay"><strong>Experience Replay</strong></a></li>
        <li><a href="#prioritized-experience-replay-per"><strong>Prioritized Experience Replay (PER)</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#actor-critic"><strong>Actor-Critic</strong></a>
      <ul>
        <li><a href="#proximal-policy-optimization-ppo"><strong>Proximal Policy Optimization (PPO)</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#model-based-rl"><strong>Model-based RL</strong></a></li>
    <li><a href="#simulation-based-search"><strong>Simulation-based Search</strong></a></li>
  </ul>

  <ul>
    <li><a href="#model-agnostic-meta-learningmaml"><strong>Model-Agnostic Meta-Learning(MAML)</strong></a></li>
    <li><a href="#per"><strong>PER</strong></a></li>
    <li><a href="#ì°¸ì¡°">ì°¸ì¡°</a></li>
  </ul>
</nav>

  <div class="post-tags">
    <strong>Tags:</strong>
    
      <a href="/tags/definition" class="tag">Definition</a>
    
      <a href="/tags/model-free" class="tag">Model Free</a>
    
      <a href="/tags/monte-carlo" class="tag">Monte Carlo</a>
    
      <a href="/tags/temporal-difference" class="tag">Temporal Difference</a>
    
      <a href="/tags/reinforcement-learning" class="tag">Reinforcement Learning</a>
    
  </div>



  <div class="post-categories">
    <strong>Categories:</strong>
    
      <a href="/categories/reinforcement-learning" class="category">Reinforcement Learning</a>
    
  </div>




 
      </div>
    </aside>
    
  </main>

  <script>
  document.addEventListener('DOMContentLoaded', () => {
    document.querySelectorAll('pre > code[class^="language-"]').forEach(codeBlock => {
      const pre = codeBlock.parentElement;

      
      const button = document.createElement('button');
      button.innerText = 'ğŸ“‹';
      button.title = 'Copy code';
      button.style = `
        position: absolute;
        top: 0.5em;
        right: 0.5em;
        padding: 2px 6px;
        font-size: 0.8rem;
        background: #f5f5f5;
        border: 1px solid #ccc;
        border-radius: 4px;
        cursor: pointer;
        z-index: 10;
      `;

      
      button.addEventListener('click', () => {
        navigator.clipboard.writeText(codeBlock.innerText);
        button.innerText = 'âœ”';
        setTimeout(() => button.innerText = 'ğŸ“‹', 1000);
      });

      
      pre.style.position = 'relative';
      pre.appendChild(button);
    });
  });
</script>

</body>
</html>












