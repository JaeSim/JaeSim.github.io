[{"id":0,"href":"/development/secondpost/","title":"두번째 글입니다. 블로그 실험용 테스트 글입니다.","section":"Development / 개발","content":"\rtest\r#\rtest2\r#\rRL 관련 항목을 기록하기 위한 post 입니다.\n페이지 업로드 확인용 임시 포스트\n"},{"id":1,"href":"/aboutme/","title":"About Me","section":"Home","content":"\rProfile\r#\rName : Jaeyoung Sim\n"},{"id":2,"href":"/development/firstpost/","title":"할일","section":"Development / 개발","content":"개발 관련 항목을 기록하기 위한 post 입니다.\nTodo List:\n상태 날짜 작업 내용 ✅ 2025-05-21 블로그 GitHub Page 연동 ✅ 2025-05-22 RL 관련 Post 남기기 (on-going) ✅ 2025-05-22 hugo + github page + giscus ✅ 2025-05-22 Google 서치 연동 ☐ Naver 서치 연동 ✅ 2025-05-22 블로그 setup 관련 post 남기기 ☐ About Me 완성 ☐ Google 서치 연동 "},{"id":3,"href":"/development/blogsetup/","title":"Github pages 를 이용한 blog Setting (hugo + hugo-book theme + giscus","section":"Development / 개발","content":"\rGithub pages 를 이용한 blog Setting (hugo + hugo-book theme + giscus\r#\r0. blog host 방식 선택\r#\r여러 블로그 host방식을 고려하였으나, 유지보수가 손이 덜가며, 오랫동안 hosting이 되는것이 우선 순위였고.\nGithub pages를 통한 호스팅 방법을 선택하였다.\n그렇다면, static page를 generation을 해주는 framework로 jekyll 와 hugo를 고민하였고 ruby 보다 go로 이루어진 hugo를 믿기로 하였다.\n다른 blog들이 생성한 페이지를 주로 참조하였으며, 이 포스팅은 해당 글들의 엮음에 불과한점을 참조 부탁한다.\n1. Setup git, hugo\r#\r개발환경은 window 11 (Intel processor) 이므로 이를 기반으로 작성되었다.\ngit 설치\r#\rhttps://git-scm.com/downloads 사이트에서 Git for Windows/x64 Setup. 항목을 클릭하여 github을 설치하였다. 각 설정은 default를 그대로 사용하였다.\nhugo 설치\r#\rhttps://github.com/gohugoio/hugo/releases 에서 본인에 맞는 최신 버전을 받는다. (hugo_extended_0.147.4_windows-amd64.zip)\n후에 언급하겠지만 hugo-book theme 의 경우 hugp-extended 버전을 받아야했다.\nhugo 명령어를 치는 경우가 많으므로, 환경변수에 등록하였다. (되도록 path 파싱에 에러가 없도록 폴더들을 영어로 구성하였다.)\ngithub repository 생성\r#\rhttps://minyeamer.github.io/blog/hugo-blog-1/ 에서 언급된것 같이 하나의 repository를 branch로 나누어서 submodule로 활용하는 방안을 채택했다.\n\u0026lt;USERNAME\u0026gt;.github.io 인 repository를 하나 생성한다. UserName이 Jon 이라면 다음과 같이 생성될 것이다. Jon/Jon.github.io\nhugo를 통한 기본 뼈대 만들기. 여기서 폴더명을 생성한 repository로 맞춘다.\nhugo new site \u0026lt;USERNAME\u0026gt;.github.io cd \u0026lt;USERNAME\u0026gt;.github.io git init git add . git commit -m \u0026#34;feat: new site\u0026#34; git branch -M main git remote add origin https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git git push -u origin main branch 만들기 git branch gh-pages main git checkout gh-pages git push origin gh-pages git checkout main gh-pages를 submoudle로 연동하기 rm -rf public git submodule add -b gh-pages https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public git add public git add .gitmodules git commit -m \u0026#34;feat: add submodule for github pages\u0026#34; git push hugo Theme : hugo-book\r#\rhttps://themes.gohugo.io/ 에서 여러 테마가 선택이 가능하고, 가벼워보이는 hugo-book theme 를 채택하였다\nhttps://github.com/alex-shpak/hugo-book 에 Read.md를 참조하였다.\ngit submodule add https://github.com/alex-shpak/hugo-book themes/hugo-book git add . git commit -m \u0026#34;feat: import hugo theme\u0026#34; root path 에 있는 hugo.toml 파일에 아래 내용을 삽입한다.\nbaseURL = \u0026#39;https://\u0026lt;USERNAME\u0026gt;.github.io\u0026#39;\rlanguageCode = \u0026#39;ko-kr\u0026#39;\rtitle = \u0026#34;\u0026lt; title what you want\u0026gt;\u0026#34;\rtheme = \u0026#39;hugo-book\u0026#39; Github 설정\r#\rGithub -\u0026gt; .github.io -\u0026gt; Settings -\u0026gt; Pages -\u0026gt; Branch -\u0026gt; gh-pages 로 변경\n2. 블로그 배포 방법\r#\rhugo static page 생성\r#\r아래 명령어로 \u0026lt;root-path\u0026gt;/content/ path에 firstPost.md 파일이 생성된다\nhugo new firstPost.md 아래 커맨드를 이용하면 현재 static 페이지를 127.0.0.1:1313 에서 확인이 가능하다\nhugo server -D 아래와 같이 draft가 true가 있다면 디버깅은 가능하지만 최종 산출물에서 빠지게 된다. 따라서 나중에 빼놓지 말고 draft = true 문구를 지우던 false로 변경한다\n// firstPost.md\r+++\rtitle = \u0026#39;firstPost\u0026#39;\rdraft = true\r+++ 아래 명령어를 통해서 public/ 폴더 밑에 산출물들을 생성한다.\nhugo 아래 명령어를 통해서 github repository에 산출물들을 배포한다. 이 산출물들은 gitub action 에 의해서 자동으로 hosting되도록 처리된다.\ncd public git add . git commit -m \u0026#34;\u0026lt;comment what you want to add\u0026gt;\u0026#34; git push origin gh-pages cd .. git add . git commit -m \u0026#34;\u0026lt;comment what you want to add\u0026gt;\u0026#34; git push origin main 3. 추가 hugo 세팅팁\r#\rrecent-post 용 home 만들기\r#\r아래와 같이 폴더 및 기본 md 파일을 구성하였고.\ncontent\r├── _index.md\r├── Content-Category1\r│ ├── _index.md\r│ └── article1.md\r└── Content-Category2\r├── _index.md\r└── post1.md conent/_index.md 파일은 다음과 같이 작성하였다.\n\u0026ldquo;recent-posts\u0026rdquo; 에서 \u0026quot; 을 삭제\r--- title: \u0026#34;Home\u0026#34; comments: false --- # **Recent Posts** {{ {{ \u0026#34;recent-posts\u0026#34; }} }} layouts/shortcodes/recent-posts.html 을 생성하여 아래와 같이 작성\n\u0026lt;style\u0026gt; .summary-box { background: #f8f9fa; padding: 1rem; border-radius: 8px; transition: background 0.3s ease; max-height: 14rem; /* 높이 제한 */ overflow: hidden; /* 넘치는 내용 숨김 */ display: -webkit-box; -webkit-line-clamp: 6; /* 최대 줄 수 (예: 6줄) */ -webkit-box-orient: vertical; text-overflow: ellipsis; /* ... 처리 */ } .summary-box h1, .summary-box h2, .summary-box h3, .summary-box h4, .summary-box h5, .summary-box h6, .summary-box p { margin: 0 0 1rem 0; } .summary-box:hover { background: #e9ecef; } \u0026lt;/style\u0026gt; \u0026lt;ul\u0026gt; {{ range (first 5 (sort .Site.RegularPages \u0026#34;Date\u0026#34; \u0026#34;desc\u0026#34;)) }} \u0026lt;li style=\u0026#34;margin-bottom: 2rem;\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt; \u0026lt;strong style=\u0026#34;font-size: 1.25rem;\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/strong\u0026gt; \u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; {{ with .Params.subtitle }} \u0026lt;small style=\u0026#34;color: #888;\u0026#34;\u0026gt;{{ . }}\u0026lt;/small\u0026gt;\u0026lt;br/\u0026gt; {{ end }} \u0026lt;small\u0026gt;{{ .Date.Format \u0026#34;2006-01-02 15:04:05 MST\u0026#34; }}\u0026lt;/small\u0026gt; \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34; style=\u0026#34;text-decoration: none; color: inherit;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;summary-box\u0026#34;\u0026gt; \u0026lt;!-- delete anchor in title--\u0026gt; {{ .Summary | replaceRE \u0026#34;\u0026lt;a class=\\\u0026#34;anchor\\\u0026#34; href=\\\u0026#34;#.*?\\\u0026#34;\u0026gt;#\u0026lt;/a\u0026gt;\u0026#34; \u0026#34;\u0026#34; | safeHTML | truncate 400}} \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; MarkDown code block copy 버튼 만들기\r#\rlayouts/partials/docs/body.html 을 생성하여 아래와 같이 작성\n\u0026lt;script\u0026gt; document.addEventListener(\u0026#39;DOMContentLoaded\u0026#39;, () =\u0026gt; { document.querySelectorAll(\u0026#39;pre \u0026gt; code[class^=\u0026#34;language-\u0026#34;]\u0026#39;).forEach(codeBlock =\u0026gt; { const pre = codeBlock.parentElement; // 코드블럭의 복사 버튼 생성 const button = document.createElement(\u0026#39;button\u0026#39;); button.innerText = \u0026#39;📋\u0026#39;; button.title = \u0026#39;Copy code\u0026#39;; button.style = ` position: absolute; top: 0.5em; right: 0.5em; padding: 2px 6px; font-size: 0.8rem; background: #f5f5f5; border: 1px solid #ccc; border-radius: 4px; cursor: pointer; z-index: 10; `; // 복사 동작 정의 button.addEventListener(\u0026#39;click\u0026#39;, () =\u0026gt; { navigator.clipboard.writeText(codeBlock.innerText); button.innerText = \u0026#39;✔\u0026#39;; setTimeout(() =\u0026gt; button.innerText = \u0026#39;📋\u0026#39;, 1000); }); // 스타일 적용 pre.style.position = \u0026#39;relative\u0026#39;; pre.appendChild(button); }); }); \u0026lt;/script\u0026gt; 4. giscus 연동하기\r#\rgiscus github에 설치 및 discuss 활성화\r#\rhttps://github.com/apps/giscus 에서 giscus install\ngithub page repository 만 선택\nSetting -\u0026gt; Discusss 활성화\nDiscussions 에서 좌측 Categories 선택하여 새 카테고리 만들기\nCategory Name 와 Description을 각각 입력하고 Discussion Format 을 Announcement 선택\nhugo 에서 보이게 설정\r#\rhttps://giscus.app/ko 페이지에 접속하여\n/.github.io 로 저장소 입력 Discussion 제목이 페이지 경로 포함 선택 페이지에 생성된 script를 복사 (아래와 유사한 형식) \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026lt;.github.io\u0026#34; data-repo-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-category=\u0026#34;Comment\u0026#34; data-category-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-mapping=\u0026#34;pathname\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;preferred_color_scheme\u0026#34; data-lang=\u0026#34;ko\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; 이를 `layouts/partials/comments.html에 아래처럼 붙여넣기 {{ if and (.IsPage) (not .IsHome) (not (eq .Params.comments false)) }} \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026lt;.github.io\u0026#34; data-repo-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-category=\u0026#34;Comment\u0026#34; data-category-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-mapping=\u0026#34;pathname\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;preferred_color_scheme\u0026#34; data-lang=\u0026#34;ko\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; {{ end }} comments = false를 넣은 페이지들은 github 코멘트가 안보임\r5. tag 설정\r#\rhugo.toml 에 아래 설정 삽입\n[taxonomies] tag = \u0026#34;tags\u0026#34; category = \u0026#34;categories\u0026#34; 각 포스트마다 아래 front matter에 아래 항목을 기입\n+++ ... tags = [\u0026#34;Definition\u0026#34;, \u0026#34;Essential\u0026#34;] categories = [\u0026#34;Reinforcement Learning\u0026#34;] ... +++ 이 블로그의 경우, 우측 ToC 항목 하단에 위치 Tag 및 Categories를 위치시킴\n이는 테마마다 다르니 각 테마별 적용 방법 확인 필요\nlayouts/partials/doc/inject/toc-after.html 을 생성\n{{ with .Params.tags }} \u0026lt;div class=\u0026#34;post-tags\u0026#34;\u0026gt; \u0026lt;strong\u0026gt;Tags:\u0026lt;/strong\u0026gt; {{ range . }} \u0026lt;a href=\u0026#34;{{ \u0026#34;tags/\u0026#34; | relLangURL }}{{ . | urlize }}\u0026#34; class=\u0026#34;tag\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} {{ with .Params.categories }} \u0026lt;div class=\u0026#34;post-categories\u0026#34;\u0026gt; \u0026lt;strong\u0026gt;Categories:\u0026lt;/strong\u0026gt; {{ range . }} \u0026lt;a href=\u0026#34;{{ \u0026#34;categories/\u0026#34; | relLangURL }}{{ . | urlize }}\u0026#34; class=\u0026#34;category\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} 카테고리 및 tag 선택했을때 해당 list가 보이도록 설정 layouts/_default/term.html 을 생성\n{{ define \u0026#34;main\u0026#34; }} \u0026lt;main\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; {{ range .Pages }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt; {{ .Date }}\u0026lt;/li\u0026gt; {{ else }} \u0026lt;li\u0026gt;\u0026lt;em\u0026gt;No posts found.\u0026lt;/em\u0026gt;\u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; \u0026lt;/main\u0026gt; {{ end }} 6. 검색 연동\r#\r구굴 서치 콘솔\r#\r소유권 확인 https://search.google.com/search-console/about 접속하여 url \u0026lt;USERNAME\u0026gt;.github.io 을 입력 google_.html 을 다운로드 후에 public/ 에 위치시킴\nsitemap 만들기 hugo.toml 에 아래와 같이 삽입 -\u0026gt; sitemap.xml이 생성된것을 확인 enableRobotsTXT = true [sitemap] # always, hourly daily, weekly, monthly, yearly, never changefreq = \u0026#34;weekly\u0026#34; filename = \u0026#34;sitemap.xml\u0026#34; priority = 0.5 layouts/robots.txt 파일을 수정 User-agent: * Allow: / Sitemap: {{ .Site.BaseURL }}sitemap.xml hugo.toml 에서 아래 설정\nenableRobotsTXT = true 아래 커맨드로 빌드시에 public/robots.txt 생성을 확인\nhugo sitemap.xml을 서치 콘솔에 등록 네이버 서치 어드바이져\r#\rnaver search advisor 접속 blog url 입력 html 다운로드후 public 안에 위치시키고 배포 -\u0026gt; 소유권 확인 참조\r#\rhttps://ialy1595.github.io/post/blog-construct-1/\nhttps://ialy1595.github.io/post/blog-construct-2/\nhttps://minyeamer.github.io/blog/hugo-blog-1/\nhttps://d5br5.dev/blog/nextjs_blog/giscus\nhttps://velog.io/@eona1301/Github-Blog-%EA%B2%80%EC%83%89%EC%B0%BD-%EB%85%B8%EC%B6%9C%EC%8B%9C%ED%82%A4%EA%B8%B0\nhttps://golangkorea.github.io/post/hugo-intro/taxonomy-basic/\nhttps://boyinblue.github.io/002_github_blog/003_naver_search_advisor.html\n"},{"id":4,"href":"/learned-query-optimizer/loger-code/","title":"LOGER 코드 분석","section":"Learned Query Optimizer","content":"\rLOGER 프로젝트의 코드 분석 내용\r#\r기본정보\r#\rLOGER: A Learned Optimizer towards Generating Efficient and Robust Query Execution Plans github repository https://github.com/TianyiChen0316/LOGER LOGER 실행을 위한 셋업\r#\r# 3.8 이여야한다. conda create -n loger python=3.8 -y conda activate loger pip install -r requirements.txt pip install pandas pip install dgl pip install packaging conda install libffi export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH pip install dgl-cu110==0.6.1 mkdir results postgre 설치\n아래내용은 balsa 프로젝트에 있는 내용\rcd ~ wget https://ftp.postgresql.org/pub/source/v12.5/postgresql-12.5.tar.gz tar xzvf postgresql-12.5.tar.gz cd postgresql-12.5 ./configure --prefix=$HOME/postgresql-12.5 --without-readline make -j make install echo \u0026#39;export PATH=/home/jae.sim/postgresql-12.5/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc cd ~/ git clone https://github.com/ossc-db/pg_hint_plan.git -b REL12_1_3_7 cd pg_hint_plan/ vim Makefile ### # Modify Makefile: change line # PG_CONFIG = pg_config # to # PG_CONFIG = /home/jae.sim/postgresql-12.5/bin/pg_config pg_ctl -D ~/imdb initdb pg_ctl -D ~/imdb start -l logfile 아래 내용을 추가\n# train.py # 아래 내용을 추가 parser.add_argument(\u0026#39;--host\u0026#39;, type=str, default=\u0026#39;/tmp\u0026#39;, help=\u0026#39;PostgreSQL host path\u0026#39;) LOGER 실행 커맨드\r#\r# start postgre pg_ctl -D ~/imdb start -l logfile conda activate loger ## database 설정은 아래 내용 참조 python train.py --database imdbload --port 5437 --host /tmp -U \u0026#34;\u0026#34; # train.py parser.add_argument(\u0026#39;-D\u0026#39;, \u0026#39;--database\u0026#39;, type=str, default=\u0026#39;imdb\u0026#39;, help=\u0026#39;PostgreSQL database.\u0026#39;) parser.add_argument(\u0026#39;-U\u0026#39;, \u0026#39;--user\u0026#39;, type=str, default=\u0026#39;postgres\u0026#39;, help=\u0026#39;PostgreSQL user.\u0026#39;) parser.add_argument(\u0026#39;-P\u0026#39;, \u0026#39;--password\u0026#39;, type=str, default=None, help=\u0026#39;PostgreSQL user password.\u0026#39;) parser.add_argument(\u0026#39;--port\u0026#39;, type=int, default=None, help=\u0026#39;PostgreSQL port.\u0026#39;) high level flow\r#\rtrain.py .main\n기초 Setup = train_set, test_set ready, database connect, log, cache 등을 설정, 자체 클래스 DeepQNet 모델 initiailize DeepQNet 에는 Step1, Step2, PredictTail 총 세개의 Nueral Network를 가짐 Step1 : table level feature encoding Step2 : 두 테이블간 embedding을 LSTM 기반으로 join representation을 생성 PredictTail : 생성된 쿼리가 좋은지 판단하는 구조. 요약 Step1: 테이블 임베딩 생성 (GNN) Step2: pairwise join composition (LSTM) PredictTail: partial plan value 예측 (value head) UseGeneratedPredict: 생성된 plan 검증 (classifier head) \u0026lt;\u0026ndash; 안씀\n테이블 임베딩을 만들었으니, 새로운 워크로드 에 대해서는 테이블 임베딩이 많이 틀릴테니, 아예 예측 자체를 못하나?\rtrain.py .train()\nStep1, Step2, PredictTail 개 를 training train_mode, test_mode of model\r#\rDeepQNet은 모드별로 동작을 수행하도록 작성됨\n# LOGER/model/dqn.py class DeepQNet: ... def train_mode(self): self.model_step1.train() self.model_step2.train() self.model_tail.train() def eval_mode(self): self.model_step1.eval() self.model_step2.eval() self.model_tail.eval() workload sql 파싱\r#\rLOGER는 sql을 파싱하는 로직을 별도의 .so 파일로 만들어두고 이를 import해서 사용함 # LOGER/core/sql.py from psqlparse import parse_dict ... class Sql: ... parse_result_all = parse_dict(self.sql) ... 여기에서 parse_dict는 아래와 같이 되어있으며, parser.cpython-38-x86_64-linux-gnu.so 는 프로젝트에 같이 탑재되어 있다.\n# LOGER/psqlparse/parser.py def __bootstrap__(): global __bootstrap__, __loader__, __file__ import sys, pkg_resources, importlib.util __file__ = pkg_resources.resource_filename(__name__, \u0026#39;parser.cpython-38-x86_64-linux-gnu.so\u0026#39;) __loader__ = None; del __bootstrap__, __loader__ spec = importlib.util.spec_from_file_location(__name__,__file__) mod = importlib.util.module_from_spec(spec) spec.loader.exec_module(mod) __bootstrap__() 상용 db 연결의 흔적\r#\roracle database를 사용한것으로 추정. 어느정도 구현이 되어있는지는 미확인\n# LOGER/train.py ... if args.oracle is not None: USE_ORACLE = True oracle_database.setup(args.oracle, dbname=args.database, cache=False) try: database.setup(dbname=args.database, cache=False) ... "},{"id":5,"href":"/reinforcement-learning/rl-essential/","title":"1. Reinforcement Learning Essential","section":"Reinforcement Learning / 강화학습","content":"\r1. 강화학습에 대한 기초 내용\r#\r들어가기 앞서\r#\r강화학습의 기초적인 내용을 학습한 뒤 정리한 것으로, 남들에게 보여주기 보다는 본인의 이해와 기억을 위해서 기술한 것입니다.\n나만의 방식으로 이해한 것이기 때문에, 주요하다고 생각하는 부분이 다를수 있으며 생략되거나 놓친 부분이 많이 있습니다.\n강화학습(RL: Reinforcement Learning) 이란?\r#\rDefinition\r#\r“Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal.”\n— Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction (2nd ed), p.1\nsituations을 state로 표기하여\n통상적으로 action, state와 reward 가 RL의 핵심 요소이다.\n요약하면, 주어진 상태(State)에서 보상(Reward)을 최대화 할 수 있는 행동(Action)을 학습하는 것\n이는 Reward Hypothesis를 기반으로한다.\nReward Hypothesis\nAll goals can be described by the maximisation of expected cumulative reward\nRewards는 Scalar feedback signal이다 Agent는 미래에 기대되는 cumulative reward가 최대화 되는 방향으로 학습 한다\nState and MDP\r#\rAgent는 학습하는 주체 (뇌, 로봇) Environment는 환경 (지구, 게임) //우리는 환경이 어떻게 동작하는지 보통 모름 State 는 다음 action을 결정하기 위한 정보이다 \\[S_t = f(H_t) \\]\rAgent가 주어진 상태(State)에서 보상(Reward)을 최대화 할 수 있는 행동(Action)을 학습하는 것\nState 에는 Environment State, Agent State 가 있고 각각 Environment, Agent관점에서의 수식적/내부적 표현이다.\nEnvironment State는 Environment 관점에서 다음 step에서 Environment가 어떻게 변화할지를(어떤 State로 변화할지) 나타낸다. Environment에서는 microsecond 에서도 수많은 정보가 오기 때문에 불필요한 정보들도 많다. 보통의 RL 문제에서 agent는 Environment state를 전부 관측할 수 없다.\n예) 로봇의 움직임을 학습하도록 설계했는데. 이 로봇은 지구의 중력이나, 마찰력 으로 물건이 어디로 움직이는지 정확하게 모른다. Agent State는 다음 step 에서 Agent가 어떤 행동을 선택할지를 나타낸 수식/표현이다.\nInformation State는 과거 history부터 모든 유용한 정보를 포함한 수학적 정의를 가진 State이다. 주로 Markov State라 부른다. (Markov 속성을 만족한다)\nMarkov Properties : 이전의 모든 State 정보를 이용해서 다음 State를 선택하는것이, 현재 State만 보고 하는것과 같다 \\[\r\\mathbb{P}[S_{t+1} \\mid S_t] = \\mathbb{P}[S_{t+1} \\mid S_1, \\dots, S_t]\r\\]\r이를 이용하면, 미래(future) 는 과거에 무엇이 주어졌든지 독립적이다. (=The future is independent of the past given the present) 달리말하면, 현재 \\(S_t\\)\r만 저장해도 된다 \\[\rH_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\r\\]\r현재의 State가 충분한 정보를 이미 담고 있다다.\nAppendix\rhistory 는 Observation과 actions, rewards의 연속이다 \\[\r\\quad \\quad H_t = O_1, R_1, A_1, ..., A_{t−1}, O_t, R_t\r\\]\rState 는 다음 action을 결정하기 위한 정보이다 \\[\r\\quad \\quad S_t = f(H_t)\r\\\\\r\\]\r이전 state는 \\[\r\\mathbb{P}[S_{t+1} \\mid S_t] = \\mathbb{P}[S_{t+1} \\mid S_1, \\dots, S_t]\r\\]\rThe future is independent of the past given the present 모든 이전 State를 알지 않아도 직전 State만 보고 결정 할 수 있다 \\[\rH_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\r\\]\rFully Observable Environments 는 Agent가 environment 어떻게 동작하는지 바로 관측이 가능함을 나타내고, 결과적으로 Environment State = Information State = Agent State 상태이다. 이를 Markov Decision Process(MDP) 라고 한다\nPartial Observable Environments 는 좀더 현실적인 환경. 로봇이 카메라를 통해서 화면을 보지만 현재 자기의 위치를 모르는 것처럼. 즉, Agent State \\( \\ne \\)\rEnvironment State 이다. Partially Observable Markov Decision Process(POMDP) 로 수식이 표현된다.\nAgent는 자기의 State에 대한 representation을 가져야만하고, 이는\n이전 History를 이용해서 사용하는 방법이 있고, \\[S_t^a = H_t\\]\rProbability 로 나타내는 방법이 있고, \\[S_t^a = \\left( \\mathbb{P}[S_t^e = s_1], \\dots, \\mathbb{P}[S_t^e = s_n] \\right)\\]\r순환신경망(Recurrent neural network) 으로 나타내는 방법도 있다. \\[S_t^a = \\sigma(S_{t-1}^a W_s + O_t W_o)\\]\rPolicy, Value Function, Model\r#\rRL은 agent는 아래 components를 한개 이상 포함한다.\nPolicy는 Agent가 어떻게 Action을 선택하는지(=behaviour function) 이다.\nState로부터 function \\(\\pi\\)\r(policy)를 이용해서를 통해 action을 결정한다.\nDeterministic policy: \\( a = \\pi(s)\\)\rstate를 넣으면 다음 취할 액션이 튀어나온다 probability로 policy를 표현하고자 한다면 다음과 같다.\nStochastic policy: \\( \\pi(a \\mid s) = \\mathbb{P}[A_t = a \\mid S_t = s]\\)\r현재 상태 s 에 있을 때, 액션 a 를 선택할 확률 Value Function은 State나 Action이 얼마나 좋은지(기대되는 미래의 reward가 얼마일지 예측) 을 나타낸다.\nState one과 State two, action one과 action two를 선택할때 최종 reward가 더 좋은쪽으로 선택한다.\n아래와 같이 표현되는데 \\( R_{t+1}, R_{t+2}, R_{t+3} \\)\r를 더하는 것과 같이 다음(미래)의 reward의 합의 기대값(어떤 폴리시 \\(\\pi\\)\r를 따르는 가정하에) \\[\rv_{\\pi}(s) = \\mathbb{E}_{\\pi} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s \\right]\r\\]\rgamma(\r\\( \\gamma\\)\r) 는 다음 스탭에 대한 discounting factor (지금은 미래의 값의 영향도 정도로 이해하고 넘어가자)\nModel은 Agent관점에서 Environment가 어떻게 동작할지 생각하는 것 을 나타낸다.\ntransitions model, rewards model 전통적으로 두가지로 나뉜다\ntransitions 모델은 directly 다음 state를 예측한다. \\[\\mathcal{P}_{ss'}^a = \\mathbb{P}[S_{t+1} = s' \\mid S_t = s, A_t = a]\\]\rRewards 모델은 reward를 예측한다. \\[\\mathcal{R}_s^a = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]\\]\rRL agent의 분류\r#\r어떤 key component를 가지고 학습하고 있는지에 따라서 RL을 분류한다\nvalue-based RL은 value function을 optimal이 되도록 한다. (묵시적으로 policy 를 가지고 있다.) policy-based RL은 policy를 업데이트한다. actor-critic은 value function과 policy를 가지고 있다. Model base로 구분하는 방법이 있다.\nmodel free는 model이 없지만(=환경에 대한 representation이 없지만) value function and/or policy로 구성된 RL model based는 model이 존재하고, value function and/or policy 로 구성된 RL Sequential decision making의 두가지 방식\r#\rReinforcement Learning은 환경(Environment)을 모르고 상호작용하면서 reward가 최대가 되도록 학습 하는 것 Planning은 환경을 알고(우리가 환경에 해당하는 rule/model을 주고) agent가 계산하는 것 Exploration and Exploitation\r#\rExploration 와 Exploitation 는 trade-off\nExploration 은 환경에 대한 정보를 더 찾는것 Exploitation 은 알고 있는 정보를 활용해서 reward를 최대화 하는것 예) 새로운 레스토랑 찾기 vs 가장 좋아하는 레스토랑 재방문\nPrediction and Control\r#\rRL에서는 prediction problem과 control problem 이 있다.\nprediction 은 현재 policy 를 따르면 앞으로 얼마나 미래에 좋을지 평가하는것 contorl 은 bset policy(=optimal policy)를 찾는것 머신러닝(ML)과 딥러닝(DL)과의 관계\r#\r머신러닝(Machine Learning)은 인공지능(AI)의 개념으로써, 학습을 통해 예측(또는 분류)를 하는 것\n딥러닝은 머신러닝의 하위 개념으로써, 인공신경망(Neural Network)를 이용해서 학습하는 것\n강화 학습은 머신러닝의 한갈래로써, 보상을 기반으로 스스로 행동 학습하는 것\nAI\r├── Machine Learning\r│ ├── Supervised / Unsupervised\r│ ├── Reinforcement Learning\r│ └── Deep Learning\r│ └── Deep Reinforcement Learning (e.g., DQN, PPO) 참고\r#\rDavid Silver 의 RL 강좌 https://davidstarsilver.wordpress.com/teaching\n한글 유튜브 + 블로그 https://smj990203.tistory.com/2\n"},{"id":6,"href":"/reinforcement-learning/rl-mdp/","title":"2. Markov Decision Process","section":"Reinforcement Learning / 강화학습","content":"\r2. Markov Decision Process\r#\rMarkov Process(MP) 란?\r#\rMP 속성\r#\rMDP는 환경에 대해서 Reinforcement Learning이 이해가능하도록 수식화한다\n거의 모든 RL 관련 문제들은 MDP로 수식화 할 수 있다(Fully observable이나 Partially observation이나) Markov Property를 이용하는데, \u0026ndash;이전 강의참조\u0026ndash;\n요약하면, 현재 state만으로 미래를 예측해도 된다는 속성이다. (다르게 말하면, 현재 state가 이미 유용한 정보를 포함하고 있다. = memoryless)\nMarkov state \\(s\\)\r로부터 \\(s'\\)\r으로 변경하는 transition probability 를 하는 수식은 다음과 같다. \\[\r\\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s'\\mid S_t = s]\r\\]\r매트릭스로 표현하면 다음과 같다. (합은 1) \\[\r\\mathcal{P} = \\left[\r\\begin{array}{ccc}\r\\mathcal{P}_{11} \u0026 \\cdots \u0026 \\mathcal{P}_{1n} \\\\\r\\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r\\mathcal{P}_{n1} \u0026 \\cdots \u0026 \\mathcal{P}_{nn}\r\\end{array}\r\\right]\r\\]\rMP는 tuple로 이루어져 있다.\nA Markov Process (or Markov Chain) is a tuple \\(\\langle \\mathcal{S}, \\mathcal{P} \\rangle \\)\r\\(\\mathcal{S}\\)\ris a (finite) set of states \\(\\mathcal{P}\\)\ris a state transition probability matrix,\n\\(\\mathcal{P}_{ss'} = \\mathbb{P} \\left[ S_{t+1} = s' \\mid S_t = s \\right]\\)\r이 예제는 단순화한 state 변화를 나타내는 것이고, MDP를 이용한 실제 사용은 훨신더 많은 state와 probability를 포함한다.\nMarkov Reward Process(MRP) 란?\r#\rMRP 속성\r#\rreward가 추가가 된 것. MP 에 value judgment가 포함된 것 - 여기서 judgment 는 누적 reward가 얼마나 좋아질지\nA Markov Rewards Process (or Markov Chain) is a tuple \\(\\langle \\mathcal{S}, \\mathcal{P}, \\textcolor{red}{\\mathcal{R}, \\gamma} \\rangle \\)\r\\(\\mathcal{S}\\)\ris a (finite) set of states \\(\\mathcal{P}\\)\ris a state transition probability matrix,\n\\(\\mathcal{P}_{ss'} = \\mathbb{P} \\left[ S_{t+1} = s' \\mid S_t = s \\right]\\)\r\\(\\mathcal{R}\\)\ris a reward function, \\(\\mathcal{R}_s = \\mathbb{E} \\left[ R_{t+1} \\mid S_t = s \\right]\\)\r\\(\\gamma\\)\ris a discount factor, \\({\\gamma \\in [0, 1]}\\)\rtimestep t에 대한 goal 은 다음과 같이 표현된다. 감마는 미래에 대한 discount factor이다. 이것이 필요한 이유는\n우리는 퍼팩한 모델이 없기 때문에 수학적 max 바운드(수학적 편의성을 위해) MP의 무한 루프를 하지 피하기 위해 근접 미래의 가치가 비근접(먼미래) 미래의 가치보다 크기 때문 시퀀스의 끝이 보장된다면 discount factor를 안쓸수도 있다 \\[\rG_t = R_{t+1} + \\gamma R_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\r\\]\rMP는 시간이 지남에 따라 확률에 의해서 state가 변경되는 것이고.\nMRP 에서는 그 변화된 시간에서 state에 도달할때마다 reward가 획득된다고 이해했다.\n이로써 현재 state 에서 바라본다면 앞으로 나의 미래 total reward를 계산할 수 있다.(discount factor 0~1) Value Function\r#\r현재상태(s)에서의 terminated 상태에서의 Expected return 이것은 Expectation 이다. 왜냐하면 environment는 stochastic 이니까 \\[\rv(s) = \\mathbb{E} [ G_t \\mid S_t = s ]\r\\]\r이는 밸망방정식으로 표현될 수 있다.\nBellman Equation for MRP\r#\rValue Function은 크게 두가지 컴포넌트로 나눌수 있다.\n현재의 리워드 \\( R_{t+1}\\)\r다음계승 state의 discounted 상태 \\(\\gamma v(S_{t+1})\\)\r\\[\r\\begin{aligned}\rv(s) \u0026= \\mathbb{E} \\left[ G_t \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma \\left( R_{t+2} + \\gamma R_{t+3} + \\cdots \\right) \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma G_{t+1} \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ \\textcolor{red}{R_{t+1} + \\gamma v(S_{t+1})} \\mid S_t = s \\right]\r\\end{aligned}\r\\]\r\\[\rv(s) = \\mathbb{E} \\left[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s \\right]\r\\]\r\\[\rv(s) = \\mathcal{R}_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'} v(s')\r\\]\r이를 벡터 매트릭스로 표현하면 아래와 같다.\n\\[\r\\begin{bmatrix}\rv(1) \\\\\r\\vdots \\\\\rv(n)\r\\end{bmatrix}\r=\r\\begin{bmatrix}\r\\mathcal{R}_1 \\\\\r\\vdots \\\\\r\\mathcal{R}_n\r\\end{bmatrix}\r+\r\\gamma\r\\begin{bmatrix}\r\\mathcal{P}_{11} \u0026 \\cdots \u0026 \\mathcal{P}_{1n} \\\\\r\\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r\\mathcal{P}_{n1} \u0026 \\cdots \u0026 \\mathcal{P}_{nn}\r\\end{bmatrix}\r\\begin{bmatrix}\rv(1) \\\\\r\\vdots \\\\\rv(n)\r\\end{bmatrix}\r\\]\rSolving the Bellman Equation\r#\r벨만 방정식은 linear equation 이지만 O(n^3) 복잡도를 가지고 있기 때문에 작은것만 풀수 있다.\nLarge MRP를 풀기위해서\nDynamic Programing 이나 Monte-Carlo evaluation 이나 Temporal-Difference Learning이 있다. Markov Decision Process(MDP) 란?\r#\rA Markov Decision Process (or Markov Chain) is a tuple \\(\\langle \\mathcal{S}, \\textcolor{red}{\\mathcal{A}}, \\mathcal{P}, \\mathcal{R}, \\gamma \\rangle \\)\r\\(\\mathcal{S}\\)\ris a (finite) set of states \\(\\mathcal{A}\\)\ris a (finite) set of actions \\(\\mathcal{P}\\)\ris a state transition probability matrix,\n\\(\\mathcal{P}^a_{ss'} = \\mathbb{P} \\left[ S_{t+1} = s' \\mid S_t = s, A_t = \\textcolor{red}a \\right]\\)\r\\(\\mathcal{R}\\)\ris a reward function, \\(\\mathcal{R}^a_s = \\mathbb{E} \\left[ R_{t+1} \\mid S_t = s , A_t = \\textcolor{red}a \\right]\\)\r\\(\\gamma\\)\ris a discount factor, \\({\\gamma \\in [0, 1]}\\)\r위 MP MRP 예제와 다르게 action이 추가됨. 그림에서는 잘 안표현되어있지만, 액션을 하면. 그 액션으로인해 특정 state로 전이되는 것은 확률이다.\n밑에 pub에 가는것 액션을 수행하면 확률적으로 class1, class2, class3에 도달한다. Policy\r#\r정책(Policy \\(\\pi\\)\r) 는 주어진 state에 대한 action의 분포 \\[\r\\pi(a \\mid s) = \\mathbb{P}[A_t = a \\mid S_t = s]\r\\]\r마크로프 속성에 의해서 현재 state는 reward를 fully characterize 한것이기 때문에 위 수식에 reward가 없다\nstate 시퀀스(상태전의)에 대해서 policy를 넣으면 Markov Process이고, state 시퀀에 Reward를 넣으면 Markov Reward process 이다.\nMarkov Reward Process 에 대해서 MDP 수식으로 (Action으로부터) 다음과 같이 수식으로 표현이 가능하다. (MDP 수식으로(policy-action이 포함된 버전으로) MP와 MRP를 표현이 가능하다)\n\\[\r\\mathcal{P}^\\pi_{s, s'} = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathcal{P}^{a}_{s s'} \\\\\r\\mathcal{R}^\\pi_s = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathcal{R}^a_s\r\\]\r이는 모든 action에 갈수 있는 prob를 average로(\r\\(\\pi\\)\r는 0~1의 값이므로) 이해를 쉽게하기 P와 R을 표현한 것.\nMDP에 대한 value function은 state-value 펑션과, action-value function 두가지 방식이 있다.\nstate-value function\r#\r다음과 같고 이는 현재 state일때 \\(\\pi\\)\rpolicy를 따를때 얼마나 좋은지를 나타낸다. (얼마나 Reward를 얻을지) \\[\rv_\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s \\right]\r\\]\r여기서 \\(\\mathbb{E}_\\pi\\)\r는 모든 샘플액션에 대한 expectation\naction-value (q) function\r#\r이를 action-value function \\(q_\\pi\\)\r로 나타낼 수 있다. 이는 현재 state에서 어떤action을 선택했을때 얼마나 좋은지를 나타낸다. (얼마나 Reward를 얻을지) \\[\rq_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s, A_t = a \\right]\r\\]\rstate-value function과 action-value function의 Bellman Expectation 방정식을 다음과 같이 나타낼수 있다. \\[\rv_\\pi(s) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s \\right]\r\\]\r\\[\rq_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a \\right]\r\\]\rstate-value-function 와 action-value function 중 어떤 것을 중점적으로 학습하는지에 따른 학습방법이 달라지는 것 같다.\n왼쪽은 state-value function관점에서의 그림과 수식표현이고, 오른쪽은 action-value 펑션관점에서 수식과 표현이다. action-value function에 대해서는, action을 선택함으로써 reward를 통해서 state-value function으로 다시 넘어가는 것을 볼수 있다.\n이 두개의 그래프를 합치면 다음과 같다. 왼쪽은 state-value function 관점에서의 수식이고, 오른쪽은 action-value function 관점에서의 수식이다.\n이것들은 앞에서 언급했던것처럼 두 가지 Junk로 나눌수 있고, (이번 Step에서의 Reward와 미래의 value function 리턴값) 다음 수식으로 나타내진다 (matric-form). 추가로 모든 MDP는 MRP로 표현이 가능하다. \\[\rv_\\pi = \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_\\pi\r\\]\r\\[\rv_\\pi = \\left( I - \\gamma \\mathcal{P}^\\pi \\right)^{-1} \\mathcal{R}^\\pi\r\\]\r우리는 이로부터(state-value, action-value function으로부터) Optimal Value Function 을 찾는다\nOptimal Value Function\r#\rMDP에서의 최적 행동을 찾는 방법은 optimal state-value function \\(v_*(s)\\)\r를 구하는 것이다. 이것은 모든 policy 에 대해서 value function을 최대화 하는것이다. (장기적으로 최대 보상을 얻기 위해서) optimal action-value function \\(q_*(s,a) \\)\r의 경우 아래와 같이 구할 수 있다.\n\\[\rv_*(s) = \\max_\\pi v_\\pi(s)\r\\]\r\\[\rq_*(s, a) = \\max_\\pi q_\\pi(s, a)\r\\]\roptimal policy (\r\\(\\pi_*\\)\r) 는 \\(q_*\\)\r를 최대화 함으로써 얻을 수 있다. \\[\r\\pi_*(a \\mid s) = \\begin{cases}\r1 \u0026 \\text{if } a = \\arg\\max\\limits_{a \\in \\mathcal{A}} q_*(s, a) \\\\\r0 \u0026 \\text{otherwise}\r\\end{cases}\r\\]\r옵티멀한 해는 위에 구한 도식과 같은 형식으로 아래 모형과 수식으로 나타낼 수 있다.\nSolving Bellman Optimality Equation\r#\rBellman Optimality Equation은 non-linear 하고 보통 No closed form 으로 제공됨. 아래와 같은 solving method 들이 있음\nvalue iteration : Iteratively updates value estimates using the Bellman optimality equation until convergence. policy iteration : Alternates between policy evaluation and policy improvement until the policy becomes stable. Q-learning : Off-policy method that directly learns the optimal action-value function from experience. Sarsa(State-Action-Reward-State-Action) : On-policy method that updates action-values based on the action actually taken by the current policy. Extensions of MDPs\r#\rinfinite and continous MDPs 무한한 기존 방법을 그대로 적용이 가능하다(Straightforward). 연속적인 숫자라면 편미분 해야한다. Partially observable MDPs (finite sets of Observation:O 와 Observation function:Z) 추가요소가 있으며. 상태를 직접적으로 알 수 없으니 관측값으로부터 추정하는 hidden stage가 있는 MDP로 해결 Undiscounted, average reward MDPs ergodic markvo process 로 처리할수 있다. ergodic은 Recuurent: 각 state는 무한 시간 동안에 방문, Aperiodic : 어떤 주기성 없이 방문 하는 속성을 가지고 있다.이것은 average reward MDP이다 (discount 되지 않으니, 큰수의 법칙에 의해서). 따러서 average bellman equation 을 풀면 된다. 참조\r#\rhttps://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-2-mdp.pdf\nhttps://www.youtube.com/watch?v=lfHX2hHRMVQ\nhttps://trivia-starage.tistory.com/280\n"},{"id":7,"href":"/reinforcement-learning/rl-dp/","title":"3. Planning by Dynamic Programming","section":"Reinforcement Learning / 강화학습","content":"\r3. Planning by Dynamic Programming\r#\rMDP를 푸는 방식들이 여러 방법이 있다.\nPolicy evaluation, Policy iteration, Value iteration 등이 있고, 이것들은 환경을 정확하게 안다면(=모델을 안다면) DP가 적용이 가능하다\n먼저 간략하게 언급하자면\npolicy iteration : policy를 평가하고 iteration 하면서 발전해나가는 방식과 (policy evaluation + policy improvement) value iteration : value function을 iteration하면서 옵티멀을 찾아가는 방법 이 있다.\n이번 섹션은 DP로 known MDP를 푸는 방법에 대한것이고, 이것은 강화학습의 flow 와 수식 간의 이해를 위한 섹션이다. 4장부터 unknown MDP를 푸는 방법이 기술되어 있다.\nDynamic Programming(DP) 이란?\r#\r정의 : // 이부분은 생략\nMDP는 DP로 문제를 풀기에 필요한 조건들을 만족한다.\n벨만 방정식(Bellman equation) 은 재귀적 decomposition value function 은 값을 저장하고, 재사용한다. full environment 정보가 주어지면 이것은 강화학습의 문제가 아니라 planning problem(mdp를)으로써 DP로 풀수 있다. MDP planning의 두가지 문제가 있다. (For prediction, For control)\nprediction problem은 input MDP(or MRP)와 policy가 주어졌을때, 이것의 output인 value function \\(v_\\pi\\)\r을 구하는것\ncontrol problem 은 옵티마이징 하는것(best policy와 그에따른 best value function을 구하는것). input으로 MDP가 주어지고 output으로 \\(v_*\\)\r(optimal value function) 또는 \\(\\pi_*\\)\r(optimal policy)\npolicy evaluation\r#\rpolicy시가 얼마나 좋은지 평가(MDP로 얼마나 얼마나 많은 reward를 얻을수 있는지?). policy를 업데이트하진 않는다\nbellman expectation equation을 사용\nbellman expectation equation을 풀기 위해서,\n이터레이션마다 policy하의 value function을 평가해서, value function을 업데이트 한다.\n\\(v1 \\rightarrow v2 \\rightarrow ... \\rightarrow v_\\pi\\)\r가 되어 true value function 을 얻을 수 있다.\n이것은 우리가 policy 에 따른(고정된 policy) 정확한 value function을 모르니 (optimal value function을 말하는것이 아님), 그것을 계산하기 위해서 iterative하게 계산한다는 의미. 여기에서 iterative는 강화학습에서 action하고 reward 받는 iterative(timestep) 과는 다른 의미\r이때 두가지 방식으로 backup이 가능하다. synchronous backup/asyncronous backup synchronous backup = 한 iteration에서 전체 상태들의 값이 한꺼번에 업데이트됨 트리에서 현재root 노드에 있다고 가정하면, 취할 수 있는 모든 action을 고려하고 갈수 있는 모든 계승 state도 고려해서 backup(되돌아가서) 현재 노드에 probability 에 따른 weight를 더한다. 이것이 결국 현재 노드의 이번 이터레이션의 value function.\n이것은 true value function으로 수렴하는것을 보장한다 (why?= discount factor 가 0~1 이므로 수축 매핑 성질을 가진다 (contraction mapping) by GPT)\nsynchronous backup policy evaluation example\r#\r아래그림은 이동을 uniform random하기 pick된다는 policy에 대한 그림이다 (왼쪽에 적힌 숫자값들 : 1/4씩 가능성이 있는경우) k=1 일때의, 주변 으로 갔을때 전부 -1 이니 -1*4/4 로 1회 업데이트 k=2 일때의 1.7 은 1.75가 짤린것. [0,1] 을보면 네곳으로 이동할수 있고 북으로가면 자기자신으로 돌아와서 이동-1, 이전step(k=1에서의) 자가자신의 값 -1 이므로 -2, 동으로가면 이동-1 과 이전step의 [0,2]의 값 -1 이 합쳐져서 -2, 마찬가지로 남으로가면 -2 서로 가면 이동 -1 과 이전step의 [0,0]의 값 0이 합쳐져서 0 따라서 (0 -2 -2 -2) / 4 하면 [0,1] 은 -1.75\n이것을 계속하면 값이 계속 업데이트가 되는데 k=3일때 벌써 수렴한것을 볼수 있다.\n이 value function을 random policy가 아니라 그리디하게 선택하게 하는 policy 하면 (값이 큰것을 선택하도록하면) 오른쪽 화살표 그림과 같이 나타난다.\nvalue function을 better 폴리시를 찾아내는데 도음을 준다. 현재 policy를 평가하는것만으로도 우리는 더좋은 새로운 폴리시를 만들수 있다.\nasyncronous backup = 전체 상태를 한 번에 갱신하는 것이 아니라, 선택된 특정 상태에 대해서만 value function을 갱신하는 방식이다. 이는 계산 효율을 높이고, 빠른 수렴을 가능하게 한다. by GPT\npolicy iteration\r#\rpolicy를 inner loop에서 iteration마다 평가하면서 policy가 더 나아지도록 적용해나가는 방식; 결국 optimal policy를 찾게 된다는 설명.\n첫번째 스텝으로 policy evaluation : policy \\(\\pi\\)\r를 평가(evaluation) 하면 value function이 나오고, (현재 policy로 각상태의 value 를 구하는것) \\[\rv_\\pi(s) = \\mathbb{E} \\left[ R_{t+1} + \\gamma R_{t+2} + \\dots \\mid S_t = s \\right]\r\\]\r두번째 스텝으로 policy improvement: \\(\rv_\\pi(s)\\)\r기반으로 policy 를 greedily 행동하게 improvement 하면 이것이 업데이트된 policy다. \\[\r\\pi' = \\text{greedy}(v_\\pi)\r\\]\r이것이 결국 optimal policy다. (value function도 결국 optimal한 것으로 수렴한다.)\n추가질문:언제 수렴했는지는 어떻게 파악할 수 있는지?\n정책이 더이상 바뀌지 않거나, 정책간의 차이가 아주 작을때 (10^-4) 수렴했다고 파악 by GTP DQN (2015)\tValidation 환경에서의 평균 reward가 더 이상 증가하지 않을때 PPO (2017)\t평균 reward의 moving average가 안정될때 (변화량 \u0026lt; threshold) 이것들은 결국 Modified Policy Iteration. 명시적으로 stop할 iteration 숫자(k)를 세팅하거나, 입실론-convergence of function 으로 stopping condition을 만들어야함.\n만약 policy가 deterministic 한 policy 이라면, \\( a = \\pi(s)\\)\r. improvement가 멈춘다면 수식적으로 수렴한다는것을(Bellman Optimal Equation을 만족함을) 증명할 수 있지만, 생략함. lecture-3 의 17page\nvalue interation\r#\r이것은 MDP를 푸는 또 다른 방식이다.\nbellman equation을 통해서 value function이 better 하도록 하는 방식 정책 반복보다 계산량이 적고, 수렴 속도가 빠를 수 있습니다.\n어떠한 optimal policy도 두개의 컴포넌트로 나뉠수 있다.\noptimal first action \\(A_*\\)\r계승되는 state S\u0026rsquo; 의 optimal policy 이를 다시말하면, 현재상태에서 다음행동 (첫번째 action)이 optimal한 것을 선택하면, 그다음은 계승 state S\u0026rsquo; 에서 optimal policy따르는것 이는 Principle of Optimality 를 나타낸다.\n한 policy이 어느 한 상태에서 최적이라면(Optimal이라면), 그 policy이 앞으로 갈 모든 경로에서도 계속 최적이어야 한다.\n이는 아래 수식을 (value function을) 최대화 하는것으로 나타낼수 있다. \\[\rv_*(s) \\leftarrow \\max_{a \\in \\mathcal{A}} \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\, v_*(s')\r\\]\r아래그림은, 어떻게 flow가 진행되는지 직관적으로 이해하기 위한 예제이다\nsyncronous 하게 업데이트가 되니 숫자가 채워져 있다. 이동시 reward는 -1 terminate state 0인상태에서 출발. 인접 슬롯은 첫번째 iter에 -1 된다. 자세한 flow는 다음과 같다. V_1에서 V_2로 간다면 인접 행렬[0,1] 은 주변 자기로부터 한칸 이동한경우가 -1 = 0(왼칸 값) + -1(이동) 이 최대값이므로 취함. (네방향 모두)\n[0,2] 의 경우도 동일: 주변 자기로부터 한칸 이동한경우가 -1 = 0(왼칸 값) + -1(이동) 이 최대값이므로 취함. (네방향 모두)\nV_2에서 V_3으로 간다면 [0,1] 의 경우 왼칸은 -1 = 0(왼칸 값) + -1(이동) 나머지 방향은 -2 = + -1(동남북 값) + -1(이동) 이므로 최대값 -1을 취함 [0,2] 의 경우 : 주변 자기로부터 한칸 이동한경우가 -2 = 1(왼칸 값) + -1(이동) 이 최대값이므로 취함. (네방향 모두) 사실 우리가 모든 environment가 어떻게 동작하는지 명확하게 안다면 위에 예제의 경우, goal로 부터 인접 값들을 채워가면서 계산하면 풀리는 문제이다.\n만약 우리가 syncronous dynamic programming 으로 푼다면 이게 언제 풀리는지 모른다. (모든 state 값을 업데이트하기 때문에 v_2에서 [2,2] 의 경우 -1 로 채워져 있는데, 이게 잘채워진건가? 를 판단하지 못함)\n결국 value iteration 은 optimal policy \\(\\pi\\)\r를 찾는것\n계속해서 value function을 업데이트하면서 최적을 찾아가고 있기 때문에 명시적으로 policy를 만들지 않는다.\npolicy evalution은 bellman expectation equation 을 푸는것이고, (v_\\pi 를 찾는것 ) value iteration 은 bellman optimality equation을 푼다. (v_* 를 찾는것)\r수식적으로는 다음과 같이 표현되어 있다.\n\\[\r\\mathbf{v}_{k+1} = \\max_{a \\in \\mathcal{A}} \\left( \\mathcal{R}^a + \\gamma \\mathcal{P}^a \\mathbf{v}_k \\right)\r\\]\rSynchronous Dynamic Programing\r#\r아래 도표와 같이 처리하면 된다 state-value function을 base로 \\( v_\\pi(s) \\)\r나 \\( v_*(s) \\)\r를 찾는다면 iteration당 \\( \\mathcal{O}(mn^2) \\)\r시간복잡도고 m = actions, n = states action-value function 을 베이로하면 \\( q_\\pi(s,a) \\)\r나 \\( q_*(s,a) \\)\r를 찾는다면 iteration당 \\( \\mathcal{O}(m^2n^2)\\)\r시간복잡도 Asynchronous Dynamic Programing\r#\r위의 예제는 모든 state를 모두 업데이트 하는데 낭비가 심함.\n정의: 모든 상태를 동시에 업데이트하지 않고, 일부 상태만 선택적으로 업데이트합니다. 장점: 계산 효율성이 높아지고, 특정 상태에 집중할 수 있습니다.\nasynchronous dynamic programming을 하기위한 3가지 idea들은 다음과 같다\nIn-place dynamic programming Prioritised sweeping Real-time dynamic programming 자세한 내용은 생략\nFull-Width Backup 과 Sample Backup\r#\rFull-width backup은 너무 비싸서 sample 기반 backup을 한다.\nsample : 에이전트가 환경과 상호작용해서 얻은 한 번의 경험 데이터\n이로인한 장점은 다음과 같다.\n이로인해서 궁극적으로 Model-free하게 된다.( environment 모델을 알지 않아도 되게 된다) 차원의 저주 해소 backup cost가 줄어듬 샘플을 통해서 Dynamic programing에서 model-free reinforcement learning 문제로 변환하게 된다.\nModel-based :\t환경의 동작 방식을 알고 있음 (혹은 학습함). 이를 사용해 planning을 함.\nModel-free\t: 환경의 동작 방식 없이, 직접 환경과 상호작용하며 학습함. 오직 경험(transition, reward)에만 의존.\n매번 샘플은 \u0026ldquo;운 좋을 수도 있고 아닐 수도\u0026rdquo; 있지만, 충분히 많이, 반복적으로, 그리고 잘 정해진 규칙으로 업데이트하면 결국 기대값에 수렴 → 최적에 수렴. by GPT 수학적으로 증명이 되었다고도 한다..\r"},{"id":8,"href":"/reinforcement-learning/rl-model-free-prediction/","title":"temp. 4. Model Free Prediction","section":"Reinforcement Learning / 강화학습","content":"\r4. model-free prediction\r#\r// NOTE: 이 페이지는 임시로 작성되었습니다.\r3장 DP에 있는것처럼, Model-free prediction 하고, Model-free control 하는 순서로 진행된다.\nepisode : 에이전트가 시작 상태에서 행동을 시작해서, 어떤 종료 조건(End state)에 도달할 때까지의 전체 과정\nMC와 TD는 major model-free algo.\nMonte Carlo (MC) : 한 에피소드가 끝날 때까지 기다린 후, 그 전체 리턴 값을 이용하여 value function을 업데이트\nMC는 하나의 에피소드 전체(시작 ~ 종료)를 관찰한 뒤, 실제로 받은 reward들을 기반으로 학습합니다. 환경 모델 없이, 경험만으로 value function이나 policy를 추정합니다. Monte-Carlo policy evaluation uses empirical mean return instead of expected return 높은 variance와 zero bias를 가짐 즉, 가장 손쉬운 방법으로 epside를 돌려보고 가능서들의 mean 값으로 처리 방문할때마다 횟수와 토탈 return을 늘리고, 이것의 평균을 통해 value function을 estimate한다. lecture-4, 7 page \\[\rV(S_t) \\leftarrow V(S_t) + \\frac{1}{N(S_t)} \\left(G_t - V(S_t)\\right)\r\\]\rBlackjack 예제처럼, 확률이나, 분포 그런것 전혀 없이 episode 로 부터 value function을 만들어냈다. (~500,000반복하면서)\nTemporal Difference (TD) : 에피소드가 끝나지 않아도, 다음 상태의 현재 추정 값을 사용해 바로 업데이트 incomplete episodes 를 bootstraping을 통해서 업데이트 아직 에피소드가 끝나지 않아서 나머지 예상되는 reward를 포함해서 value function을 업데이트함 따라서 bias가 있음 + 낮은 variance를 가짐\n이것은 Markov property를 활용한다. \\[\rV(S_t) \\leftarrow V(S_t) + \\alpha \\left( R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right)\r\\]\rTD(\r\\(\\lambda\\)\r) : 여러 step + 가중합으로 업데이트 ? MC와 TD의 중간\nMC \u0026lt;-\u0026gt; TD는 전체 에피소드를 보느냐, 일부분만 보느냐의 차이.\nTD의 step을 0~n (n이 되면 MC와 같음) 사이를 \\(\\lambda\\)\r로 가중치를 구해 사용하는것\nMonte-Carlo Reinforcement Learning 은 model-free 이다. 왜냐하면 MDP Transition 에 대한 (reward에 대한) 지식이 없기 때문. Temporal-Difference Learning 은 model-free\n5. model-free control\r#\ron-policy/off-policy intro\r#\rπ = Target Policy µ = Behavior Policy\n이두개가 같으면 on-policy 다르면 off-policy.\nOff-policy learning : “Look over someone’s shoulder” Learn about policy π from experience sampled from µ re-use experience generated from old policy Q-Learning : \\(\\varepsilon\\)\r-greedy 방식으로 탐험하지만 학습에는 반영 안할수 있음(최적의 행동만 업데이트)\nOn-policy learning : “Learn on the job” Learn about policy π from experience sampled from π\nSalsa :on-policy Q-learning. 현재 행동을 그대로 따라가며 학습\non-policy\r#\rMonte-carlo iteration\r#\rMonte-Carlo 방법을 통해서 Policy Evaluation은 가능.(=Monte-Carlo Evaluation)\ngreedy 하게 policy improvement는 action-value 펑션에 대해서만 가능하다. state-value function을 하려면, 모델에 대해 알아야만 가능하다.\n\\[\r\\pi'(s) = \\arg\\max_{a \\in \\mathcal{A}} \\left( \\mathcal{R}_s^a + \\sum_{s'} \\mathcal{P}_{ss'}^a V(s') \\right)\r\\]\r위와 대비되게 action-value function(Q 펑션) 은 model-free해서 알수 있다.\n\\[\r\\pi'(s) = \\arg\\max_{a \\in \\mathcal{A}} Q(s, a)\r\\]\r이렇게 알게된 policy를 아래 \\(\\varepsilon\\)\r-greedy 방식으로 improvement 한다.\nε-greedy\r#\r\\(\\varepsilon\\)\r-greedy Algo\n항상 최고의 행동만 고르면 탐험이 부족하고, 항상 무작위로 고르면 성능이 낮다. → 둘 사이를 적절히 섞자!\n예시 )\nε=0.1 (10%) 90% 확률로 현재 최적의 행동 10% 확률로 랜덤 행동 이것은 수학적으로 policy가 점차 좋아지는것을 나타내고 수렴한다는 계산 증명이 가능하다\nMonte-carlo Control\r#\rMonde-Carlo Policy iteration 은 이전 섹션에서 설명한것\nMonde-Carlo Control 은 하나씩의 episode 가 끝난후에 policy를 업데이트하는것 (episode 단위로 policy improvement)\n이렇게 해도 되는 이유는(수렴하는이유는) Greedy in the Limit with Infinite Exploration 성질을 만족하기 때문이다..\n(policy를 업데이트하기 위한 충분한 정보를 이미 가지고 있다 라고 볼수도 있다.) [화살표 그림 추가예정]\nGLIE 성질에 대한 설명은 추후 업데이트.\nSarsa\r#\rMC Control은 episode가 끝날때마다 정책을 개선하는데 Salsa는 매 step마다 정책을 개선.\noff-policy\r#\rbehaviour policy µ를 통해서 수집하고, target policy π 를 학습하는것 이것은 다른 분포로부터 학습하는 성질을 이용 \\[\r\\mathbb{E}_{X \\sim P}[f(X)]\r= \\sum P(X) f(X)\r= \\sum Q(X) \\frac{P(X)}{Q(X)} f(X)\r= \\mathbb{E}_{X \\sim Q} \\left[ \\frac{P(X)}{Q(X)} f(X) \\right]\r\\]\r이것은 다음과 같이 value function 계산에 주입된다. \\[\rG_t^{\\pi / \\mu} =\r\\frac{\\pi(A_t \\mid S_t)}{\\mu(A_t \\mid S_t)}\r\\frac{\\pi(A_{t+1} \\mid S_{t+1})}{\\mu(A_{t+1} \\mid S_{t+1})}\r\\cdots\r\\frac{\\pi(A_T \\mid S_T)}{\\mu(A_T \\mid S_T)}\rG_t\r\\]\r\\[\rV(S_t) \\leftarrow V(S_t) + \\alpha \\left( G_t^{\\pi / \\mu} - V(S_t) \\right)\r\\]\rQ-learning\r#\raction-value Q(s,a) 의 off-policy learning 다음 action을 선택할때 behaviour policy로부터 고르고, \\(A_{t+1} \\sim \\mu(\\cdot \\mid S_t)\\)\r학습은 target policy 기반으로 진행. \\(A' \\sim \\pi(\\cdot \\mid S_t)\\)\r위와 같이 진행해도, Q-learing은 결국에는 옵티멀한 action-value (q) function에 수렴한다는 성질을 이용한것. 밑에는 improvement 하는 수식. \\[\rQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left( R_{t+1} + \\gamma Q(S_{t+1}, A') - Q(S_t, A_t) \\right)\r\\]\r이렇게 하면 복잡한 weight 계산식 을 하지 않아도 된다.\n[마지막 랩업 표 추가 예정]\n6. value function approximation\r#\rlarge MDP를 풀수 없으니 (too many state and action) value function(state-value/action-value function)을 어떻게 근사하게 구하는가?\nLiner combinations of feature Neural network 여기서부터 Gradient Descent 가 나온다.\n정책 π 고정 Q-function을 gradient descent로 근사 (policy evaluation) 근사된 Q 기반으로 정책 개선 (policy improvement) 다시 반복 (⇒ 점진적으로 최적 정책에 수렴) value function을 근사해서 사용하므로 value-based 라고도 한다.\nAction-value function Approximation\r#\r아래와 같이 action value function은 approximate를 통해서 표현될수 있고. 델타 W를 작게하므로써 근사를 구할 수 있다. \\[\rJ(\\mathbf{w}) = \\mathbb{E}_{\\pi} \\left[ \\left( q_{\\pi}(S, A) - \\hat{q}(S, A, \\mathbf{w}) \\right)^2 \\right]\r\\]\r\\[\r- \\frac{1}{2} \\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\left( q_{\\pi}(S, A) - \\hat{q}(S, A, \\mathbf{w}) \\right) \\nabla_{\\mathbf{w}} \\hat{q}(S, A, \\mathbf{w})\r\\]\r\\[\r\\Delta \\mathbf{w} = \\alpha \\left( q_{\\pi}(S, A) - \\hat{q}(S, A, \\mathbf{w}) \\right) \\nabla_{\\mathbf{w}} \\hat{q}(S, A, \\mathbf{w})\r\\]\rBatch Method\r#\r위의 gradient descent 할때 sampling을 효율적으로 하기 위한 여러가지 방법들\nExperience Replay\r#\r과거의 transition들을 버리지 않고 buffer에 저장해 두었다가, 학습할 때마다 랜덤하게 샘플링해서 사용\nPrioritized Experience Replay (PER)\r#\r모든 transition을 균등하게 샘플하지 않고, TD-error가 큰 transition에 더 높은 확률을 부여하여 학습\n직관: TD-error가 클수록 더 학습이 필요한 \u0026ldquo;중요한\u0026rdquo; 경험\n7. policy gradient Method\r#\r이전섹션에서는 action-value function(Q 펑션)을 근사해서 옵티멀한 폴리시를 찾아갔다면, policy gradient는 Q 없이 policy parameter를 직접 업데이트 한다. policy-based\nSoftmax Policy : 이산행동일때, softmax over logits으로 정책을 정하고, policy에 대한 gradient 값 계산\n이를 통해서 현재 iteration 에 대한 policy 파라미터를 업데이트한다. Gaussian Policy : 연속행동일때, 가우시안 분포를 정책으로 정하고, policy에 대한 gradient 값 계산\n이를 통해서 현재 iteration 에 대한 policy 파라미터를 업데이트한다. policy-based 에서도 replay buffer를 쓸수 있는가?\nPolicy-based에서 Replay Buffer의 어려움 문제: Policy가 계속 변하기 때문에 ​\nPPO, TRPO [\t❌ 또는 제한적 사용 ]\t최근의 데이터만 사용 (very short buffer)\nSAC (Soft Actor-Critic) [ 확률적 policy 사용 ] (policy gradient 기반) 하지만 전체 구조는 off-policy 로써 replay buffer 적극 사용\nby GPT\n추가 research 요망\nActor-Critic\r#\rpolicy-based + value-based\nActor는 행동을 생성하고 Critic은 그 행동의 \u0026ldquo;좋음\u0026quot;을 평가해서 Advantage를 추정 이를 바탕으로 Actor의 policy gradient를 계산 lacture-7 24page\n간단한 actor-critic 구조는 actuion-value를 critic 하는것이다. Critic은 TD(0)을 통해서 Q-Function을 학습하고 Actor는 위 학습된 Q-function을 기반으로 policy gradient를 수행하는것이다.\nProximal Policy Optimization (PPO)\r#\rActor-Critic 구조에서 Clipped Objective를 도입해서, policy가 너무 크게 바뀌지 않도록 제약하는것.\n아래는 PPO surrogate objective 함수(목적함수). 이를 gradient ascent(최대화) 하는 파라미터를 현재 policy에 업데이트하면 policy는 옵티멀을 향해간다. \\[\rL^{\\text{CLIP}}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t,\\; \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]\r\\]\r8. Integration Learning and Planning\r#\rModel-based RL\r#\r[생략]\nSimulation-based Search\r#\r\u0026ldquo;전체 상태 공간을 직접 학습하기는 너무 비싸다. 대신, 유망한 부분만 집중적으로 시뮬레이션하면서 거기서 경험한 정보로 Q값을 점점 더 정확하게 만든다.\u0026rdquo;\n//note: balsa simulation learning 과는 개념이 조금 다름\n[미완]\n9. Exploration and Exploitation\r#\rExploitation : Make the best decision given current information Exploration : Gather more information 너무 탐험만 하면: 성능이 낮은 행동도 계속 시도 → 학습은 느리고, 보상은 낮음 너무 이용만 하면: 더 나은 행동을 아예 시도하지 않음 → **지역 최적해(local optimum)**에 갇힘 → 따라서, 단기 보상 vs 장기 학습 사이의 균형을 잡는 것이 중요합니다. 이 섹션에서는 여러 방법들을 제시하고 있습니다 [미완]\n10. Case Study: RL in Classic Games\r#\r[미완]\n999. Deep RL\r#\rRL과 딥러닝의 결합 딥러닝이 사용하는 위치\nPolicy Network : 현재 상태에서 행동 분포를 출력 Value Network : 상태나 상태-행동의 가치를 출력 Q-network : Q(s, a)를 직접 추정 Model Network : 환경 dynamics (transition, reward)를 예측 (model-based RL에서만 사용) a. Balsa\r#\rBalsa는 쿼리 플랜을 순차적으로 구성하는 문제를 Markov Decision Process (MDP) 으로 보고, 이를 강화학습으로 해결\nState s = 현재까지 만들어진 partial query plan Action a = 다음에 어떤 테이블을 조인할지 결정 Reward r = 쿼리 플랜의 실행 비용 또는 latency Environment = DB 쿼리 시뮬레이터 or Costmodel 추가적으로\nsimulation phase(step) 을 가져서 재앙적 plan을 탐험하지 않게하고, Timeout을 둬서 Safe Execution 시간을 보장했다. (재앙적 plan이 선택되더라도 timeout으로 하한보장) value network를 simple tree convolution networks 로 구성 # 여기에서 모델은 강화학습의 environment의 모델이 아니라, value function을 근사할(계산할) treeconv 모델을 의미함 def MakeModel(p, exp, dataset): dev = GetDevice() num_label_bins = int( dataset.costs.max().item()) + 2 # +1 for 0, +1 for ceil(max cost). query_feat_size = len(exp.query_featurizer(exp.nodes[0])) batch = exp.featurizer(exp.nodes[0]) assert batch.ndim == 1 plan_feat_size = batch.shape[0] if p.tree_conv: labels = num_label_bins if p.cross_entropy else 1 return TreeConvolution(feature_size=query_feat_size, plan_size=plan_feat_size, label_size=labels, version=p.tree_conv_version).to(dev) b. LOGGER\r#\re-beam search 소개 [Exploration and exploitation] loss function reward weighting을 통해서 poor operator에 의한 fluctuation 방지 log transformation 을 통해서 reward의 범위를 압축 (재앙적 plan의 영향도를 감쇄) ROSS Restricted Operator Search Space. (최적을 찾지 않고 최악이 안골라지게 해서 효율적) policy nertwork (GNN + LSTM) c. RELOAD\r#\rBalsa + MAML + PER\nModel-Agnostic Meta-Learning(MAML)\r#\r모든 task에 잘 작동하는 하나의 모델을 학습하는 것\u0026quot;이 아니라, 조금만 fine-tuning 하면 각 task에 잘 작동할 수 있는 초기 모델\u0026quot;을 학습하는 것.\nPER\r#\r위에 언급 [ 생략 ]\n참조\r#\rhttps://davidstarsilver.wordpress.com/teaching/\nhttps://wnthqmffhrm.tistory.com/10\nhttps://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-5-model-free-control-.pdf\n"}]