[{"id":0,"href":"/development/secondpost/","title":"ë‘ë²ˆì§¸ ê¸€ì…ë‹ˆë‹¤. ë¸”ë¡œê·¸ ì‹¤í—˜ìš© í…ŒìŠ¤íŠ¸ ê¸€ì…ë‹ˆë‹¤.","section":"Development / ê°œë°œ","content":"\rtest\r#\rtest2\r#\rRL ê´€ë ¨ í•­ëª©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•œ post ì…ë‹ˆë‹¤.\ní˜ì´ì§€ ì—…ë¡œë“œ í™•ì¸ìš© ì„ì‹œ í¬ìŠ¤íŠ¸\n"},{"id":1,"href":"/aboutme/","title":"About Me","section":"Home","content":"\rProfile\r#\rName : Jaeyoung Sim\n"},{"id":2,"href":"/development/firstpost/","title":"í• ì¼","section":"Development / ê°œë°œ","content":"ê°œë°œ ê´€ë ¨ í•­ëª©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•œ post ì…ë‹ˆë‹¤.\nTodo List:\nìƒíƒœ ë‚ ì§œ ì‘ì—… ë‚´ìš© âœ… 2025-05-21 ë¸”ë¡œê·¸ GitHub Page ì—°ë™ âœ… 2025-05-22 RL ê´€ë ¨ Post ë‚¨ê¸°ê¸° (on-going) âœ… 2025-05-22 hugo + github page + giscus âœ… 2025-05-22 Google ì„œì¹˜ ì—°ë™ â˜ Naver ì„œì¹˜ ì—°ë™ âœ… 2025-05-22 ë¸”ë¡œê·¸ setup ê´€ë ¨ post ë‚¨ê¸°ê¸° â˜ About Me ì™„ì„± â˜ Google ì„œì¹˜ ì—°ë™ "},{"id":3,"href":"/development/blogsetup/","title":"Github pages ë¥¼ ì´ìš©í•œ blog Setting (hugo + hugo-book theme + giscus","section":"Development / ê°œë°œ","content":"\rGithub pages ë¥¼ ì´ìš©í•œ blog Setting (hugo + hugo-book theme + giscus\r#\r0. blog host ë°©ì‹ ì„ íƒ\r#\rì—¬ëŸ¬ ë¸”ë¡œê·¸ hostë°©ì‹ì„ ê³ ë ¤í•˜ì˜€ìœ¼ë‚˜, ìœ ì§€ë³´ìˆ˜ê°€ ì†ì´ ëœê°€ë©°, ì˜¤ë«ë™ì•ˆ hostingì´ ë˜ëŠ”ê²ƒì´ ìš°ì„  ìˆœìœ„ì˜€ê³ .\nGithub pagesë¥¼ í†µí•œ í˜¸ìŠ¤íŒ… ë°©ë²•ì„ ì„ íƒí•˜ì˜€ë‹¤.\nê·¸ë ‡ë‹¤ë©´, static pageë¥¼ generationì„ í•´ì£¼ëŠ” frameworkë¡œ jekyll ì™€ hugoë¥¼ ê³ ë¯¼í•˜ì˜€ê³  ruby ë³´ë‹¤ goë¡œ ì´ë£¨ì–´ì§„ hugoë¥¼ ë¯¿ê¸°ë¡œ í•˜ì˜€ë‹¤.\në‹¤ë¥¸ blogë“¤ì´ ìƒì„±í•œ í˜ì´ì§€ë¥¼ ì£¼ë¡œ ì°¸ì¡°í•˜ì˜€ìœ¼ë©°, ì´ í¬ìŠ¤íŒ…ì€ í•´ë‹¹ ê¸€ë“¤ì˜ ì—®ìŒì— ë¶ˆê³¼í•œì ì„ ì°¸ì¡° ë¶€íƒí•œë‹¤.\n1. Setup git, hugo\r#\rê°œë°œí™˜ê²½ì€ window 11 (Intel processor) ì´ë¯€ë¡œ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆë‹¤.\ngit ì„¤ì¹˜\r#\rhttps://git-scm.com/downloads ì‚¬ì´íŠ¸ì—ì„œ Git for Windows/x64 Setup. í•­ëª©ì„ í´ë¦­í•˜ì—¬ githubì„ ì„¤ì¹˜í•˜ì˜€ë‹¤. ê° ì„¤ì •ì€ defaultë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤.\nhugo ì„¤ì¹˜\r#\rhttps://github.com/gohugoio/hugo/releases ì—ì„œ ë³¸ì¸ì— ë§ëŠ” ìµœì‹  ë²„ì „ì„ ë°›ëŠ”ë‹¤. (hugo_extended_0.147.4_windows-amd64.zip)\ní›„ì— ì–¸ê¸‰í•˜ê² ì§€ë§Œ hugo-book theme ì˜ ê²½ìš° hugp-extended ë²„ì „ì„ ë°›ì•„ì•¼í–ˆë‹¤.\nhugo ëª…ë ¹ì–´ë¥¼ ì¹˜ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ, í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•˜ì˜€ë‹¤. (ë˜ë„ë¡ path íŒŒì‹±ì— ì—ëŸ¬ê°€ ì—†ë„ë¡ í´ë”ë“¤ì„ ì˜ì–´ë¡œ êµ¬ì„±í•˜ì˜€ë‹¤.)\ngithub repository ìƒì„±\r#\rhttps://minyeamer.github.io/blog/hugo-blog-1/ ì—ì„œ ì–¸ê¸‰ëœê²ƒ ê°™ì´ í•˜ë‚˜ì˜ repositoryë¥¼ branchë¡œ ë‚˜ëˆ„ì–´ì„œ submoduleë¡œ í™œìš©í•˜ëŠ” ë°©ì•ˆì„ ì±„íƒí–ˆë‹¤.\n\u0026lt;USERNAME\u0026gt;.github.io ì¸ repositoryë¥¼ í•˜ë‚˜ ìƒì„±í•œë‹¤. UserNameì´ Jon ì´ë¼ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìƒì„±ë  ê²ƒì´ë‹¤. Jon/Jon.github.io\nhugoë¥¼ í†µí•œ ê¸°ë³¸ ë¼ˆëŒ€ ë§Œë“¤ê¸°. ì—¬ê¸°ì„œ í´ë”ëª…ì„ ìƒì„±í•œ repositoryë¡œ ë§ì¶˜ë‹¤.\nhugo new site \u0026lt;USERNAME\u0026gt;.github.io cd \u0026lt;USERNAME\u0026gt;.github.io git init git add . git commit -m \u0026#34;feat: new site\u0026#34; git branch -M main git remote add origin https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git git push -u origin main branch ë§Œë“¤ê¸° git branch gh-pages main git checkout gh-pages git push origin gh-pages git checkout main gh-pagesë¥¼ submoudleë¡œ ì—°ë™í•˜ê¸° rm -rf public git submodule add -b gh-pages https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public git add public git add .gitmodules git commit -m \u0026#34;feat: add submodule for github pages\u0026#34; git push hugo Theme : hugo-book\r#\rhttps://themes.gohugo.io/ ì—ì„œ ì—¬ëŸ¬ í…Œë§ˆê°€ ì„ íƒì´ ê°€ëŠ¥í•˜ê³ , ê°€ë²¼ì›Œë³´ì´ëŠ” hugo-book theme ë¥¼ ì±„íƒí•˜ì˜€ë‹¤\nhttps://github.com/alex-shpak/hugo-book ì— Read.mdë¥¼ ì°¸ì¡°í•˜ì˜€ë‹¤.\ngit submodule add https://github.com/alex-shpak/hugo-book themes/hugo-book git add . git commit -m \u0026#34;feat: import hugo theme\u0026#34; root path ì— ìˆëŠ” hugo.toml íŒŒì¼ì— ì•„ë˜ ë‚´ìš©ì„ ì‚½ì…í•œë‹¤.\nbaseURL = \u0026#39;https://\u0026lt;USERNAME\u0026gt;.github.io\u0026#39;\rlanguageCode = \u0026#39;ko-kr\u0026#39;\rtitle = \u0026#34;\u0026lt; title what you want\u0026gt;\u0026#34;\rtheme = \u0026#39;hugo-book\u0026#39; Github ì„¤ì •\r#\rGithub -\u0026gt; .github.io -\u0026gt; Settings -\u0026gt; Pages -\u0026gt; Branch -\u0026gt; gh-pages ë¡œ ë³€ê²½\n2. ë¸”ë¡œê·¸ ë°°í¬ ë°©ë²•\r#\rhugo static page ìƒì„±\r#\rì•„ë˜ ëª…ë ¹ì–´ë¡œ \u0026lt;root-path\u0026gt;/content/ pathì— firstPost.md íŒŒì¼ì´ ìƒì„±ëœë‹¤\nhugo new firstPost.md ì•„ë˜ ì»¤ë§¨ë“œë¥¼ ì´ìš©í•˜ë©´ í˜„ì¬ static í˜ì´ì§€ë¥¼ 127.0.0.1:1313 ì—ì„œ í™•ì¸ì´ ê°€ëŠ¥í•˜ë‹¤\nhugo server -D ì•„ë˜ì™€ ê°™ì´ draftê°€ trueê°€ ìˆë‹¤ë©´ ë””ë²„ê¹…ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ìµœì¢… ì‚°ì¶œë¬¼ì—ì„œ ë¹ ì§€ê²Œ ëœë‹¤. ë”°ë¼ì„œ ë‚˜ì¤‘ì— ë¹¼ë†“ì§€ ë§ê³  draft = true ë¬¸êµ¬ë¥¼ ì§€ìš°ë˜ falseë¡œ ë³€ê²½í•œë‹¤\n// firstPost.md\r+++\rtitle = \u0026#39;firstPost\u0026#39;\rdraft = true\r+++ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ í†µí•´ì„œ public/ í´ë” ë°‘ì— ì‚°ì¶œë¬¼ë“¤ì„ ìƒì„±í•œë‹¤.\nhugo ì•„ë˜ ëª…ë ¹ì–´ë¥¼ í†µí•´ì„œ github repositoryì— ì‚°ì¶œë¬¼ë“¤ì„ ë°°í¬í•œë‹¤. ì´ ì‚°ì¶œë¬¼ë“¤ì€ gitub action ì— ì˜í•´ì„œ ìë™ìœ¼ë¡œ hostingë˜ë„ë¡ ì²˜ë¦¬ëœë‹¤.\ncd public git add . git commit -m \u0026#34;\u0026lt;comment what you want to add\u0026gt;\u0026#34; git push origin gh-pages cd .. git add . git commit -m \u0026#34;\u0026lt;comment what you want to add\u0026gt;\u0026#34; git push origin main 3. ì¶”ê°€ hugo ì„¸íŒ…íŒ\r#\rrecent-post ìš© home ë§Œë“¤ê¸°\r#\rì•„ë˜ì™€ ê°™ì´ í´ë” ë° ê¸°ë³¸ md íŒŒì¼ì„ êµ¬ì„±í•˜ì˜€ê³ .\ncontent\râ”œâ”€â”€ _index.md\râ”œâ”€â”€ Content-Category1\râ”‚ â”œâ”€â”€ _index.md\râ”‚ â””â”€â”€ article1.md\râ””â”€â”€ Content-Category2\râ”œâ”€â”€ _index.md\râ””â”€â”€ post1.md conent/_index.md íŒŒì¼ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•˜ì˜€ë‹¤.\n\u0026ldquo;recent-posts\u0026rdquo; ì—ì„œ \u0026quot; ì„ ì‚­ì œ\r--- title: \u0026#34;Home\u0026#34; comments: false --- # **Recent Posts** {{ {{ \u0026#34;recent-posts\u0026#34; }} }} layouts/shortcodes/recent-posts.html ì„ ìƒì„±í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ì‘ì„±\n\u0026lt;style\u0026gt; .summary-box { background: #f8f9fa; padding: 1rem; border-radius: 8px; transition: background 0.3s ease; max-height: 14rem; /* ë†’ì´ ì œí•œ */ overflow: hidden; /* ë„˜ì¹˜ëŠ” ë‚´ìš© ìˆ¨ê¹€ */ display: -webkit-box; -webkit-line-clamp: 6; /* ìµœëŒ€ ì¤„ ìˆ˜ (ì˜ˆ: 6ì¤„) */ -webkit-box-orient: vertical; text-overflow: ellipsis; /* ... ì²˜ë¦¬ */ } .summary-box h1, .summary-box h2, .summary-box h3, .summary-box h4, .summary-box h5, .summary-box h6, .summary-box p { margin: 0 0 1rem 0; } .summary-box:hover { background: #e9ecef; } \u0026lt;/style\u0026gt; \u0026lt;ul\u0026gt; {{ range (first 5 (sort .Site.RegularPages \u0026#34;Date\u0026#34; \u0026#34;desc\u0026#34;)) }} \u0026lt;li style=\u0026#34;margin-bottom: 2rem;\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt; \u0026lt;strong style=\u0026#34;font-size: 1.25rem;\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/strong\u0026gt; \u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; {{ with .Params.subtitle }} \u0026lt;small style=\u0026#34;color: #888;\u0026#34;\u0026gt;{{ . }}\u0026lt;/small\u0026gt;\u0026lt;br/\u0026gt; {{ end }} \u0026lt;small\u0026gt;{{ .Date.Format \u0026#34;2006-01-02 15:04:05 MST\u0026#34; }}\u0026lt;/small\u0026gt; \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34; style=\u0026#34;text-decoration: none; color: inherit;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;summary-box\u0026#34;\u0026gt; \u0026lt;!-- delete anchor in title--\u0026gt; {{ .Summary | replaceRE \u0026#34;\u0026lt;a class=\\\u0026#34;anchor\\\u0026#34; href=\\\u0026#34;#.*?\\\u0026#34;\u0026gt;#\u0026lt;/a\u0026gt;\u0026#34; \u0026#34;\u0026#34; | safeHTML | truncate 400}} \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; MarkDown code block copy ë²„íŠ¼ ë§Œë“¤ê¸°\r#\rlayouts/partials/docs/body.html ì„ ìƒì„±í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ì‘ì„±\n\u0026lt;script\u0026gt; document.addEventListener(\u0026#39;DOMContentLoaded\u0026#39;, () =\u0026gt; { document.querySelectorAll(\u0026#39;pre \u0026gt; code[class^=\u0026#34;language-\u0026#34;]\u0026#39;).forEach(codeBlock =\u0026gt; { const pre = codeBlock.parentElement; // ì½”ë“œë¸”ëŸ­ì˜ ë³µì‚¬ ë²„íŠ¼ ìƒì„± const button = document.createElement(\u0026#39;button\u0026#39;); button.innerText = \u0026#39;ğŸ“‹\u0026#39;; button.title = \u0026#39;Copy code\u0026#39;; button.style = ` position: absolute; top: 0.5em; right: 0.5em; padding: 2px 6px; font-size: 0.8rem; background: #f5f5f5; border: 1px solid #ccc; border-radius: 4px; cursor: pointer; z-index: 10; `; // ë³µì‚¬ ë™ì‘ ì •ì˜ button.addEventListener(\u0026#39;click\u0026#39;, () =\u0026gt; { navigator.clipboard.writeText(codeBlock.innerText); button.innerText = \u0026#39;âœ”\u0026#39;; setTimeout(() =\u0026gt; button.innerText = \u0026#39;ğŸ“‹\u0026#39;, 1000); }); // ìŠ¤íƒ€ì¼ ì ìš© pre.style.position = \u0026#39;relative\u0026#39;; pre.appendChild(button); }); }); \u0026lt;/script\u0026gt; 4. giscus ì—°ë™í•˜ê¸°\r#\rgiscus githubì— ì„¤ì¹˜ ë° discuss í™œì„±í™”\r#\rhttps://github.com/apps/giscus ì—ì„œ giscus install\ngithub page repository ë§Œ ì„ íƒ\nSetting -\u0026gt; Discusss í™œì„±í™”\nDiscussions ì—ì„œ ì¢Œì¸¡ Categories ì„ íƒí•˜ì—¬ ìƒˆ ì¹´í…Œê³ ë¦¬ ë§Œë“¤ê¸°\nCategory Name ì™€ Descriptionì„ ê°ê° ì…ë ¥í•˜ê³  Discussion Format ì„ Announcement ì„ íƒ\nhugo ì—ì„œ ë³´ì´ê²Œ ì„¤ì •\r#\rhttps://giscus.app/ko í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬\n/.github.io ë¡œ ì €ì¥ì†Œ ì…ë ¥ Discussion ì œëª©ì´ í˜ì´ì§€ ê²½ë¡œ í¬í•¨ ì„ íƒ í˜ì´ì§€ì— ìƒì„±ëœ scriptë¥¼ ë³µì‚¬ (ì•„ë˜ì™€ ìœ ì‚¬í•œ í˜•ì‹) \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026lt;.github.io\u0026#34; data-repo-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-category=\u0026#34;Comment\u0026#34; data-category-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-mapping=\u0026#34;pathname\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;preferred_color_scheme\u0026#34; data-lang=\u0026#34;ko\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; ì´ë¥¼ `layouts/partials/comments.htmlì— ì•„ë˜ì²˜ëŸ¼ ë¶™ì—¬ë„£ê¸° {{ if and (.IsPage) (not .IsHome) (not (eq .Params.comments false)) }} \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026lt;.github.io\u0026#34; data-repo-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-category=\u0026#34;Comment\u0026#34; data-category-id=\u0026#34;\u0026lt; String \u0026gt;\u0026#34; data-mapping=\u0026#34;pathname\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;preferred_color_scheme\u0026#34; data-lang=\u0026#34;ko\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; {{ end }} comments = falseë¥¼ ë„£ì€ í˜ì´ì§€ë“¤ì€ github ì½”ë©˜íŠ¸ê°€ ì•ˆë³´ì„\r5. tag ì„¤ì •\r#\rhugo.toml ì— ì•„ë˜ ì„¤ì • ì‚½ì…\n[taxonomies] tag = \u0026#34;tags\u0026#34; category = \u0026#34;categories\u0026#34; ê° í¬ìŠ¤íŠ¸ë§ˆë‹¤ ì•„ë˜ front matterì— ì•„ë˜ í•­ëª©ì„ ê¸°ì…\n+++ ... tags = [\u0026#34;Definition\u0026#34;, \u0026#34;Essential\u0026#34;] categories = [\u0026#34;Reinforcement Learning\u0026#34;] ... +++ ì´ ë¸”ë¡œê·¸ì˜ ê²½ìš°, ìš°ì¸¡ ToC í•­ëª© í•˜ë‹¨ì— ìœ„ì¹˜ Tag ë° Categoriesë¥¼ ìœ„ì¹˜ì‹œí‚´\nì´ëŠ” í…Œë§ˆë§ˆë‹¤ ë‹¤ë¥´ë‹ˆ ê° í…Œë§ˆë³„ ì ìš© ë°©ë²• í™•ì¸ í•„ìš”\nlayouts/partials/doc/inject/toc-after.html ì„ ìƒì„±\n{{ with .Params.tags }} \u0026lt;div class=\u0026#34;post-tags\u0026#34;\u0026gt; \u0026lt;strong\u0026gt;Tags:\u0026lt;/strong\u0026gt; {{ range . }} \u0026lt;a href=\u0026#34;{{ \u0026#34;tags/\u0026#34; | relLangURL }}{{ . | urlize }}\u0026#34; class=\u0026#34;tag\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} {{ with .Params.categories }} \u0026lt;div class=\u0026#34;post-categories\u0026#34;\u0026gt; \u0026lt;strong\u0026gt;Categories:\u0026lt;/strong\u0026gt; {{ range . }} \u0026lt;a href=\u0026#34;{{ \u0026#34;categories/\u0026#34; | relLangURL }}{{ . | urlize }}\u0026#34; class=\u0026#34;category\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} ì¹´í…Œê³ ë¦¬ ë° tag ì„ íƒí–ˆì„ë•Œ í•´ë‹¹ listê°€ ë³´ì´ë„ë¡ ì„¤ì • layouts/_default/term.html ì„ ìƒì„±\n{{ define \u0026#34;main\u0026#34; }} \u0026lt;main\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; {{ range .Pages }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt; {{ .Date }}\u0026lt;/li\u0026gt; {{ else }} \u0026lt;li\u0026gt;\u0026lt;em\u0026gt;No posts found.\u0026lt;/em\u0026gt;\u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; \u0026lt;/main\u0026gt; {{ end }} 6. ê²€ìƒ‰ ì—°ë™\r#\rêµ¬êµ´ ì„œì¹˜ ì½˜ì†”\r#\rì†Œìœ ê¶Œ í™•ì¸ https://search.google.com/search-console/about ì ‘ì†í•˜ì—¬ url \u0026lt;USERNAME\u0026gt;.github.io ì„ ì…ë ¥ google_.html ì„ ë‹¤ìš´ë¡œë“œ í›„ì— public/ ì— ìœ„ì¹˜ì‹œí‚´\nsitemap ë§Œë“¤ê¸° hugo.toml ì— ì•„ë˜ì™€ ê°™ì´ ì‚½ì… -\u0026gt; sitemap.xmlì´ ìƒì„±ëœê²ƒì„ í™•ì¸ enableRobotsTXT = true [sitemap] # always, hourly daily, weekly, monthly, yearly, never changefreq = \u0026#34;weekly\u0026#34; filename = \u0026#34;sitemap.xml\u0026#34; priority = 0.5 layouts/robots.txt íŒŒì¼ì„ ìˆ˜ì • User-agent: * Allow: / Sitemap: {{ .Site.BaseURL }}sitemap.xml hugo.toml ì—ì„œ ì•„ë˜ ì„¤ì •\nenableRobotsTXT = true ì•„ë˜ ì»¤ë§¨ë“œë¡œ ë¹Œë“œì‹œì— public/robots.txt ìƒì„±ì„ í™•ì¸\nhugo sitemap.xmlì„ ì„œì¹˜ ì½˜ì†”ì— ë“±ë¡ ë„¤ì´ë²„ ì„œì¹˜ ì–´ë“œë°”ì´ì ¸\r#\rnaver search advisor ì ‘ì† blog url ì…ë ¥ html ë‹¤ìš´ë¡œë“œí›„ public ì•ˆì— ìœ„ì¹˜ì‹œí‚¤ê³  ë°°í¬ -\u0026gt; ì†Œìœ ê¶Œ í™•ì¸ ì°¸ì¡°\r#\rhttps://ialy1595.github.io/post/blog-construct-1/\nhttps://ialy1595.github.io/post/blog-construct-2/\nhttps://minyeamer.github.io/blog/hugo-blog-1/\nhttps://d5br5.dev/blog/nextjs_blog/giscus\nhttps://velog.io/@eona1301/Github-Blog-%EA%B2%80%EC%83%89%EC%B0%BD-%EB%85%B8%EC%B6%9C%EC%8B%9C%ED%82%A4%EA%B8%B0\nhttps://golangkorea.github.io/post/hugo-intro/taxonomy-basic/\nhttps://boyinblue.github.io/002_github_blog/003_naver_search_advisor.html\n"},{"id":4,"href":"/learned-query-optimizer/loger-code/","title":"LOGER ì½”ë“œ ë¶„ì„","section":"Learned Query Optimizer","content":"\rLOGER í”„ë¡œì íŠ¸ì˜ ì½”ë“œ ë¶„ì„ ë‚´ìš©\r#\rê¸°ë³¸ì •ë³´\r#\rLOGER: A Learned Optimizer towards Generating Efficient and Robust Query Execution Plans github repository https://github.com/TianyiChen0316/LOGER LOGER ì‹¤í–‰ì„ ìœ„í•œ ì…‹ì—…\r#\r# 3.8 ì´ì—¬ì•¼í•œë‹¤. conda create -n loger python=3.8 -y conda activate loger pip install -r requirements.txt pip install pandas pip install dgl pip install packaging conda install libffi export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH pip install dgl-cu110==0.6.1 mkdir results postgre ì„¤ì¹˜\nì•„ë˜ë‚´ìš©ì€ balsa í”„ë¡œì íŠ¸ì— ìˆëŠ” ë‚´ìš©\rcd ~ wget https://ftp.postgresql.org/pub/source/v12.5/postgresql-12.5.tar.gz tar xzvf postgresql-12.5.tar.gz cd postgresql-12.5 ./configure --prefix=$HOME/postgresql-12.5 --without-readline make -j make install echo \u0026#39;export PATH=/home/jae.sim/postgresql-12.5/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc cd ~/ git clone https://github.com/ossc-db/pg_hint_plan.git -b REL12_1_3_7 cd pg_hint_plan/ vim Makefile ### # Modify Makefile: change line # PG_CONFIG = pg_config # to # PG_CONFIG = /home/jae.sim/postgresql-12.5/bin/pg_config pg_ctl -D ~/imdb initdb pg_ctl -D ~/imdb start -l logfile ì•„ë˜ ë‚´ìš©ì„ ì¶”ê°€\n# train.py # ì•„ë˜ ë‚´ìš©ì„ ì¶”ê°€ parser.add_argument(\u0026#39;--host\u0026#39;, type=str, default=\u0026#39;/tmp\u0026#39;, help=\u0026#39;PostgreSQL host path\u0026#39;) LOGER ì‹¤í–‰ ì»¤ë§¨ë“œ\r#\r# start postgre pg_ctl -D ~/imdb start -l logfile conda activate loger ## database ì„¤ì •ì€ ì•„ë˜ ë‚´ìš© ì°¸ì¡° python train.py --database imdbload --port 5437 --host /tmp -U \u0026#34;\u0026#34; # train.py parser.add_argument(\u0026#39;-D\u0026#39;, \u0026#39;--database\u0026#39;, type=str, default=\u0026#39;imdb\u0026#39;, help=\u0026#39;PostgreSQL database.\u0026#39;) parser.add_argument(\u0026#39;-U\u0026#39;, \u0026#39;--user\u0026#39;, type=str, default=\u0026#39;postgres\u0026#39;, help=\u0026#39;PostgreSQL user.\u0026#39;) parser.add_argument(\u0026#39;-P\u0026#39;, \u0026#39;--password\u0026#39;, type=str, default=None, help=\u0026#39;PostgreSQL user password.\u0026#39;) parser.add_argument(\u0026#39;--port\u0026#39;, type=int, default=None, help=\u0026#39;PostgreSQL port.\u0026#39;) high level flow\r#\rtrain.py .main\nê¸°ì´ˆ Setup = train_set, test_set ready, database connect, log, cache ë“±ì„ ì„¤ì •, ìì²´ í´ë˜ìŠ¤ DeepQNet ëª¨ë¸ initiailize DeepQNet ì—ëŠ” Step1, Step2, PredictTail ì´ ì„¸ê°œì˜ Nueral Networkë¥¼ ê°€ì§ Step1 : table level feature encoding Step2 : ë‘ í…Œì´ë¸”ê°„ embeddingì„ LSTM ê¸°ë°˜ìœ¼ë¡œ join representationì„ ìƒì„± PredictTail : ìƒì„±ëœ ì¿¼ë¦¬ê°€ ì¢‹ì€ì§€ íŒë‹¨í•˜ëŠ” êµ¬ì¡°. ìš”ì•½ Step1: í…Œì´ë¸” ì„ë² ë”© ìƒì„± (GNN) Step2: pairwise join composition (LSTM) PredictTail: partial plan value ì˜ˆì¸¡ (value head) UseGeneratedPredict: ìƒì„±ëœ plan ê²€ì¦ (classifier head) \u0026lt;\u0026ndash; ì•ˆì”€\ní…Œì´ë¸” ì„ë² ë”©ì„ ë§Œë“¤ì—ˆìœ¼ë‹ˆ, ìƒˆë¡œìš´ ì›Œí¬ë¡œë“œ ì— ëŒ€í•´ì„œëŠ” í…Œì´ë¸” ì„ë² ë”©ì´ ë§ì´ í‹€ë¦´í…Œë‹ˆ, ì•„ì˜ˆ ì˜ˆì¸¡ ìì²´ë¥¼ ëª»í•˜ë‚˜?\rtrain.py .train()\nStep1, Step2, PredictTail ê°œ ë¥¼ training train_mode, test_mode of model\r#\rDeepQNetì€ ëª¨ë“œë³„ë¡œ ë™ì‘ì„ ìˆ˜í–‰í•˜ë„ë¡ ì‘ì„±ë¨\n# LOGER/model/dqn.py class DeepQNet: ... def train_mode(self): self.model_step1.train() self.model_step2.train() self.model_tail.train() def eval_mode(self): self.model_step1.eval() self.model_step2.eval() self.model_tail.eval() workload sql íŒŒì‹±\r#\rLOGERëŠ” sqlì„ íŒŒì‹±í•˜ëŠ” ë¡œì§ì„ ë³„ë„ì˜ .so íŒŒì¼ë¡œ ë§Œë“¤ì–´ë‘ê³  ì´ë¥¼ importí•´ì„œ ì‚¬ìš©í•¨ # LOGER/core/sql.py from psqlparse import parse_dict ... class Sql: ... parse_result_all = parse_dict(self.sql) ... ì—¬ê¸°ì—ì„œ parse_dictëŠ” ì•„ë˜ì™€ ê°™ì´ ë˜ì–´ìˆìœ¼ë©°, parser.cpython-38-x86_64-linux-gnu.so ëŠ” í”„ë¡œì íŠ¸ì— ê°™ì´ íƒ‘ì¬ë˜ì–´ ìˆë‹¤.\n# LOGER/psqlparse/parser.py def __bootstrap__(): global __bootstrap__, __loader__, __file__ import sys, pkg_resources, importlib.util __file__ = pkg_resources.resource_filename(__name__, \u0026#39;parser.cpython-38-x86_64-linux-gnu.so\u0026#39;) __loader__ = None; del __bootstrap__, __loader__ spec = importlib.util.spec_from_file_location(__name__,__file__) mod = importlib.util.module_from_spec(spec) spec.loader.exec_module(mod) __bootstrap__() ìƒìš© db ì—°ê²°ì˜ í”ì \r#\roracle databaseë¥¼ ì‚¬ìš©í•œê²ƒìœ¼ë¡œ ì¶”ì •. ì–´ëŠì •ë„ êµ¬í˜„ì´ ë˜ì–´ìˆëŠ”ì§€ëŠ” ë¯¸í™•ì¸\n# LOGER/train.py ... if args.oracle is not None: USE_ORACLE = True oracle_database.setup(args.oracle, dbname=args.database, cache=False) try: database.setup(dbname=args.database, cache=False) ... "},{"id":5,"href":"/reinforcement-learning/rl-essential/","title":"1. Reinforcement Learning Essential","section":"Reinforcement Learning / ê°•í™”í•™ìŠµ","content":"\r1. ê°•í™”í•™ìŠµì— ëŒ€í•œ ê¸°ì´ˆ ë‚´ìš©\r#\rë“¤ì–´ê°€ê¸° ì•ì„œ\r#\rê°•í™”í•™ìŠµì˜ ê¸°ì´ˆì ì¸ ë‚´ìš©ì„ í•™ìŠµí•œ ë’¤ ì •ë¦¬í•œ ê²ƒìœ¼ë¡œ, ë‚¨ë“¤ì—ê²Œ ë³´ì—¬ì£¼ê¸° ë³´ë‹¤ëŠ” ë³¸ì¸ì˜ ì´í•´ì™€ ê¸°ì–µì„ ìœ„í•´ì„œ ê¸°ìˆ í•œ ê²ƒì…ë‹ˆë‹¤.\në‚˜ë§Œì˜ ë°©ì‹ìœ¼ë¡œ ì´í•´í•œ ê²ƒì´ê¸° ë•Œë¬¸ì—, ì£¼ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” ë¶€ë¶„ì´ ë‹¤ë¥¼ìˆ˜ ìˆìœ¼ë©° ìƒëµë˜ê±°ë‚˜ ë†“ì¹œ ë¶€ë¶„ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤.\nê°•í™”í•™ìŠµ(RL: Reinforcement Learning) ì´ë€?\r#\rDefinition\r#\râ€œReinforcement learning is learning what to doâ€”how to map situations to actionsâ€”so as to maximize a numerical reward signal.â€\nâ€” Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction (2nd ed), p.1\nsituationsì„ stateë¡œ í‘œê¸°í•˜ì—¬\ní†µìƒì ìœ¼ë¡œ action, stateì™€ reward ê°€ RLì˜ í•µì‹¬ ìš”ì†Œì´ë‹¤.\nìš”ì•½í•˜ë©´, ì£¼ì–´ì§„ ìƒíƒœ(State)ì—ì„œ ë³´ìƒ(Reward)ì„ ìµœëŒ€í™” í•  ìˆ˜ ìˆëŠ” í–‰ë™(Action)ì„ í•™ìŠµí•˜ëŠ” ê²ƒ\nì´ëŠ” Reward Hypothesisë¥¼ ê¸°ë°˜ìœ¼ë¡œí•œë‹¤.\nReward Hypothesis\nAll goals can be described by the maximisation of expected cumulative reward\nRewardsëŠ” Scalar feedback signalì´ë‹¤ AgentëŠ” ë¯¸ë˜ì— ê¸°ëŒ€ë˜ëŠ” cumulative rewardê°€ ìµœëŒ€í™” ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ í•œë‹¤\nState and MDP\r#\rAgentëŠ” í•™ìŠµí•˜ëŠ” ì£¼ì²´ (ë‡Œ, ë¡œë´‡) EnvironmentëŠ” í™˜ê²½ (ì§€êµ¬, ê²Œì„) //ìš°ë¦¬ëŠ” í™˜ê²½ì´ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ë³´í†µ ëª¨ë¦„ State ëŠ” ë‹¤ìŒ actionì„ ê²°ì •í•˜ê¸° ìœ„í•œ ì •ë³´ì´ë‹¤ \\[S_t = f(H_t) \\]\rAgentê°€ ì£¼ì–´ì§„ ìƒíƒœ(State)ì—ì„œ ë³´ìƒ(Reward)ì„ ìµœëŒ€í™” í•  ìˆ˜ ìˆëŠ” í–‰ë™(Action)ì„ í•™ìŠµí•˜ëŠ” ê²ƒ\nState ì—ëŠ” Environment State, Agent State ê°€ ìˆê³  ê°ê° Environment, Agentê´€ì ì—ì„œì˜ ìˆ˜ì‹ì /ë‚´ë¶€ì  í‘œí˜„ì´ë‹¤.\nEnvironment StateëŠ” Environment ê´€ì ì—ì„œ ë‹¤ìŒ stepì—ì„œ Environmentê°€ ì–´ë–»ê²Œ ë³€í™”í• ì§€ë¥¼(ì–´ë–¤ Stateë¡œ ë³€í™”í• ì§€) ë‚˜íƒ€ë‚¸ë‹¤. Environmentì—ì„œëŠ” microsecond ì—ì„œë„ ìˆ˜ë§ì€ ì •ë³´ê°€ ì˜¤ê¸° ë•Œë¬¸ì— ë¶ˆí•„ìš”í•œ ì •ë³´ë“¤ë„ ë§ë‹¤. ë³´í†µì˜ RL ë¬¸ì œì—ì„œ agentëŠ” Environment stateë¥¼ ì „ë¶€ ê´€ì¸¡í•  ìˆ˜ ì—†ë‹¤.\nì˜ˆ) ë¡œë´‡ì˜ ì›€ì§ì„ì„ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„í–ˆëŠ”ë°. ì´ ë¡œë´‡ì€ ì§€êµ¬ì˜ ì¤‘ë ¥ì´ë‚˜, ë§ˆì°°ë ¥ ìœ¼ë¡œ ë¬¼ê±´ì´ ì–´ë””ë¡œ ì›€ì§ì´ëŠ”ì§€ ì •í™•í•˜ê²Œ ëª¨ë¥¸ë‹¤. Agent StateëŠ” ë‹¤ìŒ step ì—ì„œ Agentê°€ ì–´ë–¤ í–‰ë™ì„ ì„ íƒí• ì§€ë¥¼ ë‚˜íƒ€ë‚¸ ìˆ˜ì‹/í‘œí˜„ì´ë‹¤.\nInformation StateëŠ” ê³¼ê±° historyë¶€í„° ëª¨ë“  ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•œ ìˆ˜í•™ì  ì •ì˜ë¥¼ ê°€ì§„ Stateì´ë‹¤. ì£¼ë¡œ Markov Stateë¼ ë¶€ë¥¸ë‹¤. (Markov ì†ì„±ì„ ë§Œì¡±í•œë‹¤)\nMarkov Properties : ì´ì „ì˜ ëª¨ë“  State ì •ë³´ë¥¼ ì´ìš©í•´ì„œ ë‹¤ìŒ Stateë¥¼ ì„ íƒí•˜ëŠ”ê²ƒì´, í˜„ì¬ Stateë§Œ ë³´ê³  í•˜ëŠ”ê²ƒê³¼ ê°™ë‹¤ \\[\r\\mathbb{P}[S_{t+1} \\mid S_t] = \\mathbb{P}[S_{t+1} \\mid S_1, \\dots, S_t]\r\\]\rì´ë¥¼ ì´ìš©í•˜ë©´, ë¯¸ë˜(future) ëŠ” ê³¼ê±°ì— ë¬´ì—‡ì´ ì£¼ì–´ì¡Œë“ ì§€ ë…ë¦½ì ì´ë‹¤. (=The future is independent of the past given the present) ë‹¬ë¦¬ë§í•˜ë©´, í˜„ì¬ \\(S_t\\)\rë§Œ ì €ì¥í•´ë„ ëœë‹¤ \\[\rH_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\r\\]\rí˜„ì¬ì˜ Stateê°€ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì´ë¯¸ ë‹´ê³  ìˆë‹¤ë‹¤.\nAppendix\rhistory ëŠ” Observationê³¼ actions, rewardsì˜ ì—°ì†ì´ë‹¤ \\[\r\\quad \\quad H_t = O_1, R_1, A_1, ..., A_{tâˆ’1}, O_t, R_t\r\\]\rState ëŠ” ë‹¤ìŒ actionì„ ê²°ì •í•˜ê¸° ìœ„í•œ ì •ë³´ì´ë‹¤ \\[\r\\quad \\quad S_t = f(H_t)\r\\\\\r\\]\rì´ì „ stateëŠ” \\[\r\\mathbb{P}[S_{t+1} \\mid S_t] = \\mathbb{P}[S_{t+1} \\mid S_1, \\dots, S_t]\r\\]\rThe future is independent of the past given the present ëª¨ë“  ì´ì „ Stateë¥¼ ì•Œì§€ ì•Šì•„ë„ ì§ì „ Stateë§Œ ë³´ê³  ê²°ì • í•  ìˆ˜ ìˆë‹¤ \\[\rH_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\r\\]\rFully Observable Environments ëŠ” Agentê°€ environment ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ë°”ë¡œ ê´€ì¸¡ì´ ê°€ëŠ¥í•¨ì„ ë‚˜íƒ€ë‚´ê³ , ê²°ê³¼ì ìœ¼ë¡œ Environment State = Information State = Agent State ìƒíƒœì´ë‹¤. ì´ë¥¼ Markov Decision Process(MDP) ë¼ê³  í•œë‹¤\nPartial Observable Environments ëŠ” ì¢€ë” í˜„ì‹¤ì ì¸ í™˜ê²½. ë¡œë´‡ì´ ì¹´ë©”ë¼ë¥¼ í†µí•´ì„œ í™”ë©´ì„ ë³´ì§€ë§Œ í˜„ì¬ ìê¸°ì˜ ìœ„ì¹˜ë¥¼ ëª¨ë¥´ëŠ” ê²ƒì²˜ëŸ¼. ì¦‰, Agent State \\( \\ne \\)\rEnvironment State ì´ë‹¤. Partially Observable Markov Decision Process(POMDP) ë¡œ ìˆ˜ì‹ì´ í‘œí˜„ëœë‹¤.\nAgentëŠ” ìê¸°ì˜ Stateì— ëŒ€í•œ representationì„ ê°€ì ¸ì•¼ë§Œí•˜ê³ , ì´ëŠ”\nì´ì „ Historyë¥¼ ì´ìš©í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ìˆê³ , \\[S_t^a = H_t\\]\rProbability ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•ì´ ìˆê³ , \\[S_t^a = \\left( \\mathbb{P}[S_t^e = s_1], \\dots, \\mathbb{P}[S_t^e = s_n] \\right)\\]\rìˆœí™˜ì‹ ê²½ë§(Recurrent neural network) ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•ë„ ìˆë‹¤. \\[S_t^a = \\sigma(S_{t-1}^a W_s + O_t W_o)\\]\rPolicy, Value Function, Model\r#\rRLì€ agentëŠ” ì•„ë˜ componentsë¥¼ í•œê°œ ì´ìƒ í¬í•¨í•œë‹¤.\nPolicyëŠ” Agentê°€ ì–´ë–»ê²Œ Actionì„ ì„ íƒí•˜ëŠ”ì§€(=behaviour function) ì´ë‹¤.\nStateë¡œë¶€í„° function \\(\\pi\\)\r(policy)ë¥¼ ì´ìš©í•´ì„œë¥¼ í†µí•´ actionì„ ê²°ì •í•œë‹¤.\nDeterministic policy: \\( a = \\pi(s)\\)\rstateë¥¼ ë„£ìœ¼ë©´ ë‹¤ìŒ ì·¨í•  ì•¡ì…˜ì´ íŠ€ì–´ë‚˜ì˜¨ë‹¤ probabilityë¡œ policyë¥¼ í‘œí˜„í•˜ê³ ì í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\nStochastic policy: \\( \\pi(a \\mid s) = \\mathbb{P}[A_t = a \\mid S_t = s]\\)\rí˜„ì¬ ìƒíƒœ s ì— ìˆì„ ë•Œ, ì•¡ì…˜ a ë¥¼ ì„ íƒí•  í™•ë¥  Value Functionì€ Stateë‚˜ Actionì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€(ê¸°ëŒ€ë˜ëŠ” ë¯¸ë˜ì˜ rewardê°€ ì–¼ë§ˆì¼ì§€ ì˜ˆì¸¡) ì„ ë‚˜íƒ€ë‚¸ë‹¤.\nState oneê³¼ State two, action oneê³¼ action twoë¥¼ ì„ íƒí• ë•Œ ìµœì¢… rewardê°€ ë” ì¢‹ì€ìª½ìœ¼ë¡œ ì„ íƒí•œë‹¤.\nì•„ë˜ì™€ ê°™ì´ í‘œí˜„ë˜ëŠ”ë° \\( R_{t+1}, R_{t+2}, R_{t+3} \\)\rë¥¼ ë”í•˜ëŠ” ê²ƒê³¼ ê°™ì´ ë‹¤ìŒ(ë¯¸ë˜)ì˜ rewardì˜ í•©ì˜ ê¸°ëŒ€ê°’(ì–´ë–¤ í´ë¦¬ì‹œ \\(\\pi\\)\rë¥¼ ë”°ë¥´ëŠ” ê°€ì •í•˜ì—) \\[\rv_{\\pi}(s) = \\mathbb{E}_{\\pi} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s \\right]\r\\]\rgamma(\r\\( \\gamma\\)\r) ëŠ” ë‹¤ìŒ ìŠ¤íƒ­ì— ëŒ€í•œ discounting factor (ì§€ê¸ˆì€ ë¯¸ë˜ì˜ ê°’ì˜ ì˜í–¥ë„ ì •ë„ë¡œ ì´í•´í•˜ê³  ë„˜ì–´ê°€ì)\nModelì€ Agentê´€ì ì—ì„œ Environmentê°€ ì–´ë–»ê²Œ ë™ì‘í• ì§€ ìƒê°í•˜ëŠ” ê²ƒ ì„ ë‚˜íƒ€ë‚¸ë‹¤.\ntransitions model, rewards model ì „í†µì ìœ¼ë¡œ ë‘ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤\ntransitions ëª¨ë¸ì€ directly ë‹¤ìŒ stateë¥¼ ì˜ˆì¸¡í•œë‹¤. \\[\\mathcal{P}_{ss'}^a = \\mathbb{P}[S_{t+1} = s' \\mid S_t = s, A_t = a]\\]\rRewards ëª¨ë¸ì€ rewardë¥¼ ì˜ˆì¸¡í•œë‹¤. \\[\\mathcal{R}_s^a = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]\\]\rRL agentì˜ ë¶„ë¥˜\r#\rì–´ë–¤ key componentë¥¼ ê°€ì§€ê³  í•™ìŠµí•˜ê³  ìˆëŠ”ì§€ì— ë”°ë¼ì„œ RLì„ ë¶„ë¥˜í•œë‹¤\nvalue-based RLì€ value functionì„ optimalì´ ë˜ë„ë¡ í•œë‹¤. (ë¬µì‹œì ìœ¼ë¡œ policy ë¥¼ ê°€ì§€ê³  ìˆë‹¤.) policy-based RLì€ policyë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. actor-criticì€ value functionê³¼ policyë¥¼ ê°€ì§€ê³  ìˆë‹¤. Model baseë¡œ êµ¬ë¶„í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.\nmodel freeëŠ” modelì´ ì—†ì§€ë§Œ(=í™˜ê²½ì— ëŒ€í•œ representationì´ ì—†ì§€ë§Œ) value function and/or policyë¡œ êµ¬ì„±ëœ RL model basedëŠ” modelì´ ì¡´ì¬í•˜ê³ , value function and/or policy ë¡œ êµ¬ì„±ëœ RL Sequential decision makingì˜ ë‘ê°€ì§€ ë°©ì‹\r#\rReinforcement Learningì€ í™˜ê²½(Environment)ì„ ëª¨ë¥´ê³  ìƒí˜¸ì‘ìš©í•˜ë©´ì„œ rewardê°€ ìµœëŒ€ê°€ ë˜ë„ë¡ í•™ìŠµ í•˜ëŠ” ê²ƒ Planningì€ í™˜ê²½ì„ ì•Œê³ (ìš°ë¦¬ê°€ í™˜ê²½ì— í•´ë‹¹í•˜ëŠ” rule/modelì„ ì£¼ê³ ) agentê°€ ê³„ì‚°í•˜ëŠ” ê²ƒ Exploration and Exploitation\r#\rExploration ì™€ Exploitation ëŠ” trade-off\nExploration ì€ í™˜ê²½ì— ëŒ€í•œ ì •ë³´ë¥¼ ë” ì°¾ëŠ”ê²ƒ Exploitation ì€ ì•Œê³  ìˆëŠ” ì •ë³´ë¥¼ í™œìš©í•´ì„œ rewardë¥¼ ìµœëŒ€í™” í•˜ëŠ”ê²ƒ ì˜ˆ) ìƒˆë¡œìš´ ë ˆìŠ¤í† ë‘ ì°¾ê¸° vs ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ë ˆìŠ¤í† ë‘ ì¬ë°©ë¬¸\nPrediction and Control\r#\rRLì—ì„œëŠ” prediction problemê³¼ control problem ì´ ìˆë‹¤.\nprediction ì€ í˜„ì¬ policy ë¥¼ ë”°ë¥´ë©´ ì•ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë¯¸ë˜ì— ì¢‹ì„ì§€ í‰ê°€í•˜ëŠ”ê²ƒ contorl ì€ bset policy(=optimal policy)ë¥¼ ì°¾ëŠ”ê²ƒ ë¨¸ì‹ ëŸ¬ë‹(ML)ê³¼ ë”¥ëŸ¬ë‹(DL)ê³¼ì˜ ê´€ê³„\r#\rë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì¸ê³µì§€ëŠ¥(AI)ì˜ ê°œë…ìœ¼ë¡œì¨, í•™ìŠµì„ í†µí•´ ì˜ˆì¸¡(ë˜ëŠ” ë¶„ë¥˜)ë¥¼ í•˜ëŠ” ê²ƒ\në”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ê°œë…ìœ¼ë¡œì¨, ì¸ê³µì‹ ê²½ë§(Neural Network)ë¥¼ ì´ìš©í•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒ\nê°•í™” í•™ìŠµì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œê°ˆë˜ë¡œì¨, ë³´ìƒì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œ í–‰ë™ í•™ìŠµí•˜ëŠ” ê²ƒ\nAI\râ”œâ”€â”€ Machine Learning\râ”‚ â”œâ”€â”€ Supervised / Unsupervised\râ”‚ â”œâ”€â”€ Reinforcement Learning\râ”‚ â””â”€â”€ Deep Learning\râ”‚ â””â”€â”€ Deep Reinforcement Learning (e.g., DQN, PPO) ì°¸ê³ \r#\rDavid Silver ì˜ RL ê°•ì¢Œ https://davidstarsilver.wordpress.com/teaching\ní•œê¸€ ìœ íŠœë¸Œ + ë¸”ë¡œê·¸ https://smj990203.tistory.com/2\n"},{"id":6,"href":"/reinforcement-learning/rl-mdp/","title":"2. Markov Decision Process","section":"Reinforcement Learning / ê°•í™”í•™ìŠµ","content":"\r2. Markov Decision Process\r#\rMarkov Process(MP) ë€?\r#\rMP ì†ì„±\r#\rMDPëŠ” í™˜ê²½ì— ëŒ€í•´ì„œ Reinforcement Learningì´ ì´í•´ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì‹í™”í•œë‹¤\nê±°ì˜ ëª¨ë“  RL ê´€ë ¨ ë¬¸ì œë“¤ì€ MDPë¡œ ìˆ˜ì‹í™” í•  ìˆ˜ ìˆë‹¤(Fully observableì´ë‚˜ Partially observationì´ë‚˜) Markov Propertyë¥¼ ì´ìš©í•˜ëŠ”ë°, \u0026ndash;ì´ì „ ê°•ì˜ì°¸ì¡°\u0026ndash;\nìš”ì•½í•˜ë©´, í˜„ì¬ stateë§Œìœ¼ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•´ë„ ëœë‹¤ëŠ” ì†ì„±ì´ë‹¤. (ë‹¤ë¥´ê²Œ ë§í•˜ë©´, í˜„ì¬ stateê°€ ì´ë¯¸ ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤. = memoryless)\nMarkov state \\(s\\)\rë¡œë¶€í„° \\(s'\\)\rìœ¼ë¡œ ë³€ê²½í•˜ëŠ” transition probability ë¥¼ í•˜ëŠ” ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\r\\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s'\\mid S_t = s]\r\\]\rë§¤íŠ¸ë¦­ìŠ¤ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. (í•©ì€ 1) \\[\r\\mathcal{P} = \\left[\r\\begin{array}{ccc}\r\\mathcal{P}_{11} \u0026 \\cdots \u0026 \\mathcal{P}_{1n} \\\\\r\\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r\\mathcal{P}_{n1} \u0026 \\cdots \u0026 \\mathcal{P}_{nn}\r\\end{array}\r\\right]\r\\]\rMPëŠ” tupleë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.\nA Markov Process (or Markov Chain) is a tuple \\(\\langle \\mathcal{S}, \\mathcal{P} \\rangle \\)\r\\(\\mathcal{S}\\)\ris a (finite) set of states \\(\\mathcal{P}\\)\ris a state transition probability matrix,\n\\(\\mathcal{P}_{ss'} = \\mathbb{P} \\left[ S_{t+1} = s' \\mid S_t = s \\right]\\)\rì´ ì˜ˆì œëŠ” ë‹¨ìˆœí™”í•œ state ë³€í™”ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ê³ , MDPë¥¼ ì´ìš©í•œ ì‹¤ì œ ì‚¬ìš©ì€ í›¨ì‹ ë” ë§ì€ stateì™€ probabilityë¥¼ í¬í•¨í•œë‹¤.\nMarkov Reward Process(MRP) ë€?\r#\rMRP ì†ì„±\r#\rrewardê°€ ì¶”ê°€ê°€ ëœ ê²ƒ. MP ì— value judgmentê°€ í¬í•¨ëœ ê²ƒ - ì—¬ê¸°ì„œ judgment ëŠ” ëˆ„ì  rewardê°€ ì–¼ë§ˆë‚˜ ì¢‹ì•„ì§ˆì§€\nA Markov Rewards Process (or Markov Chain) is a tuple \\(\\langle \\mathcal{S}, \\mathcal{P}, \\textcolor{red}{\\mathcal{R}, \\gamma} \\rangle \\)\r\\(\\mathcal{S}\\)\ris a (finite) set of states \\(\\mathcal{P}\\)\ris a state transition probability matrix,\n\\(\\mathcal{P}_{ss'} = \\mathbb{P} \\left[ S_{t+1} = s' \\mid S_t = s \\right]\\)\r\\(\\mathcal{R}\\)\ris a reward function, \\(\\mathcal{R}_s = \\mathbb{E} \\left[ R_{t+1} \\mid S_t = s \\right]\\)\r\\(\\gamma\\)\ris a discount factor, \\({\\gamma \\in [0, 1]}\\)\rtimestep tì— ëŒ€í•œ goal ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤. ê°ë§ˆëŠ” ë¯¸ë˜ì— ëŒ€í•œ discount factorì´ë‹¤. ì´ê²ƒì´ í•„ìš”í•œ ì´ìœ ëŠ”\nìš°ë¦¬ëŠ” í¼íŒ©í•œ ëª¨ë¸ì´ ì—†ê¸° ë•Œë¬¸ì— ìˆ˜í•™ì  max ë°”ìš´ë“œ(ìˆ˜í•™ì  í¸ì˜ì„±ì„ ìœ„í•´) MPì˜ ë¬´í•œ ë£¨í”„ë¥¼ í•˜ì§€ í”¼í•˜ê¸° ìœ„í•´ ê·¼ì ‘ ë¯¸ë˜ì˜ ê°€ì¹˜ê°€ ë¹„ê·¼ì ‘(ë¨¼ë¯¸ë˜) ë¯¸ë˜ì˜ ê°€ì¹˜ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ ì‹œí€€ìŠ¤ì˜ ëì´ ë³´ì¥ëœë‹¤ë©´ discount factorë¥¼ ì•ˆì“¸ìˆ˜ë„ ìˆë‹¤ \\[\rG_t = R_{t+1} + \\gamma R_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\r\\]\rMPëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ í™•ë¥ ì— ì˜í•´ì„œ stateê°€ ë³€ê²½ë˜ëŠ” ê²ƒì´ê³ .\nMRP ì—ì„œëŠ” ê·¸ ë³€í™”ëœ ì‹œê°„ì—ì„œ stateì— ë„ë‹¬í• ë•Œë§ˆë‹¤ rewardê°€ íšë“ëœë‹¤ê³  ì´í•´í–ˆë‹¤.\nì´ë¡œì¨ í˜„ì¬ state ì—ì„œ ë°”ë¼ë³¸ë‹¤ë©´ ì•ìœ¼ë¡œ ë‚˜ì˜ ë¯¸ë˜ total rewardë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.(discount factor 0~1) Value Function\r#\rí˜„ì¬ìƒíƒœ(s)ì—ì„œì˜ terminated ìƒíƒœì—ì„œì˜ Expected return ì´ê²ƒì€ Expectation ì´ë‹¤. ì™œëƒí•˜ë©´ environmentëŠ” stochastic ì´ë‹ˆê¹Œ \\[\rv(s) = \\mathbb{E} [ G_t \\mid S_t = s ]\r\\]\rì´ëŠ” ë°¸ë§ë°©ì •ì‹ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.\nBellman Equation for MRP\r#\rValue Functionì€ í¬ê²Œ ë‘ê°€ì§€ ì»´í¬ë„ŒíŠ¸ë¡œ ë‚˜ëˆŒìˆ˜ ìˆë‹¤.\ní˜„ì¬ì˜ ë¦¬ì›Œë“œ \\( R_{t+1}\\)\rë‹¤ìŒê³„ìŠ¹ stateì˜ discounted ìƒíƒœ \\(\\gamma v(S_{t+1})\\)\r\\[\r\\begin{aligned}\rv(s) \u0026= \\mathbb{E} \\left[ G_t \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma \\left( R_{t+2} + \\gamma R_{t+3} + \\cdots \\right) \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma G_{t+1} \\mid S_t = s \\right] \\\\\r\u0026= \\mathbb{E} \\left[ \\textcolor{red}{R_{t+1} + \\gamma v(S_{t+1})} \\mid S_t = s \\right]\r\\end{aligned}\r\\]\r\\[\rv(s) = \\mathbb{E} \\left[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s \\right]\r\\]\r\\[\rv(s) = \\mathcal{R}_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'} v(s')\r\\]\rì´ë¥¼ ë²¡í„° ë§¤íŠ¸ë¦­ìŠ¤ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\\[\r\\begin{bmatrix}\rv(1) \\\\\r\\vdots \\\\\rv(n)\r\\end{bmatrix}\r=\r\\begin{bmatrix}\r\\mathcal{R}_1 \\\\\r\\vdots \\\\\r\\mathcal{R}_n\r\\end{bmatrix}\r+\r\\gamma\r\\begin{bmatrix}\r\\mathcal{P}_{11} \u0026 \\cdots \u0026 \\mathcal{P}_{1n} \\\\\r\\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r\\mathcal{P}_{n1} \u0026 \\cdots \u0026 \\mathcal{P}_{nn}\r\\end{bmatrix}\r\\begin{bmatrix}\rv(1) \\\\\r\\vdots \\\\\rv(n)\r\\end{bmatrix}\r\\]\rSolving the Bellman Equation\r#\rë²¨ë§Œ ë°©ì •ì‹ì€ linear equation ì´ì§€ë§Œ O(n^3) ë³µì¡ë„ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ì‘ì€ê²ƒë§Œ í’€ìˆ˜ ìˆë‹¤.\nLarge MRPë¥¼ í’€ê¸°ìœ„í•´ì„œ\nDynamic Programing ì´ë‚˜ Monte-Carlo evaluation ì´ë‚˜ Temporal-Difference Learningì´ ìˆë‹¤. Markov Decision Process(MDP) ë€?\r#\rA Markov Decision Process (or Markov Chain) is a tuple \\(\\langle \\mathcal{S}, \\textcolor{red}{\\mathcal{A}}, \\mathcal{P}, \\mathcal{R}, \\gamma \\rangle \\)\r\\(\\mathcal{S}\\)\ris a (finite) set of states \\(\\mathcal{A}\\)\ris a (finite) set of actions \\(\\mathcal{P}\\)\ris a state transition probability matrix,\n\\(\\mathcal{P}^a_{ss'} = \\mathbb{P} \\left[ S_{t+1} = s' \\mid S_t = s, A_t = \\textcolor{red}a \\right]\\)\r\\(\\mathcal{R}\\)\ris a reward function, \\(\\mathcal{R}^a_s = \\mathbb{E} \\left[ R_{t+1} \\mid S_t = s , A_t = \\textcolor{red}a \\right]\\)\r\\(\\gamma\\)\ris a discount factor, \\({\\gamma \\in [0, 1]}\\)\rìœ„ MP MRP ì˜ˆì œì™€ ë‹¤ë¥´ê²Œ actionì´ ì¶”ê°€ë¨. ê·¸ë¦¼ì—ì„œëŠ” ì˜ ì•ˆí‘œí˜„ë˜ì–´ìˆì§€ë§Œ, ì•¡ì…˜ì„ í•˜ë©´. ê·¸ ì•¡ì…˜ìœ¼ë¡œì¸í•´ íŠ¹ì • stateë¡œ ì „ì´ë˜ëŠ” ê²ƒì€ í™•ë¥ ì´ë‹¤.\në°‘ì— pubì— ê°€ëŠ”ê²ƒ ì•¡ì…˜ì„ ìˆ˜í–‰í•˜ë©´ í™•ë¥ ì ìœ¼ë¡œ class1, class2, class3ì— ë„ë‹¬í•œë‹¤. Policy\r#\rì •ì±…(Policy \\(\\pi\\)\r) ëŠ” ì£¼ì–´ì§„ stateì— ëŒ€í•œ actionì˜ ë¶„í¬ \\[\r\\pi(a \\mid s) = \\mathbb{P}[A_t = a \\mid S_t = s]\r\\]\rë§ˆí¬ë¡œí”„ ì†ì„±ì— ì˜í•´ì„œ í˜„ì¬ stateëŠ” rewardë¥¼ fully characterize í•œê²ƒì´ê¸° ë•Œë¬¸ì— ìœ„ ìˆ˜ì‹ì— rewardê°€ ì—†ë‹¤\nstate ì‹œí€€ìŠ¤(ìƒíƒœì „ì˜)ì— ëŒ€í•´ì„œ policyë¥¼ ë„£ìœ¼ë©´ Markov Processì´ê³ , state ì‹œí€€ì— Rewardë¥¼ ë„£ìœ¼ë©´ Markov Reward process ì´ë‹¤.\nMarkov Reward Process ì— ëŒ€í•´ì„œ MDP ìˆ˜ì‹ìœ¼ë¡œ (Actionìœ¼ë¡œë¶€í„°) ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤. (MDP ìˆ˜ì‹ìœ¼ë¡œ(policy-actionì´ í¬í•¨ëœ ë²„ì „ìœ¼ë¡œ) MPì™€ MRPë¥¼ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤)\n\\[\r\\mathcal{P}^\\pi_{s, s'} = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathcal{P}^{a}_{s s'} \\\\\r\\mathcal{R}^\\pi_s = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathcal{R}^a_s\r\\]\rì´ëŠ” ëª¨ë“  actionì— ê°ˆìˆ˜ ìˆëŠ” probë¥¼ averageë¡œ(\r\\(\\pi\\)\rëŠ” 0~1ì˜ ê°’ì´ë¯€ë¡œ) ì´í•´ë¥¼ ì‰½ê²Œí•˜ê¸° Pì™€ Rì„ í‘œí˜„í•œ ê²ƒ.\nMDPì— ëŒ€í•œ value functionì€ state-value í‘ì…˜ê³¼, action-value function ë‘ê°€ì§€ ë°©ì‹ì´ ìˆë‹¤.\nstate-value function\r#\rë‹¤ìŒê³¼ ê°™ê³  ì´ëŠ” í˜„ì¬ stateì¼ë•Œ \\(\\pi\\)\rpolicyë¥¼ ë”°ë¥¼ë•Œ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. (ì–¼ë§ˆë‚˜ Rewardë¥¼ ì–»ì„ì§€) \\[\rv_\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s \\right]\r\\]\rì—¬ê¸°ì„œ \\(\\mathbb{E}_\\pi\\)\rëŠ” ëª¨ë“  ìƒ˜í”Œì•¡ì…˜ì— ëŒ€í•œ expectation\naction-value (q) function\r#\rì´ë¥¼ action-value function \\(q_\\pi\\)\rë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” í˜„ì¬ stateì—ì„œ ì–´ë–¤actionì„ ì„ íƒí–ˆì„ë•Œ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. (ì–¼ë§ˆë‚˜ Rewardë¥¼ ì–»ì„ì§€) \\[\rq_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s, A_t = a \\right]\r\\]\rstate-value functionê³¼ action-value functionì˜ Bellman Expectation ë°©ì •ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤. \\[\rv_\\pi(s) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s \\right]\r\\]\r\\[\rq_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a \\right]\r\\]\rstate-value-function ì™€ action-value function ì¤‘ ì–´ë–¤ ê²ƒì„ ì¤‘ì ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ”ì§€ì— ë”°ë¥¸ í•™ìŠµë°©ë²•ì´ ë‹¬ë¼ì§€ëŠ” ê²ƒ ê°™ë‹¤.\nì™¼ìª½ì€ state-value functionê´€ì ì—ì„œì˜ ê·¸ë¦¼ê³¼ ìˆ˜ì‹í‘œí˜„ì´ê³ , ì˜¤ë¥¸ìª½ì€ action-value í‘ì…˜ê´€ì ì—ì„œ ìˆ˜ì‹ê³¼ í‘œí˜„ì´ë‹¤. action-value functionì— ëŒ€í•´ì„œëŠ”, actionì„ ì„ íƒí•¨ìœ¼ë¡œì¨ rewardë¥¼ í†µí•´ì„œ state-value functionìœ¼ë¡œ ë‹¤ì‹œ ë„˜ì–´ê°€ëŠ” ê²ƒì„ ë³¼ìˆ˜ ìˆë‹¤.\nì´ ë‘ê°œì˜ ê·¸ë˜í”„ë¥¼ í•©ì¹˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ì™¼ìª½ì€ state-value function ê´€ì ì—ì„œì˜ ìˆ˜ì‹ì´ê³ , ì˜¤ë¥¸ìª½ì€ action-value function ê´€ì ì—ì„œì˜ ìˆ˜ì‹ì´ë‹¤.\nì´ê²ƒë“¤ì€ ì•ì—ì„œ ì–¸ê¸‰í–ˆë˜ê²ƒì²˜ëŸ¼ ë‘ ê°€ì§€ Junkë¡œ ë‚˜ëˆŒìˆ˜ ìˆê³ , (ì´ë²ˆ Stepì—ì„œì˜ Rewardì™€ ë¯¸ë˜ì˜ value function ë¦¬í„´ê°’) ë‹¤ìŒ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì§„ë‹¤ (matric-form). ì¶”ê°€ë¡œ ëª¨ë“  MDPëŠ” MRPë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤. \\[\rv_\\pi = \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_\\pi\r\\]\r\\[\rv_\\pi = \\left( I - \\gamma \\mathcal{P}^\\pi \\right)^{-1} \\mathcal{R}^\\pi\r\\]\rìš°ë¦¬ëŠ” ì´ë¡œë¶€í„°(state-value, action-value functionìœ¼ë¡œë¶€í„°) Optimal Value Function ì„ ì°¾ëŠ”ë‹¤\nOptimal Value Function\r#\rMDPì—ì„œì˜ ìµœì  í–‰ë™ì„ ì°¾ëŠ” ë°©ë²•ì€ optimal state-value function \\(v_*(s)\\)\rë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì€ ëª¨ë“  policy ì— ëŒ€í•´ì„œ value functionì„ ìµœëŒ€í™” í•˜ëŠ”ê²ƒì´ë‹¤. (ì¥ê¸°ì ìœ¼ë¡œ ìµœëŒ€ ë³´ìƒì„ ì–»ê¸° ìœ„í•´ì„œ) optimal action-value function \\(q_*(s,a) \\)\rì˜ ê²½ìš° ì•„ë˜ì™€ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\\[\rv_*(s) = \\max_\\pi v_\\pi(s)\r\\]\r\\[\rq_*(s, a) = \\max_\\pi q_\\pi(s, a)\r\\]\roptimal policy (\r\\(\\pi_*\\)\r) ëŠ” \\(q_*\\)\rë¥¼ ìµœëŒ€í™” í•¨ìœ¼ë¡œì¨ ì–»ì„ ìˆ˜ ìˆë‹¤. \\[\r\\pi_*(a \\mid s) = \\begin{cases}\r1 \u0026 \\text{if } a = \\arg\\max\\limits_{a \\in \\mathcal{A}} q_*(s, a) \\\\\r0 \u0026 \\text{otherwise}\r\\end{cases}\r\\]\rì˜µí‹°ë©€í•œ í•´ëŠ” ìœ„ì— êµ¬í•œ ë„ì‹ê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì•„ë˜ ëª¨í˜•ê³¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\nSolving Bellman Optimality Equation\r#\rBellman Optimality Equationì€ non-linear í•˜ê³  ë³´í†µ No closed form ìœ¼ë¡œ ì œê³µë¨. ì•„ë˜ì™€ ê°™ì€ solving method ë“¤ì´ ìˆìŒ\nvalue iteration : Iteratively updates value estimates using the Bellman optimality equation until convergence. policy iteration : Alternates between policy evaluation and policy improvement until the policy becomes stable. Q-learning : Off-policy method that directly learns the optimal action-value function from experience. Sarsa(State-Action-Reward-State-Action) : On-policy method that updates action-values based on the action actually taken by the current policy. Extensions of MDPs\r#\rinfinite and continous MDPs ë¬´í•œí•œ ê¸°ì¡´ ë°©ë²•ì„ ê·¸ëŒ€ë¡œ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤(Straightforward). ì—°ì†ì ì¸ ìˆ«ìë¼ë©´ í¸ë¯¸ë¶„ í•´ì•¼í•œë‹¤. Partially observable MDPs (finite sets of Observation:O ì™€ Observation function:Z) ì¶”ê°€ìš”ì†Œê°€ ìˆìœ¼ë©°. ìƒíƒœë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ìœ¼ë‹ˆ ê´€ì¸¡ê°’ìœ¼ë¡œë¶€í„° ì¶”ì •í•˜ëŠ” hidden stageê°€ ìˆëŠ” MDPë¡œ í•´ê²° Undiscounted, average reward MDPs ergodic markvo process ë¡œ ì²˜ë¦¬í• ìˆ˜ ìˆë‹¤. ergodicì€ Recuurent: ê° stateëŠ” ë¬´í•œ ì‹œê°„ ë™ì•ˆì— ë°©ë¬¸, Aperiodic : ì–´ë–¤ ì£¼ê¸°ì„± ì—†ì´ ë°©ë¬¸ í•˜ëŠ” ì†ì„±ì„ ê°€ì§€ê³  ìˆë‹¤.ì´ê²ƒì€ average reward MDPì´ë‹¤ (discount ë˜ì§€ ì•Šìœ¼ë‹ˆ, í°ìˆ˜ì˜ ë²•ì¹™ì— ì˜í•´ì„œ). ë”°ëŸ¬ì„œ average bellman equation ì„ í’€ë©´ ëœë‹¤. ì°¸ì¡°\r#\rhttps://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-2-mdp.pdf\nhttps://www.youtube.com/watch?v=lfHX2hHRMVQ\nhttps://trivia-starage.tistory.com/280\n"},{"id":7,"href":"/reinforcement-learning/rl-dp/","title":"3. Planning by Dynamic Programming","section":"Reinforcement Learning / ê°•í™”í•™ìŠµ","content":"\r3. Planning by Dynamic Programming\r#\rMDPë¥¼ í‘¸ëŠ” ë°©ì‹ë“¤ì´ ì—¬ëŸ¬ ë°©ë²•ì´ ìˆë‹¤.\nPolicy evaluation, Policy iteration, Value iteration ë“±ì´ ìˆê³ , ì´ê²ƒë“¤ì€ í™˜ê²½ì„ ì •í™•í•˜ê²Œ ì•ˆë‹¤ë©´(=ëª¨ë¸ì„ ì•ˆë‹¤ë©´) DPê°€ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤\në¨¼ì € ê°„ëµí•˜ê²Œ ì–¸ê¸‰í•˜ìë©´\npolicy iteration : policyë¥¼ í‰ê°€í•˜ê³  iteration í•˜ë©´ì„œ ë°œì „í•´ë‚˜ê°€ëŠ” ë°©ì‹ê³¼ (policy evaluation + policy improvement) value iteration : value functionì„ iterationí•˜ë©´ì„œ ì˜µí‹°ë©€ì„ ì°¾ì•„ê°€ëŠ” ë°©ë²• ì´ ìˆë‹¤.\nì´ë²ˆ ì„¹ì…˜ì€ DPë¡œ known MDPë¥¼ í‘¸ëŠ” ë°©ë²•ì— ëŒ€í•œê²ƒì´ê³ , ì´ê²ƒì€ ê°•í™”í•™ìŠµì˜ flow ì™€ ìˆ˜ì‹ ê°„ì˜ ì´í•´ë¥¼ ìœ„í•œ ì„¹ì…˜ì´ë‹¤. 4ì¥ë¶€í„° unknown MDPë¥¼ í‘¸ëŠ” ë°©ë²•ì´ ê¸°ìˆ ë˜ì–´ ìˆë‹¤.\nDynamic Programming(DP) ì´ë€?\r#\rì •ì˜ : // ì´ë¶€ë¶„ì€ ìƒëµ\nMDPëŠ” DPë¡œ ë¬¸ì œë¥¼ í’€ê¸°ì— í•„ìš”í•œ ì¡°ê±´ë“¤ì„ ë§Œì¡±í•œë‹¤.\në²¨ë§Œ ë°©ì •ì‹(Bellman equation) ì€ ì¬ê·€ì  decomposition value function ì€ ê°’ì„ ì €ì¥í•˜ê³ , ì¬ì‚¬ìš©í•œë‹¤. full environment ì •ë³´ê°€ ì£¼ì–´ì§€ë©´ ì´ê²ƒì€ ê°•í™”í•™ìŠµì˜ ë¬¸ì œê°€ ì•„ë‹ˆë¼ planning problem(mdpë¥¼)ìœ¼ë¡œì¨ DPë¡œ í’€ìˆ˜ ìˆë‹¤. MDP planningì˜ ë‘ê°€ì§€ ë¬¸ì œê°€ ìˆë‹¤. (For prediction, For control)\nprediction problemì€ input MDP(or MRP)ì™€ policyê°€ ì£¼ì–´ì¡Œì„ë•Œ, ì´ê²ƒì˜ outputì¸ value function \\(v_\\pi\\)\rì„ êµ¬í•˜ëŠ”ê²ƒ\ncontrol problem ì€ ì˜µí‹°ë§ˆì´ì§• í•˜ëŠ”ê²ƒ(best policyì™€ ê·¸ì—ë”°ë¥¸ best value functionì„ êµ¬í•˜ëŠ”ê²ƒ). inputìœ¼ë¡œ MDPê°€ ì£¼ì–´ì§€ê³  outputìœ¼ë¡œ \\(v_*\\)\r(optimal value function) ë˜ëŠ” \\(\\pi_*\\)\r(optimal policy)\npolicy evaluation\r#\rpolicyì‹œê°€ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ í‰ê°€(MDPë¡œ ì–¼ë§ˆë‚˜ ì–¼ë§ˆë‚˜ ë§ì€ rewardë¥¼ ì–»ì„ìˆ˜ ìˆëŠ”ì§€?). policyë¥¼ ì—…ë°ì´íŠ¸í•˜ì§„ ì•ŠëŠ”ë‹¤\nbellman expectation equationì„ ì‚¬ìš©\nbellman expectation equationì„ í’€ê¸° ìœ„í•´ì„œ,\nì´í„°ë ˆì´ì…˜ë§ˆë‹¤ policyí•˜ì˜ value functionì„ í‰ê°€í•´ì„œ, value functionì„ ì—…ë°ì´íŠ¸ í•œë‹¤.\n\\(v1 \\rightarrow v2 \\rightarrow ... \\rightarrow v_\\pi\\)\rê°€ ë˜ì–´ true value function ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\nì´ê²ƒì€ ìš°ë¦¬ê°€ policy ì— ë”°ë¥¸(ê³ ì •ëœ policy) ì •í™•í•œ value functionì„ ëª¨ë¥´ë‹ˆ (optimal value functionì„ ë§í•˜ëŠ”ê²ƒì´ ì•„ë‹˜), ê·¸ê²ƒì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ iterativeí•˜ê²Œ ê³„ì‚°í•œë‹¤ëŠ” ì˜ë¯¸. ì—¬ê¸°ì—ì„œ iterativeëŠ” ê°•í™”í•™ìŠµì—ì„œ actioní•˜ê³  reward ë°›ëŠ” iterative(timestep) ê³¼ëŠ” ë‹¤ë¥¸ ì˜ë¯¸\rì´ë•Œ ë‘ê°€ì§€ ë°©ì‹ìœ¼ë¡œ backupì´ ê°€ëŠ¥í•˜ë‹¤. synchronous backup/asyncronous backup synchronous backup = í•œ iterationì—ì„œ ì „ì²´ ìƒíƒœë“¤ì˜ ê°’ì´ í•œêº¼ë²ˆì— ì—…ë°ì´íŠ¸ë¨ íŠ¸ë¦¬ì—ì„œ í˜„ì¬root ë…¸ë“œì— ìˆë‹¤ê³  ê°€ì •í•˜ë©´, ì·¨í•  ìˆ˜ ìˆëŠ” ëª¨ë“  actionì„ ê³ ë ¤í•˜ê³  ê°ˆìˆ˜ ìˆëŠ” ëª¨ë“  ê³„ìŠ¹ stateë„ ê³ ë ¤í•´ì„œ backup(ë˜ëŒì•„ê°€ì„œ) í˜„ì¬ ë…¸ë“œì— probability ì— ë”°ë¥¸ weightë¥¼ ë”í•œë‹¤. ì´ê²ƒì´ ê²°êµ­ í˜„ì¬ ë…¸ë“œì˜ ì´ë²ˆ ì´í„°ë ˆì´ì…˜ì˜ value function.\nì´ê²ƒì€ true value functionìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ”ê²ƒì„ ë³´ì¥í•œë‹¤ (why?= discount factor ê°€ 0~1 ì´ë¯€ë¡œ ìˆ˜ì¶• ë§¤í•‘ ì„±ì§ˆì„ ê°€ì§„ë‹¤ (contraction mapping) by GPT)\nsynchronous backup policy evaluation example\r#\rì•„ë˜ê·¸ë¦¼ì€ ì´ë™ì„ uniform randomí•˜ê¸° pickëœë‹¤ëŠ” policyì— ëŒ€í•œ ê·¸ë¦¼ì´ë‹¤ (ì™¼ìª½ì— ì íŒ ìˆ«ìê°’ë“¤ : 1/4ì”© ê°€ëŠ¥ì„±ì´ ìˆëŠ”ê²½ìš°) k=1 ì¼ë•Œì˜, ì£¼ë³€ ìœ¼ë¡œ ê°”ì„ë•Œ ì „ë¶€ -1 ì´ë‹ˆ -1*4/4 ë¡œ 1íšŒ ì—…ë°ì´íŠ¸ k=2 ì¼ë•Œì˜ 1.7 ì€ 1.75ê°€ ì§¤ë¦°ê²ƒ. [0,1] ì„ë³´ë©´ ë„¤ê³³ìœ¼ë¡œ ì´ë™í• ìˆ˜ ìˆê³  ë¶ìœ¼ë¡œê°€ë©´ ìê¸°ìì‹ ìœ¼ë¡œ ëŒì•„ì™€ì„œ ì´ë™-1, ì´ì „step(k=1ì—ì„œì˜) ìê°€ìì‹ ì˜ ê°’ -1 ì´ë¯€ë¡œ -2, ë™ìœ¼ë¡œê°€ë©´ ì´ë™-1 ê³¼ ì´ì „stepì˜ [0,2]ì˜ ê°’ -1 ì´ í•©ì³ì ¸ì„œ -2, ë§ˆì°¬ê°€ì§€ë¡œ ë‚¨ìœ¼ë¡œê°€ë©´ -2 ì„œë¡œ ê°€ë©´ ì´ë™ -1 ê³¼ ì´ì „stepì˜ [0,0]ì˜ ê°’ 0ì´ í•©ì³ì ¸ì„œ 0 ë”°ë¼ì„œ (0 -2 -2 -2) / 4 í•˜ë©´ [0,1] ì€ -1.75\nì´ê²ƒì„ ê³„ì†í•˜ë©´ ê°’ì´ ê³„ì† ì—…ë°ì´íŠ¸ê°€ ë˜ëŠ”ë° k=3ì¼ë•Œ ë²Œì¨ ìˆ˜ë ´í•œê²ƒì„ ë³¼ìˆ˜ ìˆë‹¤.\nì´ value functionì„ random policyê°€ ì•„ë‹ˆë¼ ê·¸ë¦¬ë””í•˜ê²Œ ì„ íƒí•˜ê²Œ í•˜ëŠ” policy í•˜ë©´ (ê°’ì´ í°ê²ƒì„ ì„ íƒí•˜ë„ë¡í•˜ë©´) ì˜¤ë¥¸ìª½ í™”ì‚´í‘œ ê·¸ë¦¼ê³¼ ê°™ì´ ë‚˜íƒ€ë‚œë‹¤.\nvalue functionì„ better í´ë¦¬ì‹œë¥¼ ì°¾ì•„ë‚´ëŠ”ë° ë„ìŒì„ ì¤€ë‹¤. í˜„ì¬ policyë¥¼ í‰ê°€í•˜ëŠ”ê²ƒë§Œìœ¼ë¡œë„ ìš°ë¦¬ëŠ” ë”ì¢‹ì€ ìƒˆë¡œìš´ í´ë¦¬ì‹œë¥¼ ë§Œë“¤ìˆ˜ ìˆë‹¤.\nasyncronous backup = ì „ì²´ ìƒíƒœë¥¼ í•œ ë²ˆì— ê°±ì‹ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì„ íƒëœ íŠ¹ì • ìƒíƒœì— ëŒ€í•´ì„œë§Œ value functionì„ ê°±ì‹ í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ëŠ” ê³„ì‚° íš¨ìœ¨ì„ ë†’ì´ê³ , ë¹ ë¥¸ ìˆ˜ë ´ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤. by GPT\npolicy iteration\r#\rpolicyë¥¼ inner loopì—ì„œ iterationë§ˆë‹¤ í‰ê°€í•˜ë©´ì„œ policyê°€ ë” ë‚˜ì•„ì§€ë„ë¡ ì ìš©í•´ë‚˜ê°€ëŠ” ë°©ì‹; ê²°êµ­ optimal policyë¥¼ ì°¾ê²Œ ëœë‹¤ëŠ” ì„¤ëª….\nì²«ë²ˆì§¸ ìŠ¤í…ìœ¼ë¡œ policy evaluation : policy \\(\\pi\\)\rë¥¼ í‰ê°€(evaluation) í•˜ë©´ value functionì´ ë‚˜ì˜¤ê³ , (í˜„ì¬ policyë¡œ ê°ìƒíƒœì˜ value ë¥¼ êµ¬í•˜ëŠ”ê²ƒ) \\[\rv_\\pi(s) = \\mathbb{E} \\left[ R_{t+1} + \\gamma R_{t+2} + \\dots \\mid S_t = s \\right]\r\\]\rë‘ë²ˆì§¸ ìŠ¤í…ìœ¼ë¡œ policy improvement: \\(\rv_\\pi(s)\\)\rê¸°ë°˜ìœ¼ë¡œ policy ë¥¼ greedily í–‰ë™í•˜ê²Œ improvement í•˜ë©´ ì´ê²ƒì´ ì—…ë°ì´íŠ¸ëœ policyë‹¤. \\[\r\\pi' = \\text{greedy}(v_\\pi)\r\\]\rì´ê²ƒì´ ê²°êµ­ optimal policyë‹¤. (value functionë„ ê²°êµ­ optimalí•œ ê²ƒìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤.)\nì¶”ê°€ì§ˆë¬¸:ì–¸ì œ ìˆ˜ë ´í–ˆëŠ”ì§€ëŠ” ì–´ë–»ê²Œ íŒŒì•…í•  ìˆ˜ ìˆëŠ”ì§€?\nì •ì±…ì´ ë”ì´ìƒ ë°”ë€Œì§€ ì•Šê±°ë‚˜, ì •ì±…ê°„ì˜ ì°¨ì´ê°€ ì•„ì£¼ ì‘ì„ë•Œ (10^-4) ìˆ˜ë ´í–ˆë‹¤ê³  íŒŒì•… by GTP DQN (2015)\tValidation í™˜ê²½ì—ì„œì˜ í‰ê·  rewardê°€ ë” ì´ìƒ ì¦ê°€í•˜ì§€ ì•Šì„ë•Œ PPO (2017)\tí‰ê·  rewardì˜ moving averageê°€ ì•ˆì •ë ë•Œ (ë³€í™”ëŸ‰ \u0026lt; threshold) ì´ê²ƒë“¤ì€ ê²°êµ­ Modified Policy Iteration. ëª…ì‹œì ìœ¼ë¡œ stopí•  iteration ìˆ«ì(k)ë¥¼ ì„¸íŒ…í•˜ê±°ë‚˜, ì…ì‹¤ë¡ -convergence of function ìœ¼ë¡œ stopping conditionì„ ë§Œë“¤ì–´ì•¼í•¨.\në§Œì•½ policyê°€ deterministic í•œ policy ì´ë¼ë©´, \\( a = \\pi(s)\\)\r. improvementê°€ ë©ˆì¶˜ë‹¤ë©´ ìˆ˜ì‹ì ìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤ëŠ”ê²ƒì„(Bellman Optimal Equationì„ ë§Œì¡±í•¨ì„) ì¦ëª…í•  ìˆ˜ ìˆì§€ë§Œ, ìƒëµí•¨. lecture-3 ì˜ 17page\nvalue interation\r#\rì´ê²ƒì€ MDPë¥¼ í‘¸ëŠ” ë˜ ë‹¤ë¥¸ ë°©ì‹ì´ë‹¤.\nbellman equationì„ í†µí•´ì„œ value functionì´ better í•˜ë„ë¡ í•˜ëŠ” ë°©ì‹ ì •ì±… ë°˜ë³µë³´ë‹¤ ê³„ì‚°ëŸ‰ì´ ì ê³ , ìˆ˜ë ´ ì†ë„ê°€ ë¹ ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì–´ë– í•œ optimal policyë„ ë‘ê°œì˜ ì»´í¬ë„ŒíŠ¸ë¡œ ë‚˜ë‰ ìˆ˜ ìˆë‹¤.\noptimal first action \\(A_*\\)\rê³„ìŠ¹ë˜ëŠ” state S\u0026rsquo; ì˜ optimal policy ì´ë¥¼ ë‹¤ì‹œë§í•˜ë©´, í˜„ì¬ìƒíƒœì—ì„œ ë‹¤ìŒí–‰ë™ (ì²«ë²ˆì§¸ action)ì´ optimalí•œ ê²ƒì„ ì„ íƒí•˜ë©´, ê·¸ë‹¤ìŒì€ ê³„ìŠ¹ state S\u0026rsquo; ì—ì„œ optimal policyë”°ë¥´ëŠ”ê²ƒ ì´ëŠ” Principle of Optimality ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\ní•œ policyì´ ì–´ëŠ í•œ ìƒíƒœì—ì„œ ìµœì ì´ë¼ë©´(Optimalì´ë¼ë©´), ê·¸ policyì´ ì•ìœ¼ë¡œ ê°ˆ ëª¨ë“  ê²½ë¡œì—ì„œë„ ê³„ì† ìµœì ì´ì–´ì•¼ í•œë‹¤.\nì´ëŠ” ì•„ë˜ ìˆ˜ì‹ì„ (value functionì„) ìµœëŒ€í™” í•˜ëŠ”ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤. \\[\rv_*(s) \\leftarrow \\max_{a \\in \\mathcal{A}} \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\, v_*(s')\r\\]\rì•„ë˜ê·¸ë¦¼ì€, ì–´ë–»ê²Œ flowê°€ ì§„í–‰ë˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ìœ„í•œ ì˜ˆì œì´ë‹¤\nsyncronous í•˜ê²Œ ì—…ë°ì´íŠ¸ê°€ ë˜ë‹ˆ ìˆ«ìê°€ ì±„ì›Œì ¸ ìˆë‹¤. ì´ë™ì‹œ rewardëŠ” -1 terminate state 0ì¸ìƒíƒœì—ì„œ ì¶œë°œ. ì¸ì ‘ ìŠ¬ë¡¯ì€ ì²«ë²ˆì§¸ iterì— -1 ëœë‹¤. ìì„¸í•œ flowëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. V_1ì—ì„œ V_2ë¡œ ê°„ë‹¤ë©´ ì¸ì ‘ í–‰ë ¬[0,1] ì€ ì£¼ë³€ ìê¸°ë¡œë¶€í„° í•œì¹¸ ì´ë™í•œê²½ìš°ê°€ -1 = 0(ì™¼ì¹¸ ê°’) + -1(ì´ë™) ì´ ìµœëŒ€ê°’ì´ë¯€ë¡œ ì·¨í•¨. (ë„¤ë°©í–¥ ëª¨ë‘)\n[0,2] ì˜ ê²½ìš°ë„ ë™ì¼: ì£¼ë³€ ìê¸°ë¡œë¶€í„° í•œì¹¸ ì´ë™í•œê²½ìš°ê°€ -1 = 0(ì™¼ì¹¸ ê°’) + -1(ì´ë™) ì´ ìµœëŒ€ê°’ì´ë¯€ë¡œ ì·¨í•¨. (ë„¤ë°©í–¥ ëª¨ë‘)\nV_2ì—ì„œ V_3ìœ¼ë¡œ ê°„ë‹¤ë©´ [0,1] ì˜ ê²½ìš° ì™¼ì¹¸ì€ -1 = 0(ì™¼ì¹¸ ê°’) + -1(ì´ë™) ë‚˜ë¨¸ì§€ ë°©í–¥ì€ -2 = + -1(ë™ë‚¨ë¶ ê°’) + -1(ì´ë™) ì´ë¯€ë¡œ ìµœëŒ€ê°’ -1ì„ ì·¨í•¨ [0,2] ì˜ ê²½ìš° : ì£¼ë³€ ìê¸°ë¡œë¶€í„° í•œì¹¸ ì´ë™í•œê²½ìš°ê°€ -2 = 1(ì™¼ì¹¸ ê°’) + -1(ì´ë™) ì´ ìµœëŒ€ê°’ì´ë¯€ë¡œ ì·¨í•¨. (ë„¤ë°©í–¥ ëª¨ë‘) ì‚¬ì‹¤ ìš°ë¦¬ê°€ ëª¨ë“  environmentê°€ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ëª…í™•í•˜ê²Œ ì•ˆë‹¤ë©´ ìœ„ì— ì˜ˆì œì˜ ê²½ìš°, goalë¡œ ë¶€í„° ì¸ì ‘ ê°’ë“¤ì„ ì±„ì›Œê°€ë©´ì„œ ê³„ì‚°í•˜ë©´ í’€ë¦¬ëŠ” ë¬¸ì œì´ë‹¤.\në§Œì•½ ìš°ë¦¬ê°€ syncronous dynamic programming ìœ¼ë¡œ í‘¼ë‹¤ë©´ ì´ê²Œ ì–¸ì œ í’€ë¦¬ëŠ”ì§€ ëª¨ë¥¸ë‹¤. (ëª¨ë“  state ê°’ì„ ì—…ë°ì´íŠ¸í•˜ê¸° ë•Œë¬¸ì— v_2ì—ì„œ [2,2] ì˜ ê²½ìš° -1 ë¡œ ì±„ì›Œì ¸ ìˆëŠ”ë°, ì´ê²Œ ì˜ì±„ì›Œì§„ê±´ê°€? ë¥¼ íŒë‹¨í•˜ì§€ ëª»í•¨)\nê²°êµ­ value iteration ì€ optimal policy \\(\\pi\\)\rë¥¼ ì°¾ëŠ”ê²ƒ\nê³„ì†í•´ì„œ value functionì„ ì—…ë°ì´íŠ¸í•˜ë©´ì„œ ìµœì ì„ ì°¾ì•„ê°€ê³  ìˆê¸° ë•Œë¬¸ì— ëª…ì‹œì ìœ¼ë¡œ policyë¥¼ ë§Œë“¤ì§€ ì•ŠëŠ”ë‹¤.\npolicy evalutionì€ bellman expectation equation ì„ í‘¸ëŠ”ê²ƒì´ê³ , (v_\\pi ë¥¼ ì°¾ëŠ”ê²ƒ ) value iteration ì€ bellman optimality equationì„ í‘¼ë‹¤. (v_* ë¥¼ ì°¾ëŠ”ê²ƒ)\rìˆ˜ì‹ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë˜ì–´ ìˆë‹¤.\n\\[\r\\mathbf{v}_{k+1} = \\max_{a \\in \\mathcal{A}} \\left( \\mathcal{R}^a + \\gamma \\mathcal{P}^a \\mathbf{v}_k \\right)\r\\]\rSynchronous Dynamic Programing\r#\rì•„ë˜ ë„í‘œì™€ ê°™ì´ ì²˜ë¦¬í•˜ë©´ ëœë‹¤ state-value functionì„ baseë¡œ \\( v_\\pi(s) \\)\rë‚˜ \\( v_*(s) \\)\rë¥¼ ì°¾ëŠ”ë‹¤ë©´ iterationë‹¹ \\( \\mathcal{O}(mn^2) \\)\rì‹œê°„ë³µì¡ë„ê³  m = actions, n = states action-value function ì„ ë² ì´ë¡œí•˜ë©´ \\( q_\\pi(s,a) \\)\rë‚˜ \\( q_*(s,a) \\)\rë¥¼ ì°¾ëŠ”ë‹¤ë©´ iterationë‹¹ \\( \\mathcal{O}(m^2n^2)\\)\rì‹œê°„ë³µì¡ë„ Asynchronous Dynamic Programing\r#\rìœ„ì˜ ì˜ˆì œëŠ” ëª¨ë“  stateë¥¼ ëª¨ë‘ ì—…ë°ì´íŠ¸ í•˜ëŠ”ë° ë‚­ë¹„ê°€ ì‹¬í•¨.\nì •ì˜: ëª¨ë“  ìƒíƒœë¥¼ ë™ì‹œì— ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šê³ , ì¼ë¶€ ìƒíƒœë§Œ ì„ íƒì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì¥ì : ê³„ì‚° íš¨ìœ¨ì„±ì´ ë†’ì•„ì§€ê³ , íŠ¹ì • ìƒíƒœì— ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nasynchronous dynamic programmingì„ í•˜ê¸°ìœ„í•œ 3ê°€ì§€ ideaë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤\nIn-place dynamic programming Prioritised sweeping Real-time dynamic programming ìì„¸í•œ ë‚´ìš©ì€ ìƒëµ\nFull-Width Backup ê³¼ Sample Backup\r#\rFull-width backupì€ ë„ˆë¬´ ë¹„ì‹¸ì„œ sample ê¸°ë°˜ backupì„ í•œë‹¤.\nsample : ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•´ì„œ ì–»ì€ í•œ ë²ˆì˜ ê²½í—˜ ë°ì´í„°\nì´ë¡œì¸í•œ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\nì´ë¡œì¸í•´ì„œ ê¶ê·¹ì ìœ¼ë¡œ Model-freeí•˜ê²Œ ëœë‹¤.( environment ëª¨ë¸ì„ ì•Œì§€ ì•Šì•„ë„ ë˜ê²Œ ëœë‹¤) ì°¨ì›ì˜ ì €ì£¼ í•´ì†Œ backup costê°€ ì¤„ì–´ë“¬ ìƒ˜í”Œì„ í†µí•´ì„œ Dynamic programingì—ì„œ model-free reinforcement learning ë¬¸ì œë¡œ ë³€í™˜í•˜ê²Œ ëœë‹¤.\nModel-based :\tí™˜ê²½ì˜ ë™ì‘ ë°©ì‹ì„ ì•Œê³  ìˆìŒ (í˜¹ì€ í•™ìŠµí•¨). ì´ë¥¼ ì‚¬ìš©í•´ planningì„ í•¨.\nModel-free\t: í™˜ê²½ì˜ ë™ì‘ ë°©ì‹ ì—†ì´, ì§ì ‘ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° í•™ìŠµí•¨. ì˜¤ì§ ê²½í—˜(transition, reward)ì—ë§Œ ì˜ì¡´.\në§¤ë²ˆ ìƒ˜í”Œì€ \u0026ldquo;ìš´ ì¢‹ì„ ìˆ˜ë„ ìˆê³  ì•„ë‹ ìˆ˜ë„\u0026rdquo; ìˆì§€ë§Œ, ì¶©ë¶„íˆ ë§ì´, ë°˜ë³µì ìœ¼ë¡œ, ê·¸ë¦¬ê³  ì˜ ì •í•´ì§„ ê·œì¹™ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë©´ ê²°êµ­ ê¸°ëŒ€ê°’ì— ìˆ˜ë ´ â†’ ìµœì ì— ìˆ˜ë ´. by GPT ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…ì´ ë˜ì—ˆë‹¤ê³ ë„ í•œë‹¤..\r"},{"id":8,"href":"/reinforcement-learning/rl-model-free-prediction/","title":"temp. 4. Model Free Prediction","section":"Reinforcement Learning / ê°•í™”í•™ìŠµ","content":"\r4. model-free prediction\r#\r// NOTE: ì´ í˜ì´ì§€ëŠ” ì„ì‹œë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\r3ì¥ DPì— ìˆëŠ”ê²ƒì²˜ëŸ¼, Model-free prediction í•˜ê³ , Model-free control í•˜ëŠ” ìˆœì„œë¡œ ì§„í–‰ëœë‹¤.\nepisode : ì—ì´ì „íŠ¸ê°€ ì‹œì‘ ìƒíƒœì—ì„œ í–‰ë™ì„ ì‹œì‘í•´ì„œ, ì–´ë–¤ ì¢…ë£Œ ì¡°ê±´(End state)ì— ë„ë‹¬í•  ë•Œê¹Œì§€ì˜ ì „ì²´ ê³¼ì •\nMCì™€ TDëŠ” major model-free algo.\nMonte Carlo (MC) : í•œ ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° í›„, ê·¸ ì „ì²´ ë¦¬í„´ ê°’ì„ ì´ìš©í•˜ì—¬ value functionì„ ì—…ë°ì´íŠ¸\nMCëŠ” í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œ ì „ì²´(ì‹œì‘ ~ ì¢…ë£Œ)ë¥¼ ê´€ì°°í•œ ë’¤, ì‹¤ì œë¡œ ë°›ì€ rewardë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. í™˜ê²½ ëª¨ë¸ ì—†ì´, ê²½í—˜ë§Œìœ¼ë¡œ value functionì´ë‚˜ policyë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. Monte-Carlo policy evaluation uses empirical mean return instead of expected return ë†’ì€ varianceì™€ zero biasë¥¼ ê°€ì§ ì¦‰, ê°€ì¥ ì†ì‰¬ìš´ ë°©ë²•ìœ¼ë¡œ epsideë¥¼ ëŒë ¤ë³´ê³  ê°€ëŠ¥ì„œë“¤ì˜ mean ê°’ìœ¼ë¡œ ì²˜ë¦¬ ë°©ë¬¸í• ë•Œë§ˆë‹¤ íšŸìˆ˜ì™€ í† íƒˆ returnì„ ëŠ˜ë¦¬ê³ , ì´ê²ƒì˜ í‰ê· ì„ í†µí•´ value functionì„ estimateí•œë‹¤. lecture-4, 7 page \\[\rV(S_t) \\leftarrow V(S_t) + \\frac{1}{N(S_t)} \\left(G_t - V(S_t)\\right)\r\\]\rBlackjack ì˜ˆì œì²˜ëŸ¼, í™•ë¥ ì´ë‚˜, ë¶„í¬ ê·¸ëŸ°ê²ƒ ì „í˜€ ì—†ì´ episode ë¡œ ë¶€í„° value functionì„ ë§Œë“¤ì–´ëƒˆë‹¤. (~500,000ë°˜ë³µí•˜ë©´ì„œ)\nTemporal Difference (TD) : ì—í”¼ì†Œë“œê°€ ëë‚˜ì§€ ì•Šì•„ë„, ë‹¤ìŒ ìƒíƒœì˜ í˜„ì¬ ì¶”ì • ê°’ì„ ì‚¬ìš©í•´ ë°”ë¡œ ì—…ë°ì´íŠ¸ incomplete episodes ë¥¼ bootstrapingì„ í†µí•´ì„œ ì—…ë°ì´íŠ¸ ì•„ì§ ì—í”¼ì†Œë“œê°€ ëë‚˜ì§€ ì•Šì•„ì„œ ë‚˜ë¨¸ì§€ ì˜ˆìƒë˜ëŠ” rewardë¥¼ í¬í•¨í•´ì„œ value functionì„ ì—…ë°ì´íŠ¸í•¨ ë”°ë¼ì„œ biasê°€ ìˆìŒ + ë‚®ì€ varianceë¥¼ ê°€ì§\nì´ê²ƒì€ Markov propertyë¥¼ í™œìš©í•œë‹¤. \\[\rV(S_t) \\leftarrow V(S_t) + \\alpha \\left( R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right)\r\\]\rTD(\r\\(\\lambda\\)\r) : ì—¬ëŸ¬ step + ê°€ì¤‘í•©ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ? MCì™€ TDì˜ ì¤‘ê°„\nMC \u0026lt;-\u0026gt; TDëŠ” ì „ì²´ ì—í”¼ì†Œë“œë¥¼ ë³´ëŠëƒ, ì¼ë¶€ë¶„ë§Œ ë³´ëŠëƒì˜ ì°¨ì´.\nTDì˜ stepì„ 0~n (nì´ ë˜ë©´ MCì™€ ê°™ìŒ) ì‚¬ì´ë¥¼ \\(\\lambda\\)\rë¡œ ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•´ ì‚¬ìš©í•˜ëŠ”ê²ƒ\nMonte-Carlo Reinforcement Learning ì€ model-free ì´ë‹¤. ì™œëƒí•˜ë©´ MDP Transition ì— ëŒ€í•œ (rewardì— ëŒ€í•œ) ì§€ì‹ì´ ì—†ê¸° ë•Œë¬¸. Temporal-Difference Learning ì€ model-free\n5. model-free control\r#\ron-policy/off-policy intro\r#\rÏ€ = Target Policy Âµ = Behavior Policy\nì´ë‘ê°œê°€ ê°™ìœ¼ë©´ on-policy ë‹¤ë¥´ë©´ off-policy.\nOff-policy learning : â€œLook over someoneâ€™s shoulderâ€ Learn about policy Ï€ from experience sampled from Âµ re-use experience generated from old policy Q-Learning : \\(\\varepsilon\\)\r-greedy ë°©ì‹ìœ¼ë¡œ íƒí—˜í•˜ì§€ë§Œ í•™ìŠµì—ëŠ” ë°˜ì˜ ì•ˆí• ìˆ˜ ìˆìŒ(ìµœì ì˜ í–‰ë™ë§Œ ì—…ë°ì´íŠ¸)\nOn-policy learning : â€œLearn on the jobâ€ Learn about policy Ï€ from experience sampled from Ï€\nSalsa :on-policy Q-learning. í˜„ì¬ í–‰ë™ì„ ê·¸ëŒ€ë¡œ ë”°ë¼ê°€ë©° í•™ìŠµ\non-policy\r#\rMonte-carlo iteration\r#\rMonte-Carlo ë°©ë²•ì„ í†µí•´ì„œ Policy Evaluationì€ ê°€ëŠ¥.(=Monte-Carlo Evaluation)\ngreedy í•˜ê²Œ policy improvementëŠ” action-value í‘ì…˜ì— ëŒ€í•´ì„œë§Œ ê°€ëŠ¥í•˜ë‹¤. state-value functionì„ í•˜ë ¤ë©´, ëª¨ë¸ì— ëŒ€í•´ ì•Œì•„ì•¼ë§Œ ê°€ëŠ¥í•˜ë‹¤.\n\\[\r\\pi'(s) = \\arg\\max_{a \\in \\mathcal{A}} \\left( \\mathcal{R}_s^a + \\sum_{s'} \\mathcal{P}_{ss'}^a V(s') \\right)\r\\]\rìœ„ì™€ ëŒ€ë¹„ë˜ê²Œ action-value function(Q í‘ì…˜) ì€ model-freeí•´ì„œ ì•Œìˆ˜ ìˆë‹¤.\n\\[\r\\pi'(s) = \\arg\\max_{a \\in \\mathcal{A}} Q(s, a)\r\\]\rì´ë ‡ê²Œ ì•Œê²Œëœ policyë¥¼ ì•„ë˜ \\(\\varepsilon\\)\r-greedy ë°©ì‹ìœ¼ë¡œ improvement í•œë‹¤.\nÎµ-greedy\r#\r\\(\\varepsilon\\)\r-greedy Algo\ní•­ìƒ ìµœê³ ì˜ í–‰ë™ë§Œ ê³ ë¥´ë©´ íƒí—˜ì´ ë¶€ì¡±í•˜ê³ , í•­ìƒ ë¬´ì‘ìœ„ë¡œ ê³ ë¥´ë©´ ì„±ëŠ¥ì´ ë‚®ë‹¤. â†’ ë‘˜ ì‚¬ì´ë¥¼ ì ì ˆíˆ ì„ì!\nì˜ˆì‹œ )\nÎµ=0.1 (10%) 90% í™•ë¥ ë¡œ í˜„ì¬ ìµœì ì˜ í–‰ë™ 10% í™•ë¥ ë¡œ ëœë¤ í–‰ë™ ì´ê²ƒì€ ìˆ˜í•™ì ìœ¼ë¡œ policyê°€ ì ì°¨ ì¢‹ì•„ì§€ëŠ”ê²ƒì„ ë‚˜íƒ€ë‚´ê³  ìˆ˜ë ´í•œë‹¤ëŠ” ê³„ì‚° ì¦ëª…ì´ ê°€ëŠ¥í•˜ë‹¤\nMonte-carlo Control\r#\rMonde-Carlo Policy iteration ì€ ì´ì „ ì„¹ì…˜ì—ì„œ ì„¤ëª…í•œê²ƒ\nMonde-Carlo Control ì€ í•˜ë‚˜ì”©ì˜ episode ê°€ ëë‚œí›„ì— policyë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ”ê²ƒ (episode ë‹¨ìœ„ë¡œ policy improvement)\nì´ë ‡ê²Œ í•´ë„ ë˜ëŠ” ì´ìœ ëŠ”(ìˆ˜ë ´í•˜ëŠ”ì´ìœ ëŠ”) Greedy in the Limit with Infinite Exploration ì„±ì§ˆì„ ë§Œì¡±í•˜ê¸° ë•Œë¬¸ì´ë‹¤..\n(policyë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì´ë¯¸ ê°€ì§€ê³  ìˆë‹¤ ë¼ê³  ë³¼ìˆ˜ë„ ìˆë‹¤.) [í™”ì‚´í‘œ ê·¸ë¦¼ ì¶”ê°€ì˜ˆì •]\nGLIE ì„±ì§ˆì— ëŒ€í•œ ì„¤ëª…ì€ ì¶”í›„ ì—…ë°ì´íŠ¸.\nSarsa\r#\rMC Controlì€ episodeê°€ ëë‚ ë•Œë§ˆë‹¤ ì •ì±…ì„ ê°œì„ í•˜ëŠ”ë° SalsaëŠ” ë§¤ stepë§ˆë‹¤ ì •ì±…ì„ ê°œì„ .\noff-policy\r#\rbehaviour policy Âµë¥¼ í†µí•´ì„œ ìˆ˜ì§‘í•˜ê³ , target policy Ï€ ë¥¼ í•™ìŠµí•˜ëŠ”ê²ƒ ì´ê²ƒì€ ë‹¤ë¥¸ ë¶„í¬ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ì„±ì§ˆì„ ì´ìš© \\[\r\\mathbb{E}_{X \\sim P}[f(X)]\r= \\sum P(X) f(X)\r= \\sum Q(X) \\frac{P(X)}{Q(X)} f(X)\r= \\mathbb{E}_{X \\sim Q} \\left[ \\frac{P(X)}{Q(X)} f(X) \\right]\r\\]\rì´ê²ƒì€ ë‹¤ìŒê³¼ ê°™ì´ value function ê³„ì‚°ì— ì£¼ì…ëœë‹¤. \\[\rG_t^{\\pi / \\mu} =\r\\frac{\\pi(A_t \\mid S_t)}{\\mu(A_t \\mid S_t)}\r\\frac{\\pi(A_{t+1} \\mid S_{t+1})}{\\mu(A_{t+1} \\mid S_{t+1})}\r\\cdots\r\\frac{\\pi(A_T \\mid S_T)}{\\mu(A_T \\mid S_T)}\rG_t\r\\]\r\\[\rV(S_t) \\leftarrow V(S_t) + \\alpha \\left( G_t^{\\pi / \\mu} - V(S_t) \\right)\r\\]\rQ-learning\r#\raction-value Q(s,a) ì˜ off-policy learning ë‹¤ìŒ actionì„ ì„ íƒí• ë•Œ behaviour policyë¡œë¶€í„° ê³ ë¥´ê³ , \\(A_{t+1} \\sim \\mu(\\cdot \\mid S_t)\\)\rí•™ìŠµì€ target policy ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰. \\(A' \\sim \\pi(\\cdot \\mid S_t)\\)\rìœ„ì™€ ê°™ì´ ì§„í–‰í•´ë„, Q-learingì€ ê²°êµ­ì—ëŠ” ì˜µí‹°ë©€í•œ action-value (q) functionì— ìˆ˜ë ´í•œë‹¤ëŠ” ì„±ì§ˆì„ ì´ìš©í•œê²ƒ. ë°‘ì—ëŠ” improvement í•˜ëŠ” ìˆ˜ì‹. \\[\rQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left( R_{t+1} + \\gamma Q(S_{t+1}, A') - Q(S_t, A_t) \\right)\r\\]\rì´ë ‡ê²Œ í•˜ë©´ ë³µì¡í•œ weight ê³„ì‚°ì‹ ì„ í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.\n[ë§ˆì§€ë§‰ ë©ì—… í‘œ ì¶”ê°€ ì˜ˆì •]\n6. value function approximation\r#\rlarge MDPë¥¼ í’€ìˆ˜ ì—†ìœ¼ë‹ˆ (too many state and action) value function(state-value/action-value function)ì„ ì–´ë–»ê²Œ ê·¼ì‚¬í•˜ê²Œ êµ¬í•˜ëŠ”ê°€?\nLiner combinations of feature Neural network ì—¬ê¸°ì„œë¶€í„° Gradient Descent ê°€ ë‚˜ì˜¨ë‹¤.\nì •ì±… Ï€ ê³ ì • Q-functionì„ gradient descentë¡œ ê·¼ì‚¬ (policy evaluation) ê·¼ì‚¬ëœ Q ê¸°ë°˜ìœ¼ë¡œ ì •ì±… ê°œì„  (policy improvement) ë‹¤ì‹œ ë°˜ë³µ (â‡’ ì ì§„ì ìœ¼ë¡œ ìµœì  ì •ì±…ì— ìˆ˜ë ´) value functionì„ ê·¼ì‚¬í•´ì„œ ì‚¬ìš©í•˜ë¯€ë¡œ value-based ë¼ê³ ë„ í•œë‹¤.\nAction-value function Approximation\r#\rì•„ë˜ì™€ ê°™ì´ action value functionì€ approximateë¥¼ í†µí•´ì„œ í‘œí˜„ë ìˆ˜ ìˆê³ . ë¸íƒ€ Wë¥¼ ì‘ê²Œí•˜ë¯€ë¡œì¨ ê·¼ì‚¬ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. \\[\rJ(\\mathbf{w}) = \\mathbb{E}_{\\pi} \\left[ \\left( q_{\\pi}(S, A) - \\hat{q}(S, A, \\mathbf{w}) \\right)^2 \\right]\r\\]\r\\[\r- \\frac{1}{2} \\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\left( q_{\\pi}(S, A) - \\hat{q}(S, A, \\mathbf{w}) \\right) \\nabla_{\\mathbf{w}} \\hat{q}(S, A, \\mathbf{w})\r\\]\r\\[\r\\Delta \\mathbf{w} = \\alpha \\left( q_{\\pi}(S, A) - \\hat{q}(S, A, \\mathbf{w}) \\right) \\nabla_{\\mathbf{w}} \\hat{q}(S, A, \\mathbf{w})\r\\]\rBatch Method\r#\rìœ„ì˜ gradient descent í• ë•Œ samplingì„ íš¨ìœ¨ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•œ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ë“¤\nExperience Replay\r#\rê³¼ê±°ì˜ transitionë“¤ì„ ë²„ë¦¬ì§€ ì•Šê³  bufferì— ì €ì¥í•´ ë‘ì—ˆë‹¤ê°€, í•™ìŠµí•  ë•Œë§ˆë‹¤ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•´ì„œ ì‚¬ìš©\nPrioritized Experience Replay (PER)\r#\rëª¨ë“  transitionì„ ê· ë“±í•˜ê²Œ ìƒ˜í”Œí•˜ì§€ ì•Šê³ , TD-errorê°€ í° transitionì— ë” ë†’ì€ í™•ë¥ ì„ ë¶€ì—¬í•˜ì—¬ í•™ìŠµ\nì§ê´€: TD-errorê°€ í´ìˆ˜ë¡ ë” í•™ìŠµì´ í•„ìš”í•œ \u0026ldquo;ì¤‘ìš”í•œ\u0026rdquo; ê²½í—˜\n7. policy gradient Method\r#\rì´ì „ì„¹ì…˜ì—ì„œëŠ” action-value function(Q í‘ì…˜)ì„ ê·¼ì‚¬í•´ì„œ ì˜µí‹°ë©€í•œ í´ë¦¬ì‹œë¥¼ ì°¾ì•„ê°”ë‹¤ë©´, policy gradientëŠ” Q ì—†ì´ policy parameterë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸ í•œë‹¤. policy-based\nSoftmax Policy : ì´ì‚°í–‰ë™ì¼ë•Œ, softmax over logitsìœ¼ë¡œ ì •ì±…ì„ ì •í•˜ê³ , policyì— ëŒ€í•œ gradient ê°’ ê³„ì‚°\nì´ë¥¼ í†µí•´ì„œ í˜„ì¬ iteration ì— ëŒ€í•œ policy íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. Gaussian Policy : ì—°ì†í–‰ë™ì¼ë•Œ, ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ì •ì±…ìœ¼ë¡œ ì •í•˜ê³ , policyì— ëŒ€í•œ gradient ê°’ ê³„ì‚°\nì´ë¥¼ í†µí•´ì„œ í˜„ì¬ iteration ì— ëŒ€í•œ policy íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. policy-based ì—ì„œë„ replay bufferë¥¼ ì“¸ìˆ˜ ìˆëŠ”ê°€?\nPolicy-basedì—ì„œ Replay Bufferì˜ ì–´ë ¤ì›€ ë¬¸ì œ: Policyê°€ ê³„ì† ë³€í•˜ê¸° ë•Œë¬¸ì— â€‹\nPPO, TRPO [\tâŒ ë˜ëŠ” ì œí•œì  ì‚¬ìš© ]\tìµœê·¼ì˜ ë°ì´í„°ë§Œ ì‚¬ìš© (very short buffer)\nSAC (Soft Actor-Critic) [ í™•ë¥ ì  policy ì‚¬ìš© ] (policy gradient ê¸°ë°˜) í•˜ì§€ë§Œ ì „ì²´ êµ¬ì¡°ëŠ” off-policy ë¡œì¨ replay buffer ì ê·¹ ì‚¬ìš©\nby GPT\nì¶”ê°€ research ìš”ë§\nActor-Critic\r#\rpolicy-based + value-based\nActorëŠ” í–‰ë™ì„ ìƒì„±í•˜ê³  Criticì€ ê·¸ í–‰ë™ì˜ \u0026ldquo;ì¢‹ìŒ\u0026quot;ì„ í‰ê°€í•´ì„œ Advantageë¥¼ ì¶”ì • ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Actorì˜ policy gradientë¥¼ ê³„ì‚° lacture-7 24page\nê°„ë‹¨í•œ actor-critic êµ¬ì¡°ëŠ” actuion-valueë¥¼ critic í•˜ëŠ”ê²ƒì´ë‹¤. Criticì€ TD(0)ì„ í†µí•´ì„œ Q-Functionì„ í•™ìŠµí•˜ê³  ActorëŠ” ìœ„ í•™ìŠµëœ Q-functionì„ ê¸°ë°˜ìœ¼ë¡œ policy gradientë¥¼ ìˆ˜í–‰í•˜ëŠ”ê²ƒì´ë‹¤.\nProximal Policy Optimization (PPO)\r#\rActor-Critic êµ¬ì¡°ì—ì„œ Clipped Objectiveë¥¼ ë„ì…í•´ì„œ, policyê°€ ë„ˆë¬´ í¬ê²Œ ë°”ë€Œì§€ ì•Šë„ë¡ ì œì•½í•˜ëŠ”ê²ƒ.\nì•„ë˜ëŠ” PPO surrogate objective í•¨ìˆ˜(ëª©ì í•¨ìˆ˜). ì´ë¥¼ gradient ascent(ìµœëŒ€í™”) í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ í˜„ì¬ policyì— ì—…ë°ì´íŠ¸í•˜ë©´ policyëŠ” ì˜µí‹°ë©€ì„ í–¥í•´ê°„ë‹¤. \\[\rL^{\\text{CLIP}}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t,\\; \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]\r\\]\r8. Integration Learning and Planning\r#\rModel-based RL\r#\r[ìƒëµ]\nSimulation-based Search\r#\r\u0026ldquo;ì „ì²´ ìƒíƒœ ê³µê°„ì„ ì§ì ‘ í•™ìŠµí•˜ê¸°ëŠ” ë„ˆë¬´ ë¹„ì‹¸ë‹¤. ëŒ€ì‹ , ìœ ë§í•œ ë¶€ë¶„ë§Œ ì§‘ì¤‘ì ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•˜ë©´ì„œ ê±°ê¸°ì„œ ê²½í—˜í•œ ì •ë³´ë¡œ Qê°’ì„ ì ì  ë” ì •í™•í•˜ê²Œ ë§Œë“ ë‹¤.\u0026rdquo;\n//note: balsa simulation learning ê³¼ëŠ” ê°œë…ì´ ì¡°ê¸ˆ ë‹¤ë¦„\n[ë¯¸ì™„]\n9. Exploration and Exploitation\r#\rExploitation : Make the best decision given current information Exploration : Gather more information ë„ˆë¬´ íƒí—˜ë§Œ í•˜ë©´: ì„±ëŠ¥ì´ ë‚®ì€ í–‰ë™ë„ ê³„ì† ì‹œë„ â†’ í•™ìŠµì€ ëŠë¦¬ê³ , ë³´ìƒì€ ë‚®ìŒ ë„ˆë¬´ ì´ìš©ë§Œ í•˜ë©´: ë” ë‚˜ì€ í–‰ë™ì„ ì•„ì˜ˆ ì‹œë„í•˜ì§€ ì•ŠìŒ â†’ **ì§€ì—­ ìµœì í•´(local optimum)**ì— ê°‡í˜ â†’ ë”°ë¼ì„œ, ë‹¨ê¸° ë³´ìƒ vs ì¥ê¸° í•™ìŠµ ì‚¬ì´ì˜ ê· í˜•ì„ ì¡ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” ì—¬ëŸ¬ ë°©ë²•ë“¤ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤ [ë¯¸ì™„]\n10. Case Study: RL in Classic Games\r#\r[ë¯¸ì™„]\n999. Deep RL\r#\rRLê³¼ ë”¥ëŸ¬ë‹ì˜ ê²°í•© ë”¥ëŸ¬ë‹ì´ ì‚¬ìš©í•˜ëŠ” ìœ„ì¹˜\nPolicy Network : í˜„ì¬ ìƒíƒœì—ì„œ í–‰ë™ ë¶„í¬ë¥¼ ì¶œë ¥ Value Network : ìƒíƒœë‚˜ ìƒíƒœ-í–‰ë™ì˜ ê°€ì¹˜ë¥¼ ì¶œë ¥ Q-network : Q(s, a)ë¥¼ ì§ì ‘ ì¶”ì • Model Network : í™˜ê²½ dynamics (transition, reward)ë¥¼ ì˜ˆì¸¡ (model-based RLì—ì„œë§Œ ì‚¬ìš©) a. Balsa\r#\rBalsaëŠ” ì¿¼ë¦¬ í”Œëœì„ ìˆœì°¨ì ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” ë¬¸ì œë¥¼ Markov Decision Process (MDP) ìœ¼ë¡œ ë³´ê³ , ì´ë¥¼ ê°•í™”í•™ìŠµìœ¼ë¡œ í•´ê²°\nState s = í˜„ì¬ê¹Œì§€ ë§Œë“¤ì–´ì§„ partial query plan Action a = ë‹¤ìŒì— ì–´ë–¤ í…Œì´ë¸”ì„ ì¡°ì¸í• ì§€ ê²°ì • Reward r = ì¿¼ë¦¬ í”Œëœì˜ ì‹¤í–‰ ë¹„ìš© ë˜ëŠ” latency Environment = DB ì¿¼ë¦¬ ì‹œë®¬ë ˆì´í„° or Costmodel ì¶”ê°€ì ìœ¼ë¡œ\nsimulation phase(step) ì„ ê°€ì ¸ì„œ ì¬ì•™ì  planì„ íƒí—˜í•˜ì§€ ì•Šê²Œí•˜ê³ , Timeoutì„ ë‘¬ì„œ Safe Execution ì‹œê°„ì„ ë³´ì¥í–ˆë‹¤. (ì¬ì•™ì  planì´ ì„ íƒë˜ë”ë¼ë„ timeoutìœ¼ë¡œ í•˜í•œë³´ì¥) value networkë¥¼ simple tree convolution networks ë¡œ êµ¬ì„± # ì—¬ê¸°ì—ì„œ ëª¨ë¸ì€ ê°•í™”í•™ìŠµì˜ environmentì˜ ëª¨ë¸ì´ ì•„ë‹ˆë¼, value functionì„ ê·¼ì‚¬í• (ê³„ì‚°í• ) treeconv ëª¨ë¸ì„ ì˜ë¯¸í•¨ def MakeModel(p, exp, dataset): dev = GetDevice() num_label_bins = int( dataset.costs.max().item()) + 2 # +1 for 0, +1 for ceil(max cost). query_feat_size = len(exp.query_featurizer(exp.nodes[0])) batch = exp.featurizer(exp.nodes[0]) assert batch.ndim == 1 plan_feat_size = batch.shape[0] if p.tree_conv: labels = num_label_bins if p.cross_entropy else 1 return TreeConvolution(feature_size=query_feat_size, plan_size=plan_feat_size, label_size=labels, version=p.tree_conv_version).to(dev) b. LOGGER\r#\re-beam search ì†Œê°œ [Exploration and exploitation] loss function reward weightingì„ í†µí•´ì„œ poor operatorì— ì˜í•œ fluctuation ë°©ì§€ log transformation ì„ í†µí•´ì„œ rewardì˜ ë²”ìœ„ë¥¼ ì••ì¶• (ì¬ì•™ì  planì˜ ì˜í–¥ë„ë¥¼ ê°ì‡„) ROSS Restricted Operator Search Space. (ìµœì ì„ ì°¾ì§€ ì•Šê³  ìµœì•…ì´ ì•ˆê³¨ë¼ì§€ê²Œ í•´ì„œ íš¨ìœ¨ì ) policy nertwork (GNN + LSTM) c. RELOAD\r#\rBalsa + MAML + PER\nModel-Agnostic Meta-Learning(MAML)\r#\rëª¨ë“  taskì— ì˜ ì‘ë™í•˜ëŠ” í•˜ë‚˜ì˜ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒ\u0026quot;ì´ ì•„ë‹ˆë¼, ì¡°ê¸ˆë§Œ fine-tuning í•˜ë©´ ê° taskì— ì˜ ì‘ë™í•  ìˆ˜ ìˆëŠ” ì´ˆê¸° ëª¨ë¸\u0026quot;ì„ í•™ìŠµí•˜ëŠ” ê²ƒ.\nPER\r#\rìœ„ì— ì–¸ê¸‰ [ ìƒëµ ]\nì°¸ì¡°\r#\rhttps://davidstarsilver.wordpress.com/teaching/\nhttps://wnthqmffhrm.tistory.com/10\nhttps://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-5-model-free-control-.pdf\n"}]