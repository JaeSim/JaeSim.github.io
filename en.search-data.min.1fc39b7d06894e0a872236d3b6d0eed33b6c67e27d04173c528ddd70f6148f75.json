[{"id":0,"href":"/development/secondpost/","title":"두번째 글입니다. 블로그 실험용 테스트 글입니다.","section":"Development / 개발","content":"\rtest\r#\rtest2\r#\rRL 관련 항목을 기록하기 위한 post 입니다.\n페이지 업로드 확인용 임시 포스트\n"},{"id":1,"href":"/aboutme/","title":"About Me","section":"Home","content":"\rProfile\r#\rName : Jaeyoung Sim\n"},{"id":2,"href":"/reinforcement-learning/rl-essential/","title":"1. Reinforcement Learning Essential","section":"Reinforcement Learning / 강화학습","content":"\r강화학습에 대한 기초 내용\r#\r들어가기 앞서\r#\r강화학습의 기초적인 내용을 학습한 뒤 정리한 것으로, 남들에게 보여주기 보다는 본인의 이해와 기억을 위해서 기술한 것입니다.\n나만의 방식으로 이해한 것이기 때문에, 주요하다고 생각하는 부분이 다를수 있으며 생략되거나 놓친 부분이 많이 있습니다.\n강화학습(RL: Reinforcement Learning) 이란?\r#\rDefinition\r#\r“Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal.”\n— Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction (2nd ed), p.1\nsituations을 state로 표기하여\n통상적으로 action, state와 reward 가 RL의 핵심 요소이다.\n요약하면, 주어진 상태(State)에서 보상(Reward)을 최대화 할 수 있는 행동(Action)을 학습하는 것\n이는 Reward Hypothesis를 기반으로한다.\nReward Hypothesis\nAll goals can be described by the maximisation of expected cumulative reward\nRewards는 Scalar feedback signal이다 Agent는 미래에 기대되는 cumulative reward가 최대화 되는 방향으로 학습 한다\nState and MDP\r#\rEnvrionment State, Agent State 가 있고 각각 Envrionment, Agent관점에서의 수식적/내부적 표현이다.\nEnvrionment State는 Envrionment 관점에서 다음 step에서 Envrionment가 어떻게 변화할지를(어떤 State로 변화할지) 나타낸다. Envrionment에서는 microsecond 에서도 수많은 정보가 오기 때문에 불필요한 정보들도 많다. Envrionment State는 우리의 알고리즘을 만드는데 유용하진 않다. 왜냐하면 Agent의 정보를 포함하고 있지 않기 때문. (우리의 알고리즘은 Agent에 있을테니 라고 이해함)\nAgent State는 다음 step 에서 Agent가 어떤 행동을 선택할지를 나타낸 수식/표현이다.\nInformation State는 과거 history부터 모든 유용한 정보를 포함한 수학적 정의를 가진 State이다. 주로 Markov State라 부른다. (Markov 속성을 만족한다)\nMarkov Properties : 이전의 모든 스테이트정보를 이용해서 다음 State를 선택하는것이, 현재State만 보고 하는것과 같다 \\[\r\\mathbb{P}[S_{t+1} \\mid S_t] = \\mathbb{P}[S_{t+1} \\mid S_1, \\dots, S_t]\r\\]\r이를 이용하면, 미래(future) 는 과거에 무엇이 주어졌든지 독립적이다. (=The future is independent of the past given the present) 달리말하면, 현재 \\(S_t\\)\r만 저장해도 된다 \\[\rH_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\r\\]\r현재의 State가 충분한 정보를 이미 담고 있다고도 볼 수 잇다.\nAppendix\rhistory 는 Observation과 actions, rewards의 연속이다 \\[\r\\quad \\quad H_t = O_1, R_1, A_1, ..., A_{t−1}, O_t, R_t\r\\]\rState 는 다음 action을 결정하기 위한 정보이다 \\[\r\\quad \\quad S_t = f(H_t)\r\\\\\r\\]\r이전 state는 \\[\r\\mathbb{P}[S_{t+1} \\mid S_t] = \\mathbb{P}[S_{t+1} \\mid S_1, \\dots, S_t]\r\\]\rThe future is independent of the past given the present 모든 이전 State를 알지 않아도 직전 State만 보고 결정 할 수 있다 \\[\rH_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\r\\]\rFully Observable Envrionments 는 Agent가 envrionment에서 어떻게 동작하는지 바로 관측이 가능함을 나타내고, 결과적으로 Envrionment State = Information State = Agent State 상태이다. 이를 Markov Desicion Process(MDP) 라고 한다\nPartial Observabable Envrionments 는 좀더 현실적인 환경. 로봇이 카메라를 통해서 화면을 보지만 현재 자기의 위치를 모르는 것처럼. 즉, Agent State \\( \\ne \\)\rEnvironment State 이다. Partially Observable Markov Decision Process(POMDP) 로 수식이 표현된다.\n우리는 환경에 대해서 잘 모르지만\n이전 History를 이용해서 사용하는 방법이 있고, \\[S_t^a = H_t\\]\rProbability 로 나타내는 방법이 있고, \\[S_t^a = \\left( \\mathbb{P}[S_t^e = s_1], \\dots, \\mathbb{P}[S_t^e = s_n] \\right)\\]\r순환신경망(Recurrent neural network) 으로 나타내는 방법도 있다. \\[S_t^a = \\sigma(S_{t-1}^a W_s + O_t W_o)\\]\rPolicy, Value Function, Model\r#\rRL은 아래 components를 한개 이상 포함한다.\nPolicy는 Agent가 어떻게 Action을 선택하는지(=behavior function) 이다.\nState로부터 function 파이를 이용해서(policy) 를 통해 action을 결정한다.\nDeterministic policy: \\( a = \\pi(s)\\)\rprobability로 policy를 표현하고자 한다면 다음과 같다.\nStochastic policy: \\( \\pi(a \\mid s) = \\mathbb{P}[A_t = a \\mid S_t = s]\\)\rValue Function은 State나 Action이 얼마나 좋은지(기대되는 미래의 reward가 얼마일지 예측) 을 나타낸다.\nState one과 State two, action one과 action two를 선택할때 최종 reward가 더 좋은쪽으로 선택한다.\n아래와 같이 표현되는데 \\( R_{t+1}, R_{t+2}, R_{t+3} \\)\r를 더하는 것과 같이 다음(미래)의 reward의 합의 기대값 \\[\rv_{\\pi}(s) = \\mathbb{E}_{\\pi} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s \\right]\r\\]\rgamma(\r\\( \\gamma\\)\r) 는 다음 스탭에 대한 discounting factor (미래를 예측하는것이니 넣은 변수로 이해함)\nModel은 Agent관점에서 Envrionment가 어떻게 동작할지 생각하는 것 을 나타낸다.\ntransitions model, rewards model 전통적으로 두가지로 나뉜다\ntranstions 모델은 directly 다음 state를 예측한다. \\[\\mathcal{P}_{ss'}^a = \\mathbb{P}[S_{t+1} = s' \\mid S_t = s, A_t = a]\\]\rRewards 모델은 reward를 예측한다. \\[\\mathcal{R}_s^a = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]\\]\rRL agent의 분류\r#\r어떤 key component를 가지고 있는지에 따라서 RL을 분류한다\nvalue-based RL은 value function을 가지고 있다. policy-based RL은 policy를 가지고 있다. actor-critic은 value function과 policy를 가지고 잇다. Model base로 구분하는 방법이 있다.\nmodel free는 모델이 없지만(=환경에 대한 representation이 없지만) value function + policy로 구성된 RL model based는 value function + policy, model 이 존재 Sequential decision making의 두가지\r#\rReinforcement Learning은 환경(Environment)을 모르고 상호작용하면서 reward가 최대가 되도록 학습 Planning은 환경을 알고(환경에 해당하는 model을 주고) agent가 계산하는 것 Exploration and Exploitation\r#\rExploration 와 Exploitation 는 trade-off\nExploration 은 환경에 대한 정보를 더 찾는것 Exploitation 은 알고 있는 정보를 활용해서 reward를 최대화 하는것 예) 새로운 레스토랑 찾기 vs 가장 좋아하는 레스토랑 재방문\n머신러닝(ML)과 딥러닝(DL)과의 관계\r#\r머신러닝(Machine Learning)은 인공지능(AI)의 개념으로써, 학습을 통해 예측(또는 분류)를 하는 것\n딥러닝은 머신러닝의 하위 개념으로써, 인공신경망(Neural Network)를 이용해서 학습하는 것\n강화 학습은 머신러닝의 한갈래로써, 보상을 기반으로 스스로 행동 학습하는 것\nAI\r├── Machine Learning\r│ ├── Supervised / Unsupervised\r│ ├── Reinforcement Learning\r│ └── Deep Learning\r│ └── Deep Reinforcement Learning (e.g., DQN, PPO) 참고\r#\rDavid Silver 의 RL 강좌 https://davidstarsilver.wordpress.com/teaching\n한글 유튜브 + 블로그 https://smj990203.tistory.com/2\n"},{"id":3,"href":"/development/firstpost/","title":"첫번째 글입니다.","section":"Development / 개발","content":"개발 관련 항목을 기록하기 위한 post 입니다.\nTodo List:\n상태 날짜 작업 내용 ✅ 2025-05-21 블로그 GitHub Page 연동 ✅ 2025-05-22 RL 관련 Post 남기기 (on-going) ✅ 2025-05-22 hugo + github page + giscus ✅ 2025-05-22 Google 서치 연동 ☐ Naver 서치 연동 ☐ 블로그 setup 관련 post 남기기 ☐ About Me 완성 ☐ Google 서치 연동 "},{"id":4,"href":"/development/blogsetup/","title":"Github pages 를 이용한 blog Setting (hugo + hugo-book theme + giscus","section":"Development / 개발","content":"\rGithub pages 를 이용한 blog Setting (hugo + hugo-book theme + giscus\r#\r0. blog host 방식 선택\r#\r여러 블로그 host방식을 고려하였으나, 유지보수가 손이 덜가며, 오랫동안 hosting이 되는것이 우선 순위였고.\nGithub pages를 통한 호스팅 방법을 선택하였다.\n그렇다면, static page를 generation을 해주는 framework로 jekyll 와 hugo를 고민하였고 ruby 보다 go로 이루어진 hugo를 믿기로 하였다.\n다른 blog들이 생성한 페이지를 주로 참조하였으며, 이 포스팅은 해당 글들의 엮음에 불과한점을 참조 부탁한다.\n1. Setup git, hugo\r#\r개발환경은 window 11 (Intel processor) 이므로 이를 기반으로 작성되었다.\ngit 설치\r#\rhttps://git-scm.com/downloads 사이트에서 Git for Windows/x64 Setup. 항목을 클릭하여 github을 설치하였다. 각 설정은 default를 그대로 사용하였다.\nhugo 설치\r#\rhttps://github.com/gohugoio/hugo/releases 에서 본인에 맞는 최신 버전을 받는다. (hugo_extended_0.147.4_windows-amd64.zip)\n후에 언급하겠지만 hugo-book theme 의 경우 hugp-extended 버전을 받아야했다.\nhugo 명령어를 치는 경우가 많으므로, 환경변수에 등록하였다. (되도록 path 파싱에 에러가 없도록 폴더들을 영어로 구성하였다.)\ngithub repository 생성\r#\rhttps://minyeamer.github.io/blog/hugo-blog-1/ 에서 언급된것 같이 하나의 repository를 branch로 나누어서 submodule로 활용하는 방안을 채택했다.\n\u0026lt;USERNAME\u0026gt;.github.io 인 repository를 하나 생성한다. UserName이 Jon 이라면 다음과 같이 생성될 것이다. Jon/Jon.github.io\nhugo를 통한 기본 뼈대 만들기. 여기서 폴더명을 생성한 repository로 맞춘다.\nhugo new site \u0026lt;USERNAME\u0026gt;.github.io cd \u0026lt;USERNAME\u0026gt;.github.io git init git add . git commit -m \u0026#34;feat: new site\u0026#34; git branch -M main git remote add origin https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git git push -u origin main branch 만들기 git branch gh-pages main git checkout gh-pages git push origin gh-pages git checkout main gh-pages를 submoudle로 연동하기 rm -rf public git submodule add -b gh-pages https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public git add public git add .gitmodules git commit -m \u0026#34;feat: add submodule for github pages\u0026#34; git push hugo Theme : hugo-book\r#\rhttps://themes.gohugo.io/ 에서 여러 테마가 선택이 가능하고, 가벼워보이는 hugo-book theme 를 채택하였다\nhttps://github.com/alex-shpak/hugo-book 에 Read.md를 참조하였다.\ngit submodule add https://github.com/alex-shpak/hugo-book themes/hugo-book git add . git commit -m \u0026#34;feat: import hugo theme\u0026#34; root path 에 있는 hugo.toml 파일에 아래 내용을 삽입한다.\nbaseURL = \u0026#39;https://\u0026lt;USERNAME\u0026gt;.github.io\u0026#39;\rlanguageCode = \u0026#39;ko-kr\u0026#39;\rtitle = \u0026#34;\u0026lt; title what you want\u0026gt;\u0026#34;\rtheme = \u0026#39;hugo-book\u0026#39; Github 설정\r#\rGithub -\u0026gt; .github.io -\u0026gt; Settings -\u0026gt; Pages -\u0026gt; Branch -\u0026gt; gh-pages 로 변경\n2. 블로그 배포 방법\r#\rhugo static page 생성\r#\r아래 명령어로 \u0026lt;root-path\u0026gt;/content/ path에 firstPost.md 파일이 생성된다\nhugo new firstPost.md 아래 커맨드를 이용하면 현재 static 페이지를 127.0.0.1:1313 에서 확인이 가능하다\nhugo server -D 아래와 같이 draft가 true가 있다면 디버깅은 가능하지만 최종 산출물에서 빠지게 된다. 따라서 나중에 빼놓지 말고 draft = true 문구를 지우던 false로 변경한다\n// firstPost.md\r+++\rtitle = \u0026#39;firstPost\u0026#39;\rdraft = true\r+++ 아래 명령어를 통해서 public/ 폴더 밑에 산출물들을 생성한다.\nhugo 아래 명령어를 통해서 github repository에 산출물들을 배포한다. 이 산출물들은 gitub action 에 의해서 자동으로 hosting되도록 처리된다.\ncd public git add . git commit -m \u0026#34;\u0026lt;comment what you want to add\u0026gt;\u0026#34; git push origin gh-pages cd .. git add . git commit -m \u0026#34;\u0026lt;comment what you want to add\u0026gt;\u0026#34; git push origin main 3. recent-post 용 home 만들기\r#\r아래와 같이 폴더 및 기본 md 파일을 구성하였고.\ncontent\r├── _index.md\r├── Content-Category1\r│ ├── _index.md\r│ └── article1.md\r└── Content-Category2\r├── _index.md\r└── post1.md conent/_index.md 파일은 다음과 같이 작성하였다.\ntest 참조\r#\rhttps://ialy1595.github.io/post/blog-construct-1/\nhttps://minyeamer.github.io/blog/hugo-blog-1/\n"}]